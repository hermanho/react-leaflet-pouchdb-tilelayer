module.exports = "(function (factory) {\n\ttypeof define === 'function' && define.amd ? define(factory) :\n\tfactory();\n}((function () { 'use strict';\n\n\tvar global = typeof self !== undefined ? self : this;\n\n\tvar commonjsGlobal = typeof globalThis !== 'undefined' ? globalThis : typeof window !== 'undefined' ? window : typeof global !== 'undefined' ? global : typeof self !== 'undefined' ? self : {};\n\n\tfunction createCommonjsModule(fn, basedir, module) {\n\t\treturn module = {\n\t\t  path: basedir,\n\t\t  exports: {},\n\t\t  require: function (path, base) {\n\t      return commonjsRequire(path, (base === undefined || base === null) ? module.path : base);\n\t    }\n\t\t}, fn(module, module.exports), module.exports;\n\t}\n\n\tfunction commonjsRequire () {\n\t\tthrow new Error('Dynamic requires are not currently supported by @rollup/plugin-commonjs');\n\t}\n\n\tvar Mutation = commonjsGlobal.MutationObserver || commonjsGlobal.WebKitMutationObserver;\n\n\tvar scheduleDrain;\n\n\t{\n\t  if (Mutation) {\n\t    var called = 0;\n\t    var observer = new Mutation(nextTick);\n\t    var element = commonjsGlobal.document.createTextNode('');\n\t    observer.observe(element, {\n\t      characterData: true\n\t    });\n\t    scheduleDrain = function () {\n\t      element.data = (called = ++called % 2);\n\t    };\n\t  } else if (!commonjsGlobal.setImmediate && typeof commonjsGlobal.MessageChannel !== 'undefined') {\n\t    var channel = new commonjsGlobal.MessageChannel();\n\t    channel.port1.onmessage = nextTick;\n\t    scheduleDrain = function () {\n\t      channel.port2.postMessage(0);\n\t    };\n\t  } else if ('document' in commonjsGlobal && 'onreadystatechange' in commonjsGlobal.document.createElement('script')) {\n\t    scheduleDrain = function () {\n\n\t      // Create a <script> element; its readystatechange event will be fired asynchronously once it is inserted\n\t      // into the document. Do so, thus queuing up the task. Remember to clean up once it's been called.\n\t      var scriptEl = commonjsGlobal.document.createElement('script');\n\t      scriptEl.onreadystatechange = function () {\n\t        nextTick();\n\n\t        scriptEl.onreadystatechange = null;\n\t        scriptEl.parentNode.removeChild(scriptEl);\n\t        scriptEl = null;\n\t      };\n\t      commonjsGlobal.document.documentElement.appendChild(scriptEl);\n\t    };\n\t  } else {\n\t    scheduleDrain = function () {\n\t      setTimeout(nextTick, 0);\n\t    };\n\t  }\n\t}\n\n\tvar draining;\n\tvar queue = [];\n\t//named nextTick for less confusing stack traces\n\tfunction nextTick() {\n\t  draining = true;\n\t  var i, oldQueue;\n\t  var len = queue.length;\n\t  while (len) {\n\t    oldQueue = queue;\n\t    queue = [];\n\t    i = -1;\n\t    while (++i < len) {\n\t      oldQueue[i]();\n\t    }\n\t    len = queue.length;\n\t  }\n\t  draining = false;\n\t}\n\n\tvar lib = immediate;\n\tfunction immediate(task) {\n\t  if (queue.push(task) === 1 && !draining) {\n\t    scheduleDrain();\n\t  }\n\t}\n\n\tvar crypto = {};\n\n\t// Unique ID creation requires a high quality random # generator.  In node.js\n\t// this is pretty straight-forward - we use the crypto API.\n\n\n\n\tvar rng = function nodeRNG() {\n\t  return crypto.randomBytes(16);\n\t};\n\n\t/**\n\t * Convert array of 16 byte values to UUID string format of the form:\n\t * XXXXXXXX-XXXX-XXXX-XXXX-XXXXXXXXXXXX\n\t */\n\tvar byteToHex = [];\n\tfor (var i = 0; i < 256; ++i) {\n\t  byteToHex[i] = (i + 0x100).toString(16).substr(1);\n\t}\n\n\tfunction bytesToUuid(buf, offset) {\n\t  var i = offset || 0;\n\t  var bth = byteToHex;\n\t  // join used to fix memory issue caused by concatenation: https://bugs.chromium.org/p/v8/issues/detail?id=3175#c4\n\t  return ([bth[buf[i++]], bth[buf[i++]], \n\t\tbth[buf[i++]], bth[buf[i++]], '-',\n\t\tbth[buf[i++]], bth[buf[i++]], '-',\n\t\tbth[buf[i++]], bth[buf[i++]], '-',\n\t\tbth[buf[i++]], bth[buf[i++]], '-',\n\t\tbth[buf[i++]], bth[buf[i++]],\n\t\tbth[buf[i++]], bth[buf[i++]],\n\t\tbth[buf[i++]], bth[buf[i++]]]).join('');\n\t}\n\n\tvar bytesToUuid_1 = bytesToUuid;\n\n\t// **`v1()` - Generate time-based UUID**\n\t//\n\t// Inspired by https://github.com/LiosK/UUID.js\n\t// and http://docs.python.org/library/uuid.html\n\n\tvar _nodeId;\n\tvar _clockseq;\n\n\t// Previous uuid creation time\n\tvar _lastMSecs = 0;\n\tvar _lastNSecs = 0;\n\n\t// See https://github.com/broofa/node-uuid for API details\n\tfunction v1(options, buf, offset) {\n\t  var i = buf && offset || 0;\n\t  var b = buf || [];\n\n\t  options = options || {};\n\t  var node = options.node || _nodeId;\n\t  var clockseq = options.clockseq !== undefined ? options.clockseq : _clockseq;\n\n\t  // node and clockseq need to be initialized to random values if they're not\n\t  // specified.  We do this lazily to minimize issues related to insufficient\n\t  // system entropy.  See #189\n\t  if (node == null || clockseq == null) {\n\t    var seedBytes = rng();\n\t    if (node == null) {\n\t      // Per 4.5, create and 48-bit node id, (47 random bits + multicast bit = 1)\n\t      node = _nodeId = [\n\t        seedBytes[0] | 0x01,\n\t        seedBytes[1], seedBytes[2], seedBytes[3], seedBytes[4], seedBytes[5]\n\t      ];\n\t    }\n\t    if (clockseq == null) {\n\t      // Per 4.2.2, randomize (14 bit) clockseq\n\t      clockseq = _clockseq = (seedBytes[6] << 8 | seedBytes[7]) & 0x3fff;\n\t    }\n\t  }\n\n\t  // UUID timestamps are 100 nano-second units since the Gregorian epoch,\n\t  // (1582-10-15 00:00).  JSNumbers aren't precise enough for this, so\n\t  // time is handled internally as 'msecs' (integer milliseconds) and 'nsecs'\n\t  // (100-nanoseconds offset from msecs) since unix epoch, 1970-01-01 00:00.\n\t  var msecs = options.msecs !== undefined ? options.msecs : new Date().getTime();\n\n\t  // Per 4.2.1.2, use count of uuid's generated during the current clock\n\t  // cycle to simulate higher resolution clock\n\t  var nsecs = options.nsecs !== undefined ? options.nsecs : _lastNSecs + 1;\n\n\t  // Time since last uuid creation (in msecs)\n\t  var dt = (msecs - _lastMSecs) + (nsecs - _lastNSecs)/10000;\n\n\t  // Per 4.2.1.2, Bump clockseq on clock regression\n\t  if (dt < 0 && options.clockseq === undefined) {\n\t    clockseq = clockseq + 1 & 0x3fff;\n\t  }\n\n\t  // Reset nsecs if clock regresses (new clockseq) or we've moved onto a new\n\t  // time interval\n\t  if ((dt < 0 || msecs > _lastMSecs) && options.nsecs === undefined) {\n\t    nsecs = 0;\n\t  }\n\n\t  // Per 4.2.1.2 Throw error if too many uuids are requested\n\t  if (nsecs >= 10000) {\n\t    throw new Error('uuid.v1(): Can\\'t create more than 10M uuids/sec');\n\t  }\n\n\t  _lastMSecs = msecs;\n\t  _lastNSecs = nsecs;\n\t  _clockseq = clockseq;\n\n\t  // Per 4.1.4 - Convert from unix epoch to Gregorian epoch\n\t  msecs += 12219292800000;\n\n\t  // `time_low`\n\t  var tl = ((msecs & 0xfffffff) * 10000 + nsecs) % 0x100000000;\n\t  b[i++] = tl >>> 24 & 0xff;\n\t  b[i++] = tl >>> 16 & 0xff;\n\t  b[i++] = tl >>> 8 & 0xff;\n\t  b[i++] = tl & 0xff;\n\n\t  // `time_mid`\n\t  var tmh = (msecs / 0x100000000 * 10000) & 0xfffffff;\n\t  b[i++] = tmh >>> 8 & 0xff;\n\t  b[i++] = tmh & 0xff;\n\n\t  // `time_high_and_version`\n\t  b[i++] = tmh >>> 24 & 0xf | 0x10; // include version\n\t  b[i++] = tmh >>> 16 & 0xff;\n\n\t  // `clock_seq_hi_and_reserved` (Per 4.2.2 - include variant)\n\t  b[i++] = clockseq >>> 8 | 0x80;\n\n\t  // `clock_seq_low`\n\t  b[i++] = clockseq & 0xff;\n\n\t  // `node`\n\t  for (var n = 0; n < 6; ++n) {\n\t    b[i + n] = node[n];\n\t  }\n\n\t  return buf ? buf : bytesToUuid_1(b);\n\t}\n\n\tvar v1_1 = v1;\n\n\tfunction v4(options, buf, offset) {\n\t  var i = buf && offset || 0;\n\n\t  if (typeof(options) == 'string') {\n\t    buf = options === 'binary' ? new Array(16) : null;\n\t    options = null;\n\t  }\n\t  options = options || {};\n\n\t  var rnds = options.random || (options.rng || rng)();\n\n\t  // Per 4.4, set bits for version and `clock_seq_hi_and_reserved`\n\t  rnds[6] = (rnds[6] & 0x0f) | 0x40;\n\t  rnds[8] = (rnds[8] & 0x3f) | 0x80;\n\n\t  // Copy bytes to buffer, if provided\n\t  if (buf) {\n\t    for (var ii = 0; ii < 16; ++ii) {\n\t      buf[i + ii] = rnds[ii];\n\t    }\n\t  }\n\n\t  return buf || bytesToUuid_1(rnds);\n\t}\n\n\tvar v4_1 = v4;\n\n\tvar uuid = v4_1;\n\tuuid.v1 = v1_1;\n\tuuid.v4 = v4_1;\n\n\tvar uuid_1 = uuid;\n\n\tvar sparkMd5 = createCommonjsModule(function (module, exports) {\n\t(function (factory) {\n\t    {\n\t        // Node/CommonJS\n\t        module.exports = factory();\n\t    }\n\t}(function (undefined$1) {\n\n\t    /*\n\t     * Fastest md5 implementation around (JKM md5).\n\t     * Credits: Joseph Myers\n\t     *\n\t     * @see http://www.myersdaily.org/joseph/javascript/md5-text.html\n\t     * @see http://jsperf.com/md5-shootout/7\n\t     */\n\n\t    /* this function is much faster,\n\t      so if possible we use it. Some IEs\n\t      are the only ones I know of that\n\t      need the idiotic second function,\n\t      generated by an if clause.  */\n\t    var hex_chr = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9', 'a', 'b', 'c', 'd', 'e', 'f'];\n\n\t    function md5cycle(x, k) {\n\t        var a = x[0],\n\t            b = x[1],\n\t            c = x[2],\n\t            d = x[3];\n\n\t        a += (b & c | ~b & d) + k[0] - 680876936 | 0;\n\t        a  = (a << 7 | a >>> 25) + b | 0;\n\t        d += (a & b | ~a & c) + k[1] - 389564586 | 0;\n\t        d  = (d << 12 | d >>> 20) + a | 0;\n\t        c += (d & a | ~d & b) + k[2] + 606105819 | 0;\n\t        c  = (c << 17 | c >>> 15) + d | 0;\n\t        b += (c & d | ~c & a) + k[3] - 1044525330 | 0;\n\t        b  = (b << 22 | b >>> 10) + c | 0;\n\t        a += (b & c | ~b & d) + k[4] - 176418897 | 0;\n\t        a  = (a << 7 | a >>> 25) + b | 0;\n\t        d += (a & b | ~a & c) + k[5] + 1200080426 | 0;\n\t        d  = (d << 12 | d >>> 20) + a | 0;\n\t        c += (d & a | ~d & b) + k[6] - 1473231341 | 0;\n\t        c  = (c << 17 | c >>> 15) + d | 0;\n\t        b += (c & d | ~c & a) + k[7] - 45705983 | 0;\n\t        b  = (b << 22 | b >>> 10) + c | 0;\n\t        a += (b & c | ~b & d) + k[8] + 1770035416 | 0;\n\t        a  = (a << 7 | a >>> 25) + b | 0;\n\t        d += (a & b | ~a & c) + k[9] - 1958414417 | 0;\n\t        d  = (d << 12 | d >>> 20) + a | 0;\n\t        c += (d & a | ~d & b) + k[10] - 42063 | 0;\n\t        c  = (c << 17 | c >>> 15) + d | 0;\n\t        b += (c & d | ~c & a) + k[11] - 1990404162 | 0;\n\t        b  = (b << 22 | b >>> 10) + c | 0;\n\t        a += (b & c | ~b & d) + k[12] + 1804603682 | 0;\n\t        a  = (a << 7 | a >>> 25) + b | 0;\n\t        d += (a & b | ~a & c) + k[13] - 40341101 | 0;\n\t        d  = (d << 12 | d >>> 20) + a | 0;\n\t        c += (d & a | ~d & b) + k[14] - 1502002290 | 0;\n\t        c  = (c << 17 | c >>> 15) + d | 0;\n\t        b += (c & d | ~c & a) + k[15] + 1236535329 | 0;\n\t        b  = (b << 22 | b >>> 10) + c | 0;\n\n\t        a += (b & d | c & ~d) + k[1] - 165796510 | 0;\n\t        a  = (a << 5 | a >>> 27) + b | 0;\n\t        d += (a & c | b & ~c) + k[6] - 1069501632 | 0;\n\t        d  = (d << 9 | d >>> 23) + a | 0;\n\t        c += (d & b | a & ~b) + k[11] + 643717713 | 0;\n\t        c  = (c << 14 | c >>> 18) + d | 0;\n\t        b += (c & a | d & ~a) + k[0] - 373897302 | 0;\n\t        b  = (b << 20 | b >>> 12) + c | 0;\n\t        a += (b & d | c & ~d) + k[5] - 701558691 | 0;\n\t        a  = (a << 5 | a >>> 27) + b | 0;\n\t        d += (a & c | b & ~c) + k[10] + 38016083 | 0;\n\t        d  = (d << 9 | d >>> 23) + a | 0;\n\t        c += (d & b | a & ~b) + k[15] - 660478335 | 0;\n\t        c  = (c << 14 | c >>> 18) + d | 0;\n\t        b += (c & a | d & ~a) + k[4] - 405537848 | 0;\n\t        b  = (b << 20 | b >>> 12) + c | 0;\n\t        a += (b & d | c & ~d) + k[9] + 568446438 | 0;\n\t        a  = (a << 5 | a >>> 27) + b | 0;\n\t        d += (a & c | b & ~c) + k[14] - 1019803690 | 0;\n\t        d  = (d << 9 | d >>> 23) + a | 0;\n\t        c += (d & b | a & ~b) + k[3] - 187363961 | 0;\n\t        c  = (c << 14 | c >>> 18) + d | 0;\n\t        b += (c & a | d & ~a) + k[8] + 1163531501 | 0;\n\t        b  = (b << 20 | b >>> 12) + c | 0;\n\t        a += (b & d | c & ~d) + k[13] - 1444681467 | 0;\n\t        a  = (a << 5 | a >>> 27) + b | 0;\n\t        d += (a & c | b & ~c) + k[2] - 51403784 | 0;\n\t        d  = (d << 9 | d >>> 23) + a | 0;\n\t        c += (d & b | a & ~b) + k[7] + 1735328473 | 0;\n\t        c  = (c << 14 | c >>> 18) + d | 0;\n\t        b += (c & a | d & ~a) + k[12] - 1926607734 | 0;\n\t        b  = (b << 20 | b >>> 12) + c | 0;\n\n\t        a += (b ^ c ^ d) + k[5] - 378558 | 0;\n\t        a  = (a << 4 | a >>> 28) + b | 0;\n\t        d += (a ^ b ^ c) + k[8] - 2022574463 | 0;\n\t        d  = (d << 11 | d >>> 21) + a | 0;\n\t        c += (d ^ a ^ b) + k[11] + 1839030562 | 0;\n\t        c  = (c << 16 | c >>> 16) + d | 0;\n\t        b += (c ^ d ^ a) + k[14] - 35309556 | 0;\n\t        b  = (b << 23 | b >>> 9) + c | 0;\n\t        a += (b ^ c ^ d) + k[1] - 1530992060 | 0;\n\t        a  = (a << 4 | a >>> 28) + b | 0;\n\t        d += (a ^ b ^ c) + k[4] + 1272893353 | 0;\n\t        d  = (d << 11 | d >>> 21) + a | 0;\n\t        c += (d ^ a ^ b) + k[7] - 155497632 | 0;\n\t        c  = (c << 16 | c >>> 16) + d | 0;\n\t        b += (c ^ d ^ a) + k[10] - 1094730640 | 0;\n\t        b  = (b << 23 | b >>> 9) + c | 0;\n\t        a += (b ^ c ^ d) + k[13] + 681279174 | 0;\n\t        a  = (a << 4 | a >>> 28) + b | 0;\n\t        d += (a ^ b ^ c) + k[0] - 358537222 | 0;\n\t        d  = (d << 11 | d >>> 21) + a | 0;\n\t        c += (d ^ a ^ b) + k[3] - 722521979 | 0;\n\t        c  = (c << 16 | c >>> 16) + d | 0;\n\t        b += (c ^ d ^ a) + k[6] + 76029189 | 0;\n\t        b  = (b << 23 | b >>> 9) + c | 0;\n\t        a += (b ^ c ^ d) + k[9] - 640364487 | 0;\n\t        a  = (a << 4 | a >>> 28) + b | 0;\n\t        d += (a ^ b ^ c) + k[12] - 421815835 | 0;\n\t        d  = (d << 11 | d >>> 21) + a | 0;\n\t        c += (d ^ a ^ b) + k[15] + 530742520 | 0;\n\t        c  = (c << 16 | c >>> 16) + d | 0;\n\t        b += (c ^ d ^ a) + k[2] - 995338651 | 0;\n\t        b  = (b << 23 | b >>> 9) + c | 0;\n\n\t        a += (c ^ (b | ~d)) + k[0] - 198630844 | 0;\n\t        a  = (a << 6 | a >>> 26) + b | 0;\n\t        d += (b ^ (a | ~c)) + k[7] + 1126891415 | 0;\n\t        d  = (d << 10 | d >>> 22) + a | 0;\n\t        c += (a ^ (d | ~b)) + k[14] - 1416354905 | 0;\n\t        c  = (c << 15 | c >>> 17) + d | 0;\n\t        b += (d ^ (c | ~a)) + k[5] - 57434055 | 0;\n\t        b  = (b << 21 |b >>> 11) + c | 0;\n\t        a += (c ^ (b | ~d)) + k[12] + 1700485571 | 0;\n\t        a  = (a << 6 | a >>> 26) + b | 0;\n\t        d += (b ^ (a | ~c)) + k[3] - 1894986606 | 0;\n\t        d  = (d << 10 | d >>> 22) + a | 0;\n\t        c += (a ^ (d | ~b)) + k[10] - 1051523 | 0;\n\t        c  = (c << 15 | c >>> 17) + d | 0;\n\t        b += (d ^ (c | ~a)) + k[1] - 2054922799 | 0;\n\t        b  = (b << 21 |b >>> 11) + c | 0;\n\t        a += (c ^ (b | ~d)) + k[8] + 1873313359 | 0;\n\t        a  = (a << 6 | a >>> 26) + b | 0;\n\t        d += (b ^ (a | ~c)) + k[15] - 30611744 | 0;\n\t        d  = (d << 10 | d >>> 22) + a | 0;\n\t        c += (a ^ (d | ~b)) + k[6] - 1560198380 | 0;\n\t        c  = (c << 15 | c >>> 17) + d | 0;\n\t        b += (d ^ (c | ~a)) + k[13] + 1309151649 | 0;\n\t        b  = (b << 21 |b >>> 11) + c | 0;\n\t        a += (c ^ (b | ~d)) + k[4] - 145523070 | 0;\n\t        a  = (a << 6 | a >>> 26) + b | 0;\n\t        d += (b ^ (a | ~c)) + k[11] - 1120210379 | 0;\n\t        d  = (d << 10 | d >>> 22) + a | 0;\n\t        c += (a ^ (d | ~b)) + k[2] + 718787259 | 0;\n\t        c  = (c << 15 | c >>> 17) + d | 0;\n\t        b += (d ^ (c | ~a)) + k[9] - 343485551 | 0;\n\t        b  = (b << 21 | b >>> 11) + c | 0;\n\n\t        x[0] = a + x[0] | 0;\n\t        x[1] = b + x[1] | 0;\n\t        x[2] = c + x[2] | 0;\n\t        x[3] = d + x[3] | 0;\n\t    }\n\n\t    function md5blk(s) {\n\t        var md5blks = [],\n\t            i; /* Andy King said do it this way. */\n\n\t        for (i = 0; i < 64; i += 4) {\n\t            md5blks[i >> 2] = s.charCodeAt(i) + (s.charCodeAt(i + 1) << 8) + (s.charCodeAt(i + 2) << 16) + (s.charCodeAt(i + 3) << 24);\n\t        }\n\t        return md5blks;\n\t    }\n\n\t    function md5blk_array(a) {\n\t        var md5blks = [],\n\t            i; /* Andy King said do it this way. */\n\n\t        for (i = 0; i < 64; i += 4) {\n\t            md5blks[i >> 2] = a[i] + (a[i + 1] << 8) + (a[i + 2] << 16) + (a[i + 3] << 24);\n\t        }\n\t        return md5blks;\n\t    }\n\n\t    function md51(s) {\n\t        var n = s.length,\n\t            state = [1732584193, -271733879, -1732584194, 271733878],\n\t            i,\n\t            length,\n\t            tail,\n\t            tmp,\n\t            lo,\n\t            hi;\n\n\t        for (i = 64; i <= n; i += 64) {\n\t            md5cycle(state, md5blk(s.substring(i - 64, i)));\n\t        }\n\t        s = s.substring(i - 64);\n\t        length = s.length;\n\t        tail = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\n\t        for (i = 0; i < length; i += 1) {\n\t            tail[i >> 2] |= s.charCodeAt(i) << ((i % 4) << 3);\n\t        }\n\t        tail[i >> 2] |= 0x80 << ((i % 4) << 3);\n\t        if (i > 55) {\n\t            md5cycle(state, tail);\n\t            for (i = 0; i < 16; i += 1) {\n\t                tail[i] = 0;\n\t            }\n\t        }\n\n\t        // Beware that the final length might not fit in 32 bits so we take care of that\n\t        tmp = n * 8;\n\t        tmp = tmp.toString(16).match(/(.*?)(.{0,8})$/);\n\t        lo = parseInt(tmp[2], 16);\n\t        hi = parseInt(tmp[1], 16) || 0;\n\n\t        tail[14] = lo;\n\t        tail[15] = hi;\n\n\t        md5cycle(state, tail);\n\t        return state;\n\t    }\n\n\t    function md51_array(a) {\n\t        var n = a.length,\n\t            state = [1732584193, -271733879, -1732584194, 271733878],\n\t            i,\n\t            length,\n\t            tail,\n\t            tmp,\n\t            lo,\n\t            hi;\n\n\t        for (i = 64; i <= n; i += 64) {\n\t            md5cycle(state, md5blk_array(a.subarray(i - 64, i)));\n\t        }\n\n\t        // Not sure if it is a bug, however IE10 will always produce a sub array of length 1\n\t        // containing the last element of the parent array if the sub array specified starts\n\t        // beyond the length of the parent array - weird.\n\t        // https://connect.microsoft.com/IE/feedback/details/771452/typed-array-subarray-issue\n\t        a = (i - 64) < n ? a.subarray(i - 64) : new Uint8Array(0);\n\n\t        length = a.length;\n\t        tail = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0];\n\t        for (i = 0; i < length; i += 1) {\n\t            tail[i >> 2] |= a[i] << ((i % 4) << 3);\n\t        }\n\n\t        tail[i >> 2] |= 0x80 << ((i % 4) << 3);\n\t        if (i > 55) {\n\t            md5cycle(state, tail);\n\t            for (i = 0; i < 16; i += 1) {\n\t                tail[i] = 0;\n\t            }\n\t        }\n\n\t        // Beware that the final length might not fit in 32 bits so we take care of that\n\t        tmp = n * 8;\n\t        tmp = tmp.toString(16).match(/(.*?)(.{0,8})$/);\n\t        lo = parseInt(tmp[2], 16);\n\t        hi = parseInt(tmp[1], 16) || 0;\n\n\t        tail[14] = lo;\n\t        tail[15] = hi;\n\n\t        md5cycle(state, tail);\n\n\t        return state;\n\t    }\n\n\t    function rhex(n) {\n\t        var s = '',\n\t            j;\n\t        for (j = 0; j < 4; j += 1) {\n\t            s += hex_chr[(n >> (j * 8 + 4)) & 0x0F] + hex_chr[(n >> (j * 8)) & 0x0F];\n\t        }\n\t        return s;\n\t    }\n\n\t    function hex(x) {\n\t        var i;\n\t        for (i = 0; i < x.length; i += 1) {\n\t            x[i] = rhex(x[i]);\n\t        }\n\t        return x.join('');\n\t    }\n\n\t    // In some cases the fast add32 function cannot be used..\n\t    if (hex(md51('hello')) !== '5d41402abc4b2a76b9719d911017c592') ;\n\n\t    // ---------------------------------------------------\n\n\t    /**\n\t     * ArrayBuffer slice polyfill.\n\t     *\n\t     * @see https://github.com/ttaubert/node-arraybuffer-slice\n\t     */\n\n\t    if (typeof ArrayBuffer !== 'undefined' && !ArrayBuffer.prototype.slice) {\n\t        (function () {\n\t            function clamp(val, length) {\n\t                val = (val | 0) || 0;\n\n\t                if (val < 0) {\n\t                    return Math.max(val + length, 0);\n\t                }\n\n\t                return Math.min(val, length);\n\t            }\n\n\t            ArrayBuffer.prototype.slice = function (from, to) {\n\t                var length = this.byteLength,\n\t                    begin = clamp(from, length),\n\t                    end = length,\n\t                    num,\n\t                    target,\n\t                    targetArray,\n\t                    sourceArray;\n\n\t                if (to !== undefined$1) {\n\t                    end = clamp(to, length);\n\t                }\n\n\t                if (begin > end) {\n\t                    return new ArrayBuffer(0);\n\t                }\n\n\t                num = end - begin;\n\t                target = new ArrayBuffer(num);\n\t                targetArray = new Uint8Array(target);\n\n\t                sourceArray = new Uint8Array(this, begin, num);\n\t                targetArray.set(sourceArray);\n\n\t                return target;\n\t            };\n\t        })();\n\t    }\n\n\t    // ---------------------------------------------------\n\n\t    /**\n\t     * Helpers.\n\t     */\n\n\t    function toUtf8(str) {\n\t        if (/[\\u0080-\\uFFFF]/.test(str)) {\n\t            str = unescape(encodeURIComponent(str));\n\t        }\n\n\t        return str;\n\t    }\n\n\t    function utf8Str2ArrayBuffer(str, returnUInt8Array) {\n\t        var length = str.length,\n\t           buff = new ArrayBuffer(length),\n\t           arr = new Uint8Array(buff),\n\t           i;\n\n\t        for (i = 0; i < length; i += 1) {\n\t            arr[i] = str.charCodeAt(i);\n\t        }\n\n\t        return returnUInt8Array ? arr : buff;\n\t    }\n\n\t    function arrayBuffer2Utf8Str(buff) {\n\t        return String.fromCharCode.apply(null, new Uint8Array(buff));\n\t    }\n\n\t    function concatenateArrayBuffers(first, second, returnUInt8Array) {\n\t        var result = new Uint8Array(first.byteLength + second.byteLength);\n\n\t        result.set(new Uint8Array(first));\n\t        result.set(new Uint8Array(second), first.byteLength);\n\n\t        return returnUInt8Array ? result : result.buffer;\n\t    }\n\n\t    function hexToBinaryString(hex) {\n\t        var bytes = [],\n\t            length = hex.length,\n\t            x;\n\n\t        for (x = 0; x < length - 1; x += 2) {\n\t            bytes.push(parseInt(hex.substr(x, 2), 16));\n\t        }\n\n\t        return String.fromCharCode.apply(String, bytes);\n\t    }\n\n\t    // ---------------------------------------------------\n\n\t    /**\n\t     * SparkMD5 OOP implementation.\n\t     *\n\t     * Use this class to perform an incremental md5, otherwise use the\n\t     * static methods instead.\n\t     */\n\n\t    function SparkMD5() {\n\t        // call reset to init the instance\n\t        this.reset();\n\t    }\n\n\t    /**\n\t     * Appends a string.\n\t     * A conversion will be applied if an utf8 string is detected.\n\t     *\n\t     * @param {String} str The string to be appended\n\t     *\n\t     * @return {SparkMD5} The instance itself\n\t     */\n\t    SparkMD5.prototype.append = function (str) {\n\t        // Converts the string to utf8 bytes if necessary\n\t        // Then append as binary\n\t        this.appendBinary(toUtf8(str));\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Appends a binary string.\n\t     *\n\t     * @param {String} contents The binary string to be appended\n\t     *\n\t     * @return {SparkMD5} The instance itself\n\t     */\n\t    SparkMD5.prototype.appendBinary = function (contents) {\n\t        this._buff += contents;\n\t        this._length += contents.length;\n\n\t        var length = this._buff.length,\n\t            i;\n\n\t        for (i = 64; i <= length; i += 64) {\n\t            md5cycle(this._hash, md5blk(this._buff.substring(i - 64, i)));\n\t        }\n\n\t        this._buff = this._buff.substring(i - 64);\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Finishes the incremental computation, reseting the internal state and\n\t     * returning the result.\n\t     *\n\t     * @param {Boolean} raw True to get the raw string, false to get the hex string\n\t     *\n\t     * @return {String} The result\n\t     */\n\t    SparkMD5.prototype.end = function (raw) {\n\t        var buff = this._buff,\n\t            length = buff.length,\n\t            i,\n\t            tail = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n\t            ret;\n\n\t        for (i = 0; i < length; i += 1) {\n\t            tail[i >> 2] |= buff.charCodeAt(i) << ((i % 4) << 3);\n\t        }\n\n\t        this._finish(tail, length);\n\t        ret = hex(this._hash);\n\n\t        if (raw) {\n\t            ret = hexToBinaryString(ret);\n\t        }\n\n\t        this.reset();\n\n\t        return ret;\n\t    };\n\n\t    /**\n\t     * Resets the internal state of the computation.\n\t     *\n\t     * @return {SparkMD5} The instance itself\n\t     */\n\t    SparkMD5.prototype.reset = function () {\n\t        this._buff = '';\n\t        this._length = 0;\n\t        this._hash = [1732584193, -271733879, -1732584194, 271733878];\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Gets the internal state of the computation.\n\t     *\n\t     * @return {Object} The state\n\t     */\n\t    SparkMD5.prototype.getState = function () {\n\t        return {\n\t            buff: this._buff,\n\t            length: this._length,\n\t            hash: this._hash\n\t        };\n\t    };\n\n\t    /**\n\t     * Gets the internal state of the computation.\n\t     *\n\t     * @param {Object} state The state\n\t     *\n\t     * @return {SparkMD5} The instance itself\n\t     */\n\t    SparkMD5.prototype.setState = function (state) {\n\t        this._buff = state.buff;\n\t        this._length = state.length;\n\t        this._hash = state.hash;\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Releases memory used by the incremental buffer and other additional\n\t     * resources. If you plan to use the instance again, use reset instead.\n\t     */\n\t    SparkMD5.prototype.destroy = function () {\n\t        delete this._hash;\n\t        delete this._buff;\n\t        delete this._length;\n\t    };\n\n\t    /**\n\t     * Finish the final calculation based on the tail.\n\t     *\n\t     * @param {Array}  tail   The tail (will be modified)\n\t     * @param {Number} length The length of the remaining buffer\n\t     */\n\t    SparkMD5.prototype._finish = function (tail, length) {\n\t        var i = length,\n\t            tmp,\n\t            lo,\n\t            hi;\n\n\t        tail[i >> 2] |= 0x80 << ((i % 4) << 3);\n\t        if (i > 55) {\n\t            md5cycle(this._hash, tail);\n\t            for (i = 0; i < 16; i += 1) {\n\t                tail[i] = 0;\n\t            }\n\t        }\n\n\t        // Do the final computation based on the tail and length\n\t        // Beware that the final length may not fit in 32 bits so we take care of that\n\t        tmp = this._length * 8;\n\t        tmp = tmp.toString(16).match(/(.*?)(.{0,8})$/);\n\t        lo = parseInt(tmp[2], 16);\n\t        hi = parseInt(tmp[1], 16) || 0;\n\n\t        tail[14] = lo;\n\t        tail[15] = hi;\n\t        md5cycle(this._hash, tail);\n\t    };\n\n\t    /**\n\t     * Performs the md5 hash on a string.\n\t     * A conversion will be applied if utf8 string is detected.\n\t     *\n\t     * @param {String}  str The string\n\t     * @param {Boolean} raw True to get the raw string, false to get the hex string\n\t     *\n\t     * @return {String} The result\n\t     */\n\t    SparkMD5.hash = function (str, raw) {\n\t        // Converts the string to utf8 bytes if necessary\n\t        // Then compute it using the binary function\n\t        return SparkMD5.hashBinary(toUtf8(str), raw);\n\t    };\n\n\t    /**\n\t     * Performs the md5 hash on a binary string.\n\t     *\n\t     * @param {String}  content The binary string\n\t     * @param {Boolean} raw     True to get the raw string, false to get the hex string\n\t     *\n\t     * @return {String} The result\n\t     */\n\t    SparkMD5.hashBinary = function (content, raw) {\n\t        var hash = md51(content),\n\t            ret = hex(hash);\n\n\t        return raw ? hexToBinaryString(ret) : ret;\n\t    };\n\n\t    // ---------------------------------------------------\n\n\t    /**\n\t     * SparkMD5 OOP implementation for array buffers.\n\t     *\n\t     * Use this class to perform an incremental md5 ONLY for array buffers.\n\t     */\n\t    SparkMD5.ArrayBuffer = function () {\n\t        // call reset to init the instance\n\t        this.reset();\n\t    };\n\n\t    /**\n\t     * Appends an array buffer.\n\t     *\n\t     * @param {ArrayBuffer} arr The array to be appended\n\t     *\n\t     * @return {SparkMD5.ArrayBuffer} The instance itself\n\t     */\n\t    SparkMD5.ArrayBuffer.prototype.append = function (arr) {\n\t        var buff = concatenateArrayBuffers(this._buff.buffer, arr, true),\n\t            length = buff.length,\n\t            i;\n\n\t        this._length += arr.byteLength;\n\n\t        for (i = 64; i <= length; i += 64) {\n\t            md5cycle(this._hash, md5blk_array(buff.subarray(i - 64, i)));\n\t        }\n\n\t        this._buff = (i - 64) < length ? new Uint8Array(buff.buffer.slice(i - 64)) : new Uint8Array(0);\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Finishes the incremental computation, reseting the internal state and\n\t     * returning the result.\n\t     *\n\t     * @param {Boolean} raw True to get the raw string, false to get the hex string\n\t     *\n\t     * @return {String} The result\n\t     */\n\t    SparkMD5.ArrayBuffer.prototype.end = function (raw) {\n\t        var buff = this._buff,\n\t            length = buff.length,\n\t            tail = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n\t            i,\n\t            ret;\n\n\t        for (i = 0; i < length; i += 1) {\n\t            tail[i >> 2] |= buff[i] << ((i % 4) << 3);\n\t        }\n\n\t        this._finish(tail, length);\n\t        ret = hex(this._hash);\n\n\t        if (raw) {\n\t            ret = hexToBinaryString(ret);\n\t        }\n\n\t        this.reset();\n\n\t        return ret;\n\t    };\n\n\t    /**\n\t     * Resets the internal state of the computation.\n\t     *\n\t     * @return {SparkMD5.ArrayBuffer} The instance itself\n\t     */\n\t    SparkMD5.ArrayBuffer.prototype.reset = function () {\n\t        this._buff = new Uint8Array(0);\n\t        this._length = 0;\n\t        this._hash = [1732584193, -271733879, -1732584194, 271733878];\n\n\t        return this;\n\t    };\n\n\t    /**\n\t     * Gets the internal state of the computation.\n\t     *\n\t     * @return {Object} The state\n\t     */\n\t    SparkMD5.ArrayBuffer.prototype.getState = function () {\n\t        var state = SparkMD5.prototype.getState.call(this);\n\n\t        // Convert buffer to a string\n\t        state.buff = arrayBuffer2Utf8Str(state.buff);\n\n\t        return state;\n\t    };\n\n\t    /**\n\t     * Gets the internal state of the computation.\n\t     *\n\t     * @param {Object} state The state\n\t     *\n\t     * @return {SparkMD5.ArrayBuffer} The instance itself\n\t     */\n\t    SparkMD5.ArrayBuffer.prototype.setState = function (state) {\n\t        // Convert string to buffer\n\t        state.buff = utf8Str2ArrayBuffer(state.buff, true);\n\n\t        return SparkMD5.prototype.setState.call(this, state);\n\t    };\n\n\t    SparkMD5.ArrayBuffer.prototype.destroy = SparkMD5.prototype.destroy;\n\n\t    SparkMD5.ArrayBuffer.prototype._finish = SparkMD5.prototype._finish;\n\n\t    /**\n\t     * Performs the md5 hash on an array buffer.\n\t     *\n\t     * @param {ArrayBuffer} arr The array buffer\n\t     * @param {Boolean}     raw True to get the raw string, false to get the hex one\n\t     *\n\t     * @return {String} The result\n\t     */\n\t    SparkMD5.ArrayBuffer.hash = function (arr, raw) {\n\t        var hash = md51_array(new Uint8Array(arr)),\n\t            ret = hex(hash);\n\n\t        return raw ? hexToBinaryString(ret) : ret;\n\t    };\n\n\t    return SparkMD5;\n\t}));\n\t});\n\n\t/**\n\t * Stringify/parse functions that don't operate\n\t * recursively, so they avoid call stack exceeded\n\t * errors.\n\t */\n\tvar stringify = function stringify(input) {\n\t  var queue = [];\n\t  queue.push({obj: input});\n\n\t  var res = '';\n\t  var next, obj, prefix, val, i, arrayPrefix, keys, k, key, value, objPrefix;\n\t  while ((next = queue.pop())) {\n\t    obj = next.obj;\n\t    prefix = next.prefix || '';\n\t    val = next.val || '';\n\t    res += prefix;\n\t    if (val) {\n\t      res += val;\n\t    } else if (typeof obj !== 'object') {\n\t      res += typeof obj === 'undefined' ? null : JSON.stringify(obj);\n\t    } else if (obj === null) {\n\t      res += 'null';\n\t    } else if (Array.isArray(obj)) {\n\t      queue.push({val: ']'});\n\t      for (i = obj.length - 1; i >= 0; i--) {\n\t        arrayPrefix = i === 0 ? '' : ',';\n\t        queue.push({obj: obj[i], prefix: arrayPrefix});\n\t      }\n\t      queue.push({val: '['});\n\t    } else { // object\n\t      keys = [];\n\t      for (k in obj) {\n\t        if (obj.hasOwnProperty(k)) {\n\t          keys.push(k);\n\t        }\n\t      }\n\t      queue.push({val: '}'});\n\t      for (i = keys.length - 1; i >= 0; i--) {\n\t        key = keys[i];\n\t        value = obj[key];\n\t        objPrefix = (i > 0 ? ',' : '');\n\t        objPrefix += JSON.stringify(key) + ':';\n\t        queue.push({obj: value, prefix: objPrefix});\n\t      }\n\t      queue.push({val: '{'});\n\t    }\n\t  }\n\t  return res;\n\t};\n\n\t// Convenience function for the parse function.\n\t// This pop function is basically copied from\n\t// pouchCollate.parseIndexableString\n\tfunction pop(obj, stack, metaStack) {\n\t  var lastMetaElement = metaStack[metaStack.length - 1];\n\t  if (obj === lastMetaElement.element) {\n\t    // popping a meta-element, e.g. an object whose value is another object\n\t    metaStack.pop();\n\t    lastMetaElement = metaStack[metaStack.length - 1];\n\t  }\n\t  var element = lastMetaElement.element;\n\t  var lastElementIndex = lastMetaElement.index;\n\t  if (Array.isArray(element)) {\n\t    element.push(obj);\n\t  } else if (lastElementIndex === stack.length - 2) { // obj with key+value\n\t    var key = stack.pop();\n\t    element[key] = obj;\n\t  } else {\n\t    stack.push(obj); // obj with key only\n\t  }\n\t}\n\n\tvar parse = function (str) {\n\t  var stack = [];\n\t  var metaStack = []; // stack for arrays and objects\n\t  var i = 0;\n\t  var collationIndex,parsedNum,numChar;\n\t  var parsedString,lastCh,numConsecutiveSlashes,ch;\n\t  var arrayElement, objElement;\n\t  while (true) {\n\t    collationIndex = str[i++];\n\t    if (collationIndex === '}' ||\n\t        collationIndex === ']' ||\n\t        typeof collationIndex === 'undefined') {\n\t      if (stack.length === 1) {\n\t        return stack.pop();\n\t      } else {\n\t        pop(stack.pop(), stack, metaStack);\n\t        continue;\n\t      }\n\t    }\n\t    switch (collationIndex) {\n\t      case ' ':\n\t      case '\\t':\n\t      case '\\n':\n\t      case ':':\n\t      case ',':\n\t        break;\n\t      case 'n':\n\t        i += 3; // 'ull'\n\t        pop(null, stack, metaStack);\n\t        break;\n\t      case 't':\n\t        i += 3; // 'rue'\n\t        pop(true, stack, metaStack);\n\t        break;\n\t      case 'f':\n\t        i += 4; // 'alse'\n\t        pop(false, stack, metaStack);\n\t        break;\n\t      case '0':\n\t      case '1':\n\t      case '2':\n\t      case '3':\n\t      case '4':\n\t      case '5':\n\t      case '6':\n\t      case '7':\n\t      case '8':\n\t      case '9':\n\t      case '-':\n\t        parsedNum = '';\n\t        i--;\n\t        while (true) {\n\t          numChar = str[i++];\n\t          if (/[\\d\\.\\-e\\+]/.test(numChar)) {\n\t            parsedNum += numChar;\n\t          } else {\n\t            i--;\n\t            break;\n\t          }\n\t        }\n\t        pop(parseFloat(parsedNum), stack, metaStack);\n\t        break;\n\t      case '\"':\n\t        parsedString = '';\n\t        lastCh = void 0;\n\t        numConsecutiveSlashes = 0;\n\t        while (true) {\n\t          ch = str[i++];\n\t          if (ch !== '\"' || (lastCh === '\\\\' &&\n\t              numConsecutiveSlashes % 2 === 1)) {\n\t            parsedString += ch;\n\t            lastCh = ch;\n\t            if (lastCh === '\\\\') {\n\t              numConsecutiveSlashes++;\n\t            } else {\n\t              numConsecutiveSlashes = 0;\n\t            }\n\t          } else {\n\t            break;\n\t          }\n\t        }\n\t        pop(JSON.parse('\"' + parsedString + '\"'), stack, metaStack);\n\t        break;\n\t      case '[':\n\t        arrayElement = { element: [], index: stack.length };\n\t        stack.push(arrayElement.element);\n\t        metaStack.push(arrayElement);\n\t        break;\n\t      case '{':\n\t        objElement = { element: {}, index: stack.length };\n\t        stack.push(objElement.element);\n\t        metaStack.push(objElement);\n\t        break;\n\t      default:\n\t        throw new Error(\n\t          'unexpectedly reached end of input: ' + collationIndex);\n\t    }\n\t  }\n\t};\n\n\tvar vuvuzela = {\n\t\tstringify: stringify,\n\t\tparse: parse\n\t};\n\n\tvar argsarray = argsArray;\n\n\tfunction argsArray(fun) {\n\t  return function () {\n\t    var len = arguments.length;\n\t    if (len) {\n\t      var args = [];\n\t      var i = -1;\n\t      while (++i < len) {\n\t        args[i] = arguments[i];\n\t      }\n\t      return fun.call(this, args);\n\t    } else {\n\t      return fun.call(this, []);\n\t    }\n\t  };\n\t}\n\n\tvar inherits;\n\tif (typeof Object.create === 'function'){\n\t  inherits = function inherits(ctor, superCtor) {\n\t    // implementation from standard node.js 'util' module\n\t    ctor.super_ = superCtor;\n\t    ctor.prototype = Object.create(superCtor.prototype, {\n\t      constructor: {\n\t        value: ctor,\n\t        enumerable: false,\n\t        writable: true,\n\t        configurable: true\n\t      }\n\t    });\n\t  };\n\t} else {\n\t  inherits = function inherits(ctor, superCtor) {\n\t    ctor.super_ = superCtor;\n\t    var TempCtor = function () {};\n\t    TempCtor.prototype = superCtor.prototype;\n\t    ctor.prototype = new TempCtor();\n\t    ctor.prototype.constructor = ctor;\n\t  };\n\t}\n\tvar inherits$1 = inherits;\n\n\t// Copyright Joyent, Inc. and other Node contributors.\n\tvar formatRegExp = /%[sdj%]/g;\n\tfunction format(f) {\n\t  if (!isString(f)) {\n\t    var objects = [];\n\t    for (var i = 0; i < arguments.length; i++) {\n\t      objects.push(inspect(arguments[i]));\n\t    }\n\t    return objects.join(' ');\n\t  }\n\n\t  var i = 1;\n\t  var args = arguments;\n\t  var len = args.length;\n\t  var str = String(f).replace(formatRegExp, function(x) {\n\t    if (x === '%%') return '%';\n\t    if (i >= len) return x;\n\t    switch (x) {\n\t      case '%s': return String(args[i++]);\n\t      case '%d': return Number(args[i++]);\n\t      case '%j':\n\t        try {\n\t          return JSON.stringify(args[i++]);\n\t        } catch (_) {\n\t          return '[Circular]';\n\t        }\n\t      default:\n\t        return x;\n\t    }\n\t  });\n\t  for (var x = args[i]; i < len; x = args[++i]) {\n\t    if (isNull(x) || !isObject(x)) {\n\t      str += ' ' + x;\n\t    } else {\n\t      str += ' ' + inspect(x);\n\t    }\n\t  }\n\t  return str;\n\t}\n\n\t// Mark that a method should not be used.\n\t// Returns a modified function which warns once by default.\n\t// If --no-deprecation is set, then it is a no-op.\n\tfunction deprecate(fn, msg) {\n\t  // Allow for deprecating things in the process of starting up.\n\t  if (isUndefined(global.process)) {\n\t    return function() {\n\t      return deprecate(fn, msg).apply(this, arguments);\n\t    };\n\t  }\n\n\t  var warned = false;\n\t  function deprecated() {\n\t    if (!warned) {\n\t      {\n\t        console.error(msg);\n\t      }\n\t      warned = true;\n\t    }\n\t    return fn.apply(this, arguments);\n\t  }\n\n\t  return deprecated;\n\t}\n\n\tvar debugs = {};\n\tvar debugEnviron;\n\tfunction debuglog(set) {\n\t  if (isUndefined(debugEnviron))\n\t    debugEnviron =  '';\n\t  set = set.toUpperCase();\n\t  if (!debugs[set]) {\n\t    if (new RegExp('\\\\b' + set + '\\\\b', 'i').test(debugEnviron)) {\n\t      var pid = 0;\n\t      debugs[set] = function() {\n\t        var msg = format.apply(null, arguments);\n\t        console.error('%s %d: %s', set, pid, msg);\n\t      };\n\t    } else {\n\t      debugs[set] = function() {};\n\t    }\n\t  }\n\t  return debugs[set];\n\t}\n\n\t/**\n\t * Echos the value of a value. Trys to print the value out\n\t * in the best way possible given the different types.\n\t *\n\t * @param {Object} obj The object to print out.\n\t * @param {Object} opts Optional options object that alters the output.\n\t */\n\t/* legacy: obj, showHidden, depth, colors*/\n\tfunction inspect(obj, opts) {\n\t  // default options\n\t  var ctx = {\n\t    seen: [],\n\t    stylize: stylizeNoColor\n\t  };\n\t  // legacy...\n\t  if (arguments.length >= 3) ctx.depth = arguments[2];\n\t  if (arguments.length >= 4) ctx.colors = arguments[3];\n\t  if (isBoolean(opts)) {\n\t    // legacy...\n\t    ctx.showHidden = opts;\n\t  } else if (opts) {\n\t    // got an \"options\" object\n\t    _extend(ctx, opts);\n\t  }\n\t  // set default options\n\t  if (isUndefined(ctx.showHidden)) ctx.showHidden = false;\n\t  if (isUndefined(ctx.depth)) ctx.depth = 2;\n\t  if (isUndefined(ctx.colors)) ctx.colors = false;\n\t  if (isUndefined(ctx.customInspect)) ctx.customInspect = true;\n\t  if (ctx.colors) ctx.stylize = stylizeWithColor;\n\t  return formatValue(ctx, obj, ctx.depth);\n\t}\n\n\t// http://en.wikipedia.org/wiki/ANSI_escape_code#graphics\n\tinspect.colors = {\n\t  'bold' : [1, 22],\n\t  'italic' : [3, 23],\n\t  'underline' : [4, 24],\n\t  'inverse' : [7, 27],\n\t  'white' : [37, 39],\n\t  'grey' : [90, 39],\n\t  'black' : [30, 39],\n\t  'blue' : [34, 39],\n\t  'cyan' : [36, 39],\n\t  'green' : [32, 39],\n\t  'magenta' : [35, 39],\n\t  'red' : [31, 39],\n\t  'yellow' : [33, 39]\n\t};\n\n\t// Don't use 'blue' not visible on cmd.exe\n\tinspect.styles = {\n\t  'special': 'cyan',\n\t  'number': 'yellow',\n\t  'boolean': 'yellow',\n\t  'undefined': 'grey',\n\t  'null': 'bold',\n\t  'string': 'green',\n\t  'date': 'magenta',\n\t  // \"name\": intentionally not styling\n\t  'regexp': 'red'\n\t};\n\n\n\tfunction stylizeWithColor(str, styleType) {\n\t  var style = inspect.styles[styleType];\n\n\t  if (style) {\n\t    return '\\u001b[' + inspect.colors[style][0] + 'm' + str +\n\t           '\\u001b[' + inspect.colors[style][1] + 'm';\n\t  } else {\n\t    return str;\n\t  }\n\t}\n\n\n\tfunction stylizeNoColor(str, styleType) {\n\t  return str;\n\t}\n\n\n\tfunction arrayToHash(array) {\n\t  var hash = {};\n\n\t  array.forEach(function(val, idx) {\n\t    hash[val] = true;\n\t  });\n\n\t  return hash;\n\t}\n\n\n\tfunction formatValue(ctx, value, recurseTimes) {\n\t  // Provide a hook for user-specified inspect functions.\n\t  // Check that value is an object with an inspect function on it\n\t  if (ctx.customInspect &&\n\t      value &&\n\t      isFunction(value.inspect) &&\n\t      // Filter out the util module, it's inspect function is special\n\t      value.inspect !== inspect &&\n\t      // Also filter out any prototype objects using the circular check.\n\t      !(value.constructor && value.constructor.prototype === value)) {\n\t    var ret = value.inspect(recurseTimes, ctx);\n\t    if (!isString(ret)) {\n\t      ret = formatValue(ctx, ret, recurseTimes);\n\t    }\n\t    return ret;\n\t  }\n\n\t  // Primitive types cannot have properties\n\t  var primitive = formatPrimitive(ctx, value);\n\t  if (primitive) {\n\t    return primitive;\n\t  }\n\n\t  // Look up the keys of the object.\n\t  var keys = Object.keys(value);\n\t  var visibleKeys = arrayToHash(keys);\n\n\t  if (ctx.showHidden) {\n\t    keys = Object.getOwnPropertyNames(value);\n\t  }\n\n\t  // IE doesn't make error fields non-enumerable\n\t  // http://msdn.microsoft.com/en-us/library/ie/dww52sbt(v=vs.94).aspx\n\t  if (isError(value)\n\t      && (keys.indexOf('message') >= 0 || keys.indexOf('description') >= 0)) {\n\t    return formatError(value);\n\t  }\n\n\t  // Some type of object without properties can be shortcutted.\n\t  if (keys.length === 0) {\n\t    if (isFunction(value)) {\n\t      var name = value.name ? ': ' + value.name : '';\n\t      return ctx.stylize('[Function' + name + ']', 'special');\n\t    }\n\t    if (isRegExp(value)) {\n\t      return ctx.stylize(RegExp.prototype.toString.call(value), 'regexp');\n\t    }\n\t    if (isDate(value)) {\n\t      return ctx.stylize(Date.prototype.toString.call(value), 'date');\n\t    }\n\t    if (isError(value)) {\n\t      return formatError(value);\n\t    }\n\t  }\n\n\t  var base = '', array = false, braces = ['{', '}'];\n\n\t  // Make Array say that they are Array\n\t  if (isArray(value)) {\n\t    array = true;\n\t    braces = ['[', ']'];\n\t  }\n\n\t  // Make functions say that they are functions\n\t  if (isFunction(value)) {\n\t    var n = value.name ? ': ' + value.name : '';\n\t    base = ' [Function' + n + ']';\n\t  }\n\n\t  // Make RegExps say that they are RegExps\n\t  if (isRegExp(value)) {\n\t    base = ' ' + RegExp.prototype.toString.call(value);\n\t  }\n\n\t  // Make dates with properties first say the date\n\t  if (isDate(value)) {\n\t    base = ' ' + Date.prototype.toUTCString.call(value);\n\t  }\n\n\t  // Make error with message first say the error\n\t  if (isError(value)) {\n\t    base = ' ' + formatError(value);\n\t  }\n\n\t  if (keys.length === 0 && (!array || value.length == 0)) {\n\t    return braces[0] + base + braces[1];\n\t  }\n\n\t  if (recurseTimes < 0) {\n\t    if (isRegExp(value)) {\n\t      return ctx.stylize(RegExp.prototype.toString.call(value), 'regexp');\n\t    } else {\n\t      return ctx.stylize('[Object]', 'special');\n\t    }\n\t  }\n\n\t  ctx.seen.push(value);\n\n\t  var output;\n\t  if (array) {\n\t    output = formatArray(ctx, value, recurseTimes, visibleKeys, keys);\n\t  } else {\n\t    output = keys.map(function(key) {\n\t      return formatProperty(ctx, value, recurseTimes, visibleKeys, key, array);\n\t    });\n\t  }\n\n\t  ctx.seen.pop();\n\n\t  return reduceToSingleString(output, base, braces);\n\t}\n\n\n\tfunction formatPrimitive(ctx, value) {\n\t  if (isUndefined(value))\n\t    return ctx.stylize('undefined', 'undefined');\n\t  if (isString(value)) {\n\t    var simple = '\\'' + JSON.stringify(value).replace(/^\"|\"$/g, '')\n\t                                             .replace(/'/g, \"\\\\'\")\n\t                                             .replace(/\\\\\"/g, '\"') + '\\'';\n\t    return ctx.stylize(simple, 'string');\n\t  }\n\t  if (isNumber(value))\n\t    return ctx.stylize('' + value, 'number');\n\t  if (isBoolean(value))\n\t    return ctx.stylize('' + value, 'boolean');\n\t  // For some reason typeof null is \"object\", so special case here.\n\t  if (isNull(value))\n\t    return ctx.stylize('null', 'null');\n\t}\n\n\n\tfunction formatError(value) {\n\t  return '[' + Error.prototype.toString.call(value) + ']';\n\t}\n\n\n\tfunction formatArray(ctx, value, recurseTimes, visibleKeys, keys) {\n\t  var output = [];\n\t  for (var i = 0, l = value.length; i < l; ++i) {\n\t    if (hasOwnProperty(value, String(i))) {\n\t      output.push(formatProperty(ctx, value, recurseTimes, visibleKeys,\n\t          String(i), true));\n\t    } else {\n\t      output.push('');\n\t    }\n\t  }\n\t  keys.forEach(function(key) {\n\t    if (!key.match(/^\\d+$/)) {\n\t      output.push(formatProperty(ctx, value, recurseTimes, visibleKeys,\n\t          key, true));\n\t    }\n\t  });\n\t  return output;\n\t}\n\n\n\tfunction formatProperty(ctx, value, recurseTimes, visibleKeys, key, array) {\n\t  var name, str, desc;\n\t  desc = Object.getOwnPropertyDescriptor(value, key) || { value: value[key] };\n\t  if (desc.get) {\n\t    if (desc.set) {\n\t      str = ctx.stylize('[Getter/Setter]', 'special');\n\t    } else {\n\t      str = ctx.stylize('[Getter]', 'special');\n\t    }\n\t  } else {\n\t    if (desc.set) {\n\t      str = ctx.stylize('[Setter]', 'special');\n\t    }\n\t  }\n\t  if (!hasOwnProperty(visibleKeys, key)) {\n\t    name = '[' + key + ']';\n\t  }\n\t  if (!str) {\n\t    if (ctx.seen.indexOf(desc.value) < 0) {\n\t      if (isNull(recurseTimes)) {\n\t        str = formatValue(ctx, desc.value, null);\n\t      } else {\n\t        str = formatValue(ctx, desc.value, recurseTimes - 1);\n\t      }\n\t      if (str.indexOf('\\n') > -1) {\n\t        if (array) {\n\t          str = str.split('\\n').map(function(line) {\n\t            return '  ' + line;\n\t          }).join('\\n').substr(2);\n\t        } else {\n\t          str = '\\n' + str.split('\\n').map(function(line) {\n\t            return '   ' + line;\n\t          }).join('\\n');\n\t        }\n\t      }\n\t    } else {\n\t      str = ctx.stylize('[Circular]', 'special');\n\t    }\n\t  }\n\t  if (isUndefined(name)) {\n\t    if (array && key.match(/^\\d+$/)) {\n\t      return str;\n\t    }\n\t    name = JSON.stringify('' + key);\n\t    if (name.match(/^\"([a-zA-Z_][a-zA-Z_0-9]*)\"$/)) {\n\t      name = name.substr(1, name.length - 2);\n\t      name = ctx.stylize(name, 'name');\n\t    } else {\n\t      name = name.replace(/'/g, \"\\\\'\")\n\t                 .replace(/\\\\\"/g, '\"')\n\t                 .replace(/(^\"|\"$)/g, \"'\");\n\t      name = ctx.stylize(name, 'string');\n\t    }\n\t  }\n\n\t  return name + ': ' + str;\n\t}\n\n\n\tfunction reduceToSingleString(output, base, braces) {\n\t  var length = output.reduce(function(prev, cur) {\n\t    if (cur.indexOf('\\n') >= 0) ;\n\t    return prev + cur.replace(/\\u001b\\[\\d\\d?m/g, '').length + 1;\n\t  }, 0);\n\n\t  if (length > 60) {\n\t    return braces[0] +\n\t           (base === '' ? '' : base + '\\n ') +\n\t           ' ' +\n\t           output.join(',\\n  ') +\n\t           ' ' +\n\t           braces[1];\n\t  }\n\n\t  return braces[0] + base + ' ' + output.join(', ') + ' ' + braces[1];\n\t}\n\n\n\t// NOTE: These type checking functions intentionally don't use `instanceof`\n\t// because it is fragile and can be easily faked with `Object.create()`.\n\tfunction isArray(ar) {\n\t  return Array.isArray(ar);\n\t}\n\n\tfunction isBoolean(arg) {\n\t  return typeof arg === 'boolean';\n\t}\n\n\tfunction isNull(arg) {\n\t  return arg === null;\n\t}\n\n\tfunction isNullOrUndefined(arg) {\n\t  return arg == null;\n\t}\n\n\tfunction isNumber(arg) {\n\t  return typeof arg === 'number';\n\t}\n\n\tfunction isString(arg) {\n\t  return typeof arg === 'string';\n\t}\n\n\tfunction isSymbol(arg) {\n\t  return typeof arg === 'symbol';\n\t}\n\n\tfunction isUndefined(arg) {\n\t  return arg === void 0;\n\t}\n\n\tfunction isRegExp(re) {\n\t  return isObject(re) && objectToString(re) === '[object RegExp]';\n\t}\n\n\tfunction isObject(arg) {\n\t  return typeof arg === 'object' && arg !== null;\n\t}\n\n\tfunction isDate(d) {\n\t  return isObject(d) && objectToString(d) === '[object Date]';\n\t}\n\n\tfunction isError(e) {\n\t  return isObject(e) &&\n\t      (objectToString(e) === '[object Error]' || e instanceof Error);\n\t}\n\n\tfunction isFunction(arg) {\n\t  return typeof arg === 'function';\n\t}\n\n\tfunction isPrimitive(arg) {\n\t  return arg === null ||\n\t         typeof arg === 'boolean' ||\n\t         typeof arg === 'number' ||\n\t         typeof arg === 'string' ||\n\t         typeof arg === 'symbol' ||  // ES6 symbol\n\t         typeof arg === 'undefined';\n\t}\n\n\tfunction isBuffer(maybeBuf) {\n\t  return Buffer.isBuffer(maybeBuf);\n\t}\n\n\tfunction objectToString(o) {\n\t  return Object.prototype.toString.call(o);\n\t}\n\n\n\tfunction pad(n) {\n\t  return n < 10 ? '0' + n.toString(10) : n.toString(10);\n\t}\n\n\n\tvar months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep',\n\t              'Oct', 'Nov', 'Dec'];\n\n\t// 26 Feb 16:19:34\n\tfunction timestamp() {\n\t  var d = new Date();\n\t  var time = [pad(d.getHours()),\n\t              pad(d.getMinutes()),\n\t              pad(d.getSeconds())].join(':');\n\t  return [d.getDate(), months[d.getMonth()], time].join(' ');\n\t}\n\n\n\t// log is just a thin wrapper to console.log that prepends a timestamp\n\tfunction log() {\n\t  console.log('%s - %s', timestamp(), format.apply(null, arguments));\n\t}\n\n\tfunction _extend(origin, add) {\n\t  // Don't do anything if add isn't an object\n\t  if (!add || !isObject(add)) return origin;\n\n\t  var keys = Object.keys(add);\n\t  var i = keys.length;\n\t  while (i--) {\n\t    origin[keys[i]] = add[keys[i]];\n\t  }\n\t  return origin;\n\t}\n\tfunction hasOwnProperty(obj, prop) {\n\t  return Object.prototype.hasOwnProperty.call(obj, prop);\n\t}\n\n\tvar require$$0 = {\n\t  inherits: inherits$1,\n\t  _extend: _extend,\n\t  log: log,\n\t  isBuffer: isBuffer,\n\t  isPrimitive: isPrimitive,\n\t  isFunction: isFunction,\n\t  isError: isError,\n\t  isDate: isDate,\n\t  isObject: isObject,\n\t  isRegExp: isRegExp,\n\t  isUndefined: isUndefined,\n\t  isSymbol: isSymbol,\n\t  isString: isString,\n\t  isNumber: isNumber,\n\t  isNullOrUndefined: isNullOrUndefined,\n\t  isNull: isNull,\n\t  isBoolean: isBoolean,\n\t  isArray: isArray,\n\t  inspect: inspect,\n\t  deprecate: deprecate,\n\t  format: format,\n\t  debuglog: debuglog\n\t};\n\n\tvar inherits_browser = createCommonjsModule(function (module) {\n\tif (typeof Object.create === 'function') {\n\t  // implementation from standard node.js 'util' module\n\t  module.exports = function inherits(ctor, superCtor) {\n\t    if (superCtor) {\n\t      ctor.super_ = superCtor;\n\t      ctor.prototype = Object.create(superCtor.prototype, {\n\t        constructor: {\n\t          value: ctor,\n\t          enumerable: false,\n\t          writable: true,\n\t          configurable: true\n\t        }\n\t      });\n\t    }\n\t  };\n\t} else {\n\t  // old school shim for old browsers\n\t  module.exports = function inherits(ctor, superCtor) {\n\t    if (superCtor) {\n\t      ctor.super_ = superCtor;\n\t      var TempCtor = function () {};\n\t      TempCtor.prototype = superCtor.prototype;\n\t      ctor.prototype = new TempCtor();\n\t      ctor.prototype.constructor = ctor;\n\t    }\n\t  };\n\t}\n\t});\n\n\tvar inherits$2 = createCommonjsModule(function (module) {\n\ttry {\n\t  var util = require$$0;\n\t  /* istanbul ignore next */\n\t  if (typeof util.inherits !== 'function') throw '';\n\t  module.exports = util.inherits;\n\t} catch (e) {\n\t  /* istanbul ignore next */\n\t  module.exports = inherits_browser;\n\t}\n\t});\n\n\tvar domain;\n\n\t// This constructor is used to store event handlers. Instantiating this is\n\t// faster than explicitly calling `Object.create(null)` to get a \"clean\" empty\n\t// object (tested with v8 v4.9).\n\tfunction EventHandlers() {}\n\tEventHandlers.prototype = Object.create(null);\n\n\tfunction EventEmitter() {\n\t  EventEmitter.init.call(this);\n\t}\n\n\t// nodejs oddity\n\t// require('events') === require('events').EventEmitter\n\tEventEmitter.EventEmitter = EventEmitter;\n\n\tEventEmitter.usingDomains = false;\n\n\tEventEmitter.prototype.domain = undefined;\n\tEventEmitter.prototype._events = undefined;\n\tEventEmitter.prototype._maxListeners = undefined;\n\n\t// By default EventEmitters will print a warning if more than 10 listeners are\n\t// added to it. This is a useful default which helps finding memory leaks.\n\tEventEmitter.defaultMaxListeners = 10;\n\n\tEventEmitter.init = function() {\n\t  this.domain = null;\n\t  if (EventEmitter.usingDomains) {\n\t    // if there is an active domain, then attach to it.\n\t    if (domain.active ) ;\n\t  }\n\n\t  if (!this._events || this._events === Object.getPrototypeOf(this)._events) {\n\t    this._events = new EventHandlers();\n\t    this._eventsCount = 0;\n\t  }\n\n\t  this._maxListeners = this._maxListeners || undefined;\n\t};\n\n\t// Obviously not all Emitters should be limited to 10. This function allows\n\t// that to be increased. Set to zero for unlimited.\n\tEventEmitter.prototype.setMaxListeners = function setMaxListeners(n) {\n\t  if (typeof n !== 'number' || n < 0 || isNaN(n))\n\t    throw new TypeError('\"n\" argument must be a positive number');\n\t  this._maxListeners = n;\n\t  return this;\n\t};\n\n\tfunction $getMaxListeners(that) {\n\t  if (that._maxListeners === undefined)\n\t    return EventEmitter.defaultMaxListeners;\n\t  return that._maxListeners;\n\t}\n\n\tEventEmitter.prototype.getMaxListeners = function getMaxListeners() {\n\t  return $getMaxListeners(this);\n\t};\n\n\t// These standalone emit* functions are used to optimize calling of event\n\t// handlers for fast cases because emit() itself often has a variable number of\n\t// arguments and can be deoptimized because of that. These functions always have\n\t// the same number of arguments and thus do not get deoptimized, so the code\n\t// inside them can execute faster.\n\tfunction emitNone(handler, isFn, self) {\n\t  if (isFn)\n\t    handler.call(self);\n\t  else {\n\t    var len = handler.length;\n\t    var listeners = arrayClone(handler, len);\n\t    for (var i = 0; i < len; ++i)\n\t      listeners[i].call(self);\n\t  }\n\t}\n\tfunction emitOne(handler, isFn, self, arg1) {\n\t  if (isFn)\n\t    handler.call(self, arg1);\n\t  else {\n\t    var len = handler.length;\n\t    var listeners = arrayClone(handler, len);\n\t    for (var i = 0; i < len; ++i)\n\t      listeners[i].call(self, arg1);\n\t  }\n\t}\n\tfunction emitTwo(handler, isFn, self, arg1, arg2) {\n\t  if (isFn)\n\t    handler.call(self, arg1, arg2);\n\t  else {\n\t    var len = handler.length;\n\t    var listeners = arrayClone(handler, len);\n\t    for (var i = 0; i < len; ++i)\n\t      listeners[i].call(self, arg1, arg2);\n\t  }\n\t}\n\tfunction emitThree(handler, isFn, self, arg1, arg2, arg3) {\n\t  if (isFn)\n\t    handler.call(self, arg1, arg2, arg3);\n\t  else {\n\t    var len = handler.length;\n\t    var listeners = arrayClone(handler, len);\n\t    for (var i = 0; i < len; ++i)\n\t      listeners[i].call(self, arg1, arg2, arg3);\n\t  }\n\t}\n\n\tfunction emitMany(handler, isFn, self, args) {\n\t  if (isFn)\n\t    handler.apply(self, args);\n\t  else {\n\t    var len = handler.length;\n\t    var listeners = arrayClone(handler, len);\n\t    for (var i = 0; i < len; ++i)\n\t      listeners[i].apply(self, args);\n\t  }\n\t}\n\n\tEventEmitter.prototype.emit = function emit(type) {\n\t  var er, handler, len, args, i, events, domain;\n\t  var doError = (type === 'error');\n\n\t  events = this._events;\n\t  if (events)\n\t    doError = (doError && events.error == null);\n\t  else if (!doError)\n\t    return false;\n\n\t  domain = this.domain;\n\n\t  // If there is no 'error' event listener then throw.\n\t  if (doError) {\n\t    er = arguments[1];\n\t    if (domain) {\n\t      if (!er)\n\t        er = new Error('Uncaught, unspecified \"error\" event');\n\t      er.domainEmitter = this;\n\t      er.domain = domain;\n\t      er.domainThrown = false;\n\t      domain.emit('error', er);\n\t    } else if (er instanceof Error) {\n\t      throw er; // Unhandled 'error' event\n\t    } else {\n\t      // At least give some kind of context to the user\n\t      var err = new Error('Uncaught, unspecified \"error\" event. (' + er + ')');\n\t      err.context = er;\n\t      throw err;\n\t    }\n\t    return false;\n\t  }\n\n\t  handler = events[type];\n\n\t  if (!handler)\n\t    return false;\n\n\t  var isFn = typeof handler === 'function';\n\t  len = arguments.length;\n\t  switch (len) {\n\t    // fast cases\n\t    case 1:\n\t      emitNone(handler, isFn, this);\n\t      break;\n\t    case 2:\n\t      emitOne(handler, isFn, this, arguments[1]);\n\t      break;\n\t    case 3:\n\t      emitTwo(handler, isFn, this, arguments[1], arguments[2]);\n\t      break;\n\t    case 4:\n\t      emitThree(handler, isFn, this, arguments[1], arguments[2], arguments[3]);\n\t      break;\n\t    // slower\n\t    default:\n\t      args = new Array(len - 1);\n\t      for (i = 1; i < len; i++)\n\t        args[i - 1] = arguments[i];\n\t      emitMany(handler, isFn, this, args);\n\t  }\n\n\t  return true;\n\t};\n\n\tfunction _addListener(target, type, listener, prepend) {\n\t  var m;\n\t  var events;\n\t  var existing;\n\n\t  if (typeof listener !== 'function')\n\t    throw new TypeError('\"listener\" argument must be a function');\n\n\t  events = target._events;\n\t  if (!events) {\n\t    events = target._events = new EventHandlers();\n\t    target._eventsCount = 0;\n\t  } else {\n\t    // To avoid recursion in the case that type === \"newListener\"! Before\n\t    // adding it to the listeners, first emit \"newListener\".\n\t    if (events.newListener) {\n\t      target.emit('newListener', type,\n\t                  listener.listener ? listener.listener : listener);\n\n\t      // Re-assign `events` because a newListener handler could have caused the\n\t      // this._events to be assigned to a new object\n\t      events = target._events;\n\t    }\n\t    existing = events[type];\n\t  }\n\n\t  if (!existing) {\n\t    // Optimize the case of one listener. Don't need the extra array object.\n\t    existing = events[type] = listener;\n\t    ++target._eventsCount;\n\t  } else {\n\t    if (typeof existing === 'function') {\n\t      // Adding the second element, need to change to array.\n\t      existing = events[type] = prepend ? [listener, existing] :\n\t                                          [existing, listener];\n\t    } else {\n\t      // If we've already got an array, just append.\n\t      if (prepend) {\n\t        existing.unshift(listener);\n\t      } else {\n\t        existing.push(listener);\n\t      }\n\t    }\n\n\t    // Check for listener leak\n\t    if (!existing.warned) {\n\t      m = $getMaxListeners(target);\n\t      if (m && m > 0 && existing.length > m) {\n\t        existing.warned = true;\n\t        var w = new Error('Possible EventEmitter memory leak detected. ' +\n\t                            existing.length + ' ' + type + ' listeners added. ' +\n\t                            'Use emitter.setMaxListeners() to increase limit');\n\t        w.name = 'MaxListenersExceededWarning';\n\t        w.emitter = target;\n\t        w.type = type;\n\t        w.count = existing.length;\n\t        emitWarning(w);\n\t      }\n\t    }\n\t  }\n\n\t  return target;\n\t}\n\tfunction emitWarning(e) {\n\t  typeof console.warn === 'function' ? console.warn(e) : console.log(e);\n\t}\n\tEventEmitter.prototype.addListener = function addListener(type, listener) {\n\t  return _addListener(this, type, listener, false);\n\t};\n\n\tEventEmitter.prototype.on = EventEmitter.prototype.addListener;\n\n\tEventEmitter.prototype.prependListener =\n\t    function prependListener(type, listener) {\n\t      return _addListener(this, type, listener, true);\n\t    };\n\n\tfunction _onceWrap(target, type, listener) {\n\t  var fired = false;\n\t  function g() {\n\t    target.removeListener(type, g);\n\t    if (!fired) {\n\t      fired = true;\n\t      listener.apply(target, arguments);\n\t    }\n\t  }\n\t  g.listener = listener;\n\t  return g;\n\t}\n\n\tEventEmitter.prototype.once = function once(type, listener) {\n\t  if (typeof listener !== 'function')\n\t    throw new TypeError('\"listener\" argument must be a function');\n\t  this.on(type, _onceWrap(this, type, listener));\n\t  return this;\n\t};\n\n\tEventEmitter.prototype.prependOnceListener =\n\t    function prependOnceListener(type, listener) {\n\t      if (typeof listener !== 'function')\n\t        throw new TypeError('\"listener\" argument must be a function');\n\t      this.prependListener(type, _onceWrap(this, type, listener));\n\t      return this;\n\t    };\n\n\t// emits a 'removeListener' event iff the listener was removed\n\tEventEmitter.prototype.removeListener =\n\t    function removeListener(type, listener) {\n\t      var list, events, position, i, originalListener;\n\n\t      if (typeof listener !== 'function')\n\t        throw new TypeError('\"listener\" argument must be a function');\n\n\t      events = this._events;\n\t      if (!events)\n\t        return this;\n\n\t      list = events[type];\n\t      if (!list)\n\t        return this;\n\n\t      if (list === listener || (list.listener && list.listener === listener)) {\n\t        if (--this._eventsCount === 0)\n\t          this._events = new EventHandlers();\n\t        else {\n\t          delete events[type];\n\t          if (events.removeListener)\n\t            this.emit('removeListener', type, list.listener || listener);\n\t        }\n\t      } else if (typeof list !== 'function') {\n\t        position = -1;\n\n\t        for (i = list.length; i-- > 0;) {\n\t          if (list[i] === listener ||\n\t              (list[i].listener && list[i].listener === listener)) {\n\t            originalListener = list[i].listener;\n\t            position = i;\n\t            break;\n\t          }\n\t        }\n\n\t        if (position < 0)\n\t          return this;\n\n\t        if (list.length === 1) {\n\t          list[0] = undefined;\n\t          if (--this._eventsCount === 0) {\n\t            this._events = new EventHandlers();\n\t            return this;\n\t          } else {\n\t            delete events[type];\n\t          }\n\t        } else {\n\t          spliceOne(list, position);\n\t        }\n\n\t        if (events.removeListener)\n\t          this.emit('removeListener', type, originalListener || listener);\n\t      }\n\n\t      return this;\n\t    };\n\n\tEventEmitter.prototype.removeAllListeners =\n\t    function removeAllListeners(type) {\n\t      var listeners, events;\n\n\t      events = this._events;\n\t      if (!events)\n\t        return this;\n\n\t      // not listening for removeListener, no need to emit\n\t      if (!events.removeListener) {\n\t        if (arguments.length === 0) {\n\t          this._events = new EventHandlers();\n\t          this._eventsCount = 0;\n\t        } else if (events[type]) {\n\t          if (--this._eventsCount === 0)\n\t            this._events = new EventHandlers();\n\t          else\n\t            delete events[type];\n\t        }\n\t        return this;\n\t      }\n\n\t      // emit removeListener for all listeners on all events\n\t      if (arguments.length === 0) {\n\t        var keys = Object.keys(events);\n\t        for (var i = 0, key; i < keys.length; ++i) {\n\t          key = keys[i];\n\t          if (key === 'removeListener') continue;\n\t          this.removeAllListeners(key);\n\t        }\n\t        this.removeAllListeners('removeListener');\n\t        this._events = new EventHandlers();\n\t        this._eventsCount = 0;\n\t        return this;\n\t      }\n\n\t      listeners = events[type];\n\n\t      if (typeof listeners === 'function') {\n\t        this.removeListener(type, listeners);\n\t      } else if (listeners) {\n\t        // LIFO order\n\t        do {\n\t          this.removeListener(type, listeners[listeners.length - 1]);\n\t        } while (listeners[0]);\n\t      }\n\n\t      return this;\n\t    };\n\n\tEventEmitter.prototype.listeners = function listeners(type) {\n\t  var evlistener;\n\t  var ret;\n\t  var events = this._events;\n\n\t  if (!events)\n\t    ret = [];\n\t  else {\n\t    evlistener = events[type];\n\t    if (!evlistener)\n\t      ret = [];\n\t    else if (typeof evlistener === 'function')\n\t      ret = [evlistener.listener || evlistener];\n\t    else\n\t      ret = unwrapListeners(evlistener);\n\t  }\n\n\t  return ret;\n\t};\n\n\tEventEmitter.listenerCount = function(emitter, type) {\n\t  if (typeof emitter.listenerCount === 'function') {\n\t    return emitter.listenerCount(type);\n\t  } else {\n\t    return listenerCount.call(emitter, type);\n\t  }\n\t};\n\n\tEventEmitter.prototype.listenerCount = listenerCount;\n\tfunction listenerCount(type) {\n\t  var events = this._events;\n\n\t  if (events) {\n\t    var evlistener = events[type];\n\n\t    if (typeof evlistener === 'function') {\n\t      return 1;\n\t    } else if (evlistener) {\n\t      return evlistener.length;\n\t    }\n\t  }\n\n\t  return 0;\n\t}\n\n\tEventEmitter.prototype.eventNames = function eventNames() {\n\t  return this._eventsCount > 0 ? Reflect.ownKeys(this._events) : [];\n\t};\n\n\t// About 1.5x faster than the two-arg version of Array#splice().\n\tfunction spliceOne(list, index) {\n\t  for (var i = index, k = i + 1, n = list.length; k < n; i += 1, k += 1)\n\t    list[i] = list[k];\n\t  list.pop();\n\t}\n\n\tfunction arrayClone(arr, i) {\n\t  var copy = new Array(i);\n\t  while (i--)\n\t    copy[i] = arr[i];\n\t  return copy;\n\t}\n\n\tfunction unwrapListeners(arr) {\n\t  var ret = new Array(arr.length);\n\t  for (var i = 0; i < ret.length; ++i) {\n\t    ret[i] = arr[i].listener || arr[i];\n\t  }\n\t  return ret;\n\t}\n\n\tfunction mangle(key) {\n\t  return '$' + key;\n\t}\n\tfunction unmangle(key) {\n\t  return key.substring(1);\n\t}\n\tfunction Map$1() {\n\t  this._store = {};\n\t}\n\tMap$1.prototype.get = function (key) {\n\t  var mangled = mangle(key);\n\t  return this._store[mangled];\n\t};\n\tMap$1.prototype.set = function (key, value) {\n\t  var mangled = mangle(key);\n\t  this._store[mangled] = value;\n\t  return true;\n\t};\n\tMap$1.prototype.has = function (key) {\n\t  var mangled = mangle(key);\n\t  return mangled in this._store;\n\t};\n\tMap$1.prototype.delete = function (key) {\n\t  var mangled = mangle(key);\n\t  var res = mangled in this._store;\n\t  delete this._store[mangled];\n\t  return res;\n\t};\n\tMap$1.prototype.forEach = function (cb) {\n\t  var keys = Object.keys(this._store);\n\t  for (var i = 0, len = keys.length; i < len; i++) {\n\t    var key = keys[i];\n\t    var value = this._store[key];\n\t    key = unmangle(key);\n\t    cb(value, key);\n\t  }\n\t};\n\tObject.defineProperty(Map$1.prototype, 'size', {\n\t  get: function () {\n\t    return Object.keys(this._store).length;\n\t  }\n\t});\n\n\tfunction Set$1(array) {\n\t  this._store = new Map$1();\n\n\t  // init with an array\n\t  if (array && Array.isArray(array)) {\n\t    for (var i = 0, len = array.length; i < len; i++) {\n\t      this.add(array[i]);\n\t    }\n\t  }\n\t}\n\tSet$1.prototype.add = function (key) {\n\t  return this._store.set(key, true);\n\t};\n\tSet$1.prototype.has = function (key) {\n\t  return this._store.has(key);\n\t};\n\tSet$1.prototype.forEach = function (cb) {\n\t  this._store.forEach(function (value, key) {\n\t    cb(key);\n\t  });\n\t};\n\tObject.defineProperty(Set$1.prototype, 'size', {\n\t  get: function () {\n\t    return this._store.size;\n\t  }\n\t});\n\n\t/* global Map,Set,Symbol */\n\t// Based on https://kangax.github.io/compat-table/es6/ we can sniff out\n\t// incomplete Map/Set implementations which would otherwise cause our tests to fail.\n\t// Notably they fail in IE11 and iOS 8.4, which this prevents.\n\tfunction supportsMapAndSet() {\n\t  if (typeof Symbol === 'undefined' || typeof Map === 'undefined' || typeof Set === 'undefined') {\n\t    return false;\n\t  }\n\t  var prop = Object.getOwnPropertyDescriptor(Map, Symbol.species);\n\t  return prop && 'get' in prop && Map[Symbol.species] === Map;\n\t}\n\n\t// based on https://github.com/montagejs/collections\n\n\tvar ExportedSet;\n\tvar ExportedMap;\n\n\t{\n\t  if (supportsMapAndSet()) { // prefer built-in Map/Set\n\t    ExportedSet = Set;\n\t    ExportedMap = Map;\n\t  } else { // fall back to our polyfill\n\t    ExportedSet = Set$1;\n\t    ExportedMap = Map$1;\n\t  }\n\t}\n\n\tfunction isBinaryObject(object) {\n\t  return (typeof ArrayBuffer !== 'undefined' && object instanceof ArrayBuffer) ||\n\t    (typeof Blob !== 'undefined' && object instanceof Blob);\n\t}\n\n\tfunction cloneArrayBuffer(buff) {\n\t  if (typeof buff.slice === 'function') {\n\t    return buff.slice(0);\n\t  }\n\t  // IE10-11 slice() polyfill\n\t  var target = new ArrayBuffer(buff.byteLength);\n\t  var targetArray = new Uint8Array(target);\n\t  var sourceArray = new Uint8Array(buff);\n\t  targetArray.set(sourceArray);\n\t  return target;\n\t}\n\n\tfunction cloneBinaryObject(object) {\n\t  if (object instanceof ArrayBuffer) {\n\t    return cloneArrayBuffer(object);\n\t  }\n\t  var size = object.size;\n\t  var type = object.type;\n\t  // Blob\n\t  if (typeof object.slice === 'function') {\n\t    return object.slice(0, size, type);\n\t  }\n\t  // PhantomJS slice() replacement\n\t  return object.webkitSlice(0, size, type);\n\t}\n\n\t// most of this is borrowed from lodash.isPlainObject:\n\t// https://github.com/fis-components/lodash.isplainobject/\n\t// blob/29c358140a74f252aeb08c9eb28bef86f2217d4a/index.js\n\n\tvar funcToString = Function.prototype.toString;\n\tvar objectCtorString = funcToString.call(Object);\n\n\tfunction isPlainObject(value) {\n\t  var proto = Object.getPrototypeOf(value);\n\t  /* istanbul ignore if */\n\t  if (proto === null) { // not sure when this happens, but I guess it can\n\t    return true;\n\t  }\n\t  var Ctor = proto.constructor;\n\t  return (typeof Ctor == 'function' &&\n\t    Ctor instanceof Ctor && funcToString.call(Ctor) == objectCtorString);\n\t}\n\n\tfunction clone(object) {\n\t  var newObject;\n\t  var i;\n\t  var len;\n\n\t  if (!object || typeof object !== 'object') {\n\t    return object;\n\t  }\n\n\t  if (Array.isArray(object)) {\n\t    newObject = [];\n\t    for (i = 0, len = object.length; i < len; i++) {\n\t      newObject[i] = clone(object[i]);\n\t    }\n\t    return newObject;\n\t  }\n\n\t  // special case: to avoid inconsistencies between IndexedDB\n\t  // and other backends, we automatically stringify Dates\n\t  if (object instanceof Date) {\n\t    return object.toISOString();\n\t  }\n\n\t  if (isBinaryObject(object)) {\n\t    return cloneBinaryObject(object);\n\t  }\n\n\t  if (!isPlainObject(object)) {\n\t    return object; // don't clone objects like Workers\n\t  }\n\n\t  newObject = {};\n\t  for (i in object) {\n\t    /* istanbul ignore else */\n\t    if (Object.prototype.hasOwnProperty.call(object, i)) {\n\t      var value = clone(object[i]);\n\t      if (typeof value !== 'undefined') {\n\t        newObject[i] = value;\n\t      }\n\t    }\n\t  }\n\t  return newObject;\n\t}\n\n\tfunction once(fun) {\n\t  var called = false;\n\t  return argsarray(function (args) {\n\t    /* istanbul ignore if */\n\t    if (called) {\n\t      // this is a smoke test and should never actually happen\n\t      throw new Error('once called more than once');\n\t    } else {\n\t      called = true;\n\t      fun.apply(this, args);\n\t    }\n\t  });\n\t}\n\n\tfunction toPromise(func) {\n\t  //create the function we will be returning\n\t  return argsarray(function (args) {\n\t    // Clone arguments\n\t    args = clone(args);\n\t    var self = this;\n\t    // if the last argument is a function, assume its a callback\n\t    var usedCB = (typeof args[args.length - 1] === 'function') ? args.pop() : false;\n\t    var promise = new Promise(function (fulfill, reject) {\n\t      var resp;\n\t      try {\n\t        var callback = once(function (err, mesg) {\n\t          if (err) {\n\t            reject(err);\n\t          } else {\n\t            fulfill(mesg);\n\t          }\n\t        });\n\t        // create a callback for this invocation\n\t        // apply the function in the orig context\n\t        args.push(callback);\n\t        resp = func.apply(self, args);\n\t        if (resp && typeof resp.then === 'function') {\n\t          fulfill(resp);\n\t        }\n\t      } catch (e) {\n\t        reject(e);\n\t      }\n\t    });\n\t    // if there is a callback, call it back\n\t    if (usedCB) {\n\t      promise.then(function (result) {\n\t        usedCB(null, result);\n\t      }, usedCB);\n\t    }\n\t    return promise;\n\t  });\n\t}\n\n\tfunction logApiCall(self, name, args) {\n\t  /* istanbul ignore if */\n\t  if (self.constructor.listeners('debug').length) {\n\t    var logArgs = ['api', self.name, name];\n\t    for (var i = 0; i < args.length - 1; i++) {\n\t      logArgs.push(args[i]);\n\t    }\n\t    self.constructor.emit('debug', logArgs);\n\n\t    // override the callback itself to log the response\n\t    var origCallback = args[args.length - 1];\n\t    args[args.length - 1] = function (err, res) {\n\t      var responseArgs = ['api', self.name, name];\n\t      responseArgs = responseArgs.concat(\n\t        err ? ['error', err] : ['success', res]\n\t      );\n\t      self.constructor.emit('debug', responseArgs);\n\t      origCallback(err, res);\n\t    };\n\t  }\n\t}\n\n\tfunction adapterFun(name, callback) {\n\t  return toPromise(argsarray(function (args) {\n\t    if (this._closed) {\n\t      return Promise.reject(new Error('database is closed'));\n\t    }\n\t    if (this._destroyed) {\n\t      return Promise.reject(new Error('database is destroyed'));\n\t    }\n\t    var self = this;\n\t    logApiCall(self, name, args);\n\t    if (!this.taskqueue.isReady) {\n\t      return new Promise(function (fulfill, reject) {\n\t        self.taskqueue.addTask(function (failed) {\n\t          if (failed) {\n\t            reject(failed);\n\t          } else {\n\t            fulfill(self[name].apply(self, args));\n\t          }\n\t        });\n\t      });\n\t    }\n\t    return callback.apply(this, args);\n\t  }));\n\t}\n\n\t// like underscore/lodash _.pick()\n\tfunction pick(obj, arr) {\n\t  var res = {};\n\t  for (var i = 0, len = arr.length; i < len; i++) {\n\t    var prop = arr[i];\n\t    if (prop in obj) {\n\t      res[prop] = obj[prop];\n\t    }\n\t  }\n\t  return res;\n\t}\n\n\t// Most browsers throttle concurrent requests at 6, so it's silly\n\t// to shim _bulk_get by trying to launch potentially hundreds of requests\n\t// and then letting the majority time out. We can handle this ourselves.\n\tvar MAX_NUM_CONCURRENT_REQUESTS = 6;\n\n\tfunction identityFunction(x) {\n\t  return x;\n\t}\n\n\tfunction formatResultForOpenRevsGet(result) {\n\t  return [{\n\t    ok: result\n\t  }];\n\t}\n\n\t// shim for P/CouchDB adapters that don't directly implement _bulk_get\n\tfunction bulkGet(db, opts, callback) {\n\t  var requests = opts.docs;\n\n\t  // consolidate into one request per doc if possible\n\t  var requestsById = new ExportedMap();\n\t  requests.forEach(function (request) {\n\t    if (requestsById.has(request.id)) {\n\t      requestsById.get(request.id).push(request);\n\t    } else {\n\t      requestsById.set(request.id, [request]);\n\t    }\n\t  });\n\n\t  var numDocs = requestsById.size;\n\t  var numDone = 0;\n\t  var perDocResults = new Array(numDocs);\n\n\t  function collapseResultsAndFinish() {\n\t    var results = [];\n\t    perDocResults.forEach(function (res) {\n\t      res.docs.forEach(function (info) {\n\t        results.push({\n\t          id: res.id,\n\t          docs: [info]\n\t        });\n\t      });\n\t    });\n\t    callback(null, {results: results});\n\t  }\n\n\t  function checkDone() {\n\t    if (++numDone === numDocs) {\n\t      collapseResultsAndFinish();\n\t    }\n\t  }\n\n\t  function gotResult(docIndex, id, docs) {\n\t    perDocResults[docIndex] = {id: id, docs: docs};\n\t    checkDone();\n\t  }\n\n\t  var allRequests = [];\n\t  requestsById.forEach(function (value, key) {\n\t    allRequests.push(key);\n\t  });\n\n\t  var i = 0;\n\n\t  function nextBatch() {\n\n\t    if (i >= allRequests.length) {\n\t      return;\n\t    }\n\n\t    var upTo = Math.min(i + MAX_NUM_CONCURRENT_REQUESTS, allRequests.length);\n\t    var batch = allRequests.slice(i, upTo);\n\t    processBatch(batch, i);\n\t    i += batch.length;\n\t  }\n\n\t  function processBatch(batch, offset) {\n\t    batch.forEach(function (docId, j) {\n\t      var docIdx = offset + j;\n\t      var docRequests = requestsById.get(docId);\n\n\t      // just use the first request as the \"template\"\n\t      // TODO: The _bulk_get API allows for more subtle use cases than this,\n\t      // but for now it is unlikely that there will be a mix of different\n\t      // \"atts_since\" or \"attachments\" in the same request, since it's just\n\t      // replicate.js that is using this for the moment.\n\t      // Also, atts_since is aspirational, since we don't support it yet.\n\t      var docOpts = pick(docRequests[0], ['atts_since', 'attachments']);\n\t      docOpts.open_revs = docRequests.map(function (request) {\n\t        // rev is optional, open_revs disallowed\n\t        return request.rev;\n\t      });\n\n\t      // remove falsey / undefined revisions\n\t      docOpts.open_revs = docOpts.open_revs.filter(identityFunction);\n\n\t      var formatResult = identityFunction;\n\n\t      if (docOpts.open_revs.length === 0) {\n\t        delete docOpts.open_revs;\n\n\t        // when fetching only the \"winning\" leaf,\n\t        // transform the result so it looks like an open_revs\n\t        // request\n\t        formatResult = formatResultForOpenRevsGet;\n\t      }\n\n\t      // globally-supplied options\n\t      ['revs', 'attachments', 'binary', 'ajax', 'latest'].forEach(function (param) {\n\t        if (param in opts) {\n\t          docOpts[param] = opts[param];\n\t        }\n\t      });\n\t      db.get(docId, docOpts, function (err, res) {\n\t        var result;\n\t        /* istanbul ignore if */\n\t        if (err) {\n\t          result = [{error: err}];\n\t        } else {\n\t          result = formatResult(res);\n\t        }\n\t        gotResult(docIdx, docId, result);\n\t        nextBatch();\n\t      });\n\t    });\n\t  }\n\n\t  nextBatch();\n\n\t}\n\n\tvar hasLocal;\n\n\ttry {\n\t  localStorage.setItem('_pouch_check_localstorage', 1);\n\t  hasLocal = !!localStorage.getItem('_pouch_check_localstorage');\n\t} catch (e) {\n\t  hasLocal = false;\n\t}\n\n\tfunction hasLocalStorage() {\n\t  return hasLocal;\n\t}\n\n\t// Custom nextTick() shim for browsers. In node, this will just be process.nextTick(). We\n\n\tinherits$2(Changes, EventEmitter);\n\n\t/* istanbul ignore next */\n\tfunction attachBrowserEvents(self) {\n\t  if (hasLocalStorage()) {\n\t    addEventListener(\"storage\", function (e) {\n\t      self.emit(e.key);\n\t    });\n\t  }\n\t}\n\n\tfunction Changes() {\n\t  EventEmitter.call(this);\n\t  this._listeners = {};\n\n\t  attachBrowserEvents(this);\n\t}\n\tChanges.prototype.addListener = function (dbName, id, db, opts) {\n\t  /* istanbul ignore if */\n\t  if (this._listeners[id]) {\n\t    return;\n\t  }\n\t  var self = this;\n\t  var inprogress = false;\n\t  function eventFunction() {\n\t    /* istanbul ignore if */\n\t    if (!self._listeners[id]) {\n\t      return;\n\t    }\n\t    if (inprogress) {\n\t      inprogress = 'waiting';\n\t      return;\n\t    }\n\t    inprogress = true;\n\t    var changesOpts = pick(opts, [\n\t      'style', 'include_docs', 'attachments', 'conflicts', 'filter',\n\t      'doc_ids', 'view', 'since', 'query_params', 'binary', 'return_docs'\n\t    ]);\n\n\t    /* istanbul ignore next */\n\t    function onError() {\n\t      inprogress = false;\n\t    }\n\n\t    db.changes(changesOpts).on('change', function (c) {\n\t      if (c.seq > opts.since && !opts.cancelled) {\n\t        opts.since = c.seq;\n\t        opts.onChange(c);\n\t      }\n\t    }).on('complete', function () {\n\t      if (inprogress === 'waiting') {\n\t        lib(eventFunction);\n\t      }\n\t      inprogress = false;\n\t    }).on('error', onError);\n\t  }\n\t  this._listeners[id] = eventFunction;\n\t  this.on(dbName, eventFunction);\n\t};\n\n\tChanges.prototype.removeListener = function (dbName, id) {\n\t  /* istanbul ignore if */\n\t  if (!(id in this._listeners)) {\n\t    return;\n\t  }\n\t  EventEmitter.prototype.removeListener.call(this, dbName,\n\t    this._listeners[id]);\n\t  delete this._listeners[id];\n\t};\n\n\n\t/* istanbul ignore next */\n\tChanges.prototype.notifyLocalWindows = function (dbName) {\n\t  //do a useless change on a storage thing\n\t  //in order to get other windows's listeners to activate\n\t  if (hasLocalStorage()) {\n\t    localStorage[dbName] = (localStorage[dbName] === \"a\") ? \"b\" : \"a\";\n\t  }\n\t};\n\n\tChanges.prototype.notify = function (dbName) {\n\t  this.emit(dbName);\n\t  this.notifyLocalWindows(dbName);\n\t};\n\n\tfunction guardedConsole(method) {\n\t  /* istanbul ignore else */\n\t  if (typeof console !== 'undefined' && typeof console[method] === 'function') {\n\t    var args = Array.prototype.slice.call(arguments, 1);\n\t    console[method].apply(console, args);\n\t  }\n\t}\n\n\tfunction randomNumber(min, max) {\n\t  var maxTimeout = 600000; // Hard-coded default of 10 minutes\n\t  min = parseInt(min, 10) || 0;\n\t  max = parseInt(max, 10);\n\t  if (max !== max || max <= min) {\n\t    max = (min || 1) << 1; //doubling\n\t  } else {\n\t    max = max + 1;\n\t  }\n\t  // In order to not exceed maxTimeout, pick a random value between half of maxTimeout and maxTimeout\n\t  if (max > maxTimeout) {\n\t    min = maxTimeout >> 1; // divide by two\n\t    max = maxTimeout;\n\t  }\n\t  var ratio = Math.random();\n\t  var range = max - min;\n\n\t  return ~~(range * ratio + min); // ~~ coerces to an int, but fast.\n\t}\n\n\tfunction defaultBackOff(min) {\n\t  var max = 0;\n\t  if (!min) {\n\t    max = 2000;\n\t  }\n\t  return randomNumber(min, max);\n\t}\n\n\t// designed to give info to browser users, who are disturbed\n\t// when they see http errors in the console\n\tfunction explainError(status, str) {\n\t  guardedConsole('info', 'The above ' + status + ' is totally normal. ' + str);\n\t}\n\n\tvar assign;\n\t{\n\t  if (typeof Object.assign === 'function') {\n\t    assign = Object.assign;\n\t  } else {\n\t    // lite Object.assign polyfill based on\n\t    // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign\n\t    assign = function (target) {\n\t      var to = Object(target);\n\n\t      for (var index = 1; index < arguments.length; index++) {\n\t        var nextSource = arguments[index];\n\n\t        if (nextSource != null) { // Skip over if undefined or null\n\t          for (var nextKey in nextSource) {\n\t            // Avoid bugs when hasOwnProperty is shadowed\n\t            if (Object.prototype.hasOwnProperty.call(nextSource, nextKey)) {\n\t              to[nextKey] = nextSource[nextKey];\n\t            }\n\t          }\n\t        }\n\t      }\n\t      return to;\n\t    };\n\t  }\n\t}\n\n\tvar $inject_Object_assign = assign;\n\n\tinherits$2(PouchError, Error);\n\n\tfunction PouchError(status, error, reason) {\n\t  Error.call(this, reason);\n\t  this.status = status;\n\t  this.name = error;\n\t  this.message = reason;\n\t  this.error = true;\n\t}\n\n\tPouchError.prototype.toString = function () {\n\t  return JSON.stringify({\n\t    status: this.status,\n\t    name: this.name,\n\t    message: this.message,\n\t    reason: this.reason\n\t  });\n\t};\n\n\tvar UNAUTHORIZED = new PouchError(401, 'unauthorized', \"Name or password is incorrect.\");\n\tvar MISSING_BULK_DOCS = new PouchError(400, 'bad_request', \"Missing JSON list of 'docs'\");\n\tvar MISSING_DOC = new PouchError(404, 'not_found', 'missing');\n\tvar REV_CONFLICT = new PouchError(409, 'conflict', 'Document update conflict');\n\tvar INVALID_ID = new PouchError(400, 'bad_request', '_id field must contain a string');\n\tvar MISSING_ID = new PouchError(412, 'missing_id', '_id is required for puts');\n\tvar RESERVED_ID = new PouchError(400, 'bad_request', 'Only reserved document ids may start with underscore.');\n\tvar NOT_OPEN = new PouchError(412, 'precondition_failed', 'Database not open');\n\tvar UNKNOWN_ERROR = new PouchError(500, 'unknown_error', 'Database encountered an unknown error');\n\tvar BAD_ARG = new PouchError(500, 'badarg', 'Some query argument is invalid');\n\tvar INVALID_REQUEST = new PouchError(400, 'invalid_request', 'Request was invalid');\n\tvar QUERY_PARSE_ERROR = new PouchError(400, 'query_parse_error', 'Some query parameter is invalid');\n\tvar DOC_VALIDATION = new PouchError(500, 'doc_validation', 'Bad special document member');\n\tvar BAD_REQUEST = new PouchError(400, 'bad_request', 'Something wrong with the request');\n\tvar NOT_AN_OBJECT = new PouchError(400, 'bad_request', 'Document must be a JSON object');\n\tvar DB_MISSING = new PouchError(404, 'not_found', 'Database not found');\n\tvar IDB_ERROR = new PouchError(500, 'indexed_db_went_bad', 'unknown');\n\tvar WSQ_ERROR = new PouchError(500, 'web_sql_went_bad', 'unknown');\n\tvar LDB_ERROR = new PouchError(500, 'levelDB_went_went_bad', 'unknown');\n\tvar FORBIDDEN = new PouchError(403, 'forbidden', 'Forbidden by design doc validate_doc_update function');\n\tvar INVALID_REV = new PouchError(400, 'bad_request', 'Invalid rev format');\n\tvar FILE_EXISTS = new PouchError(412, 'file_exists', 'The database could not be created, the file already exists.');\n\tvar MISSING_STUB = new PouchError(412, 'missing_stub', 'A pre-existing attachment stub wasn\\'t found');\n\tvar INVALID_URL = new PouchError(413, 'invalid_url', 'Provided URL is invalid');\n\n\tfunction createError(error, reason) {\n\t  function CustomPouchError(reason) {\n\t    // inherit error properties from our parent error manually\n\t    // so as to allow proper JSON parsing.\n\t    /* jshint ignore:start */\n\t    for (var p in error) {\n\t      if (typeof error[p] !== 'function') {\n\t        this[p] = error[p];\n\t      }\n\t    }\n\t    /* jshint ignore:end */\n\t    if (reason !== undefined) {\n\t      this.reason = reason;\n\t    }\n\t  }\n\t  CustomPouchError.prototype = PouchError.prototype;\n\t  return new CustomPouchError(reason);\n\t}\n\n\tfunction generateErrorFromResponse(err) {\n\n\t  if (typeof err !== 'object') {\n\t    var data = err;\n\t    err = UNKNOWN_ERROR;\n\t    err.data = data;\n\t  }\n\n\t  if ('error' in err && err.error === 'conflict') {\n\t    err.name = 'conflict';\n\t    err.status = 409;\n\t  }\n\n\t  if (!('name' in err)) {\n\t    err.name = err.error || 'unknown';\n\t  }\n\n\t  if (!('status' in err)) {\n\t    err.status = 500;\n\t  }\n\n\t  if (!('message' in err)) {\n\t    err.message = err.message || err.reason;\n\t  }\n\n\t  return err;\n\t}\n\n\tfunction tryFilter(filter, doc, req) {\n\t  try {\n\t    return !filter(doc, req);\n\t  } catch (err) {\n\t    var msg = 'Filter function threw: ' + err.toString();\n\t    return createError(BAD_REQUEST, msg);\n\t  }\n\t}\n\n\tfunction filterChange(opts) {\n\t  var req = {};\n\t  var hasFilter = opts.filter && typeof opts.filter === 'function';\n\t  req.query = opts.query_params;\n\n\t  return function filter(change) {\n\t    if (!change.doc) {\n\t      // CSG sends events on the changes feed that don't have documents,\n\t      // this hack makes a whole lot of existing code robust.\n\t      change.doc = {};\n\t    }\n\n\t    var filterReturn = hasFilter && tryFilter(opts.filter, change.doc, req);\n\n\t    if (typeof filterReturn === 'object') {\n\t      return filterReturn;\n\t    }\n\n\t    if (filterReturn) {\n\t      return false;\n\t    }\n\n\t    if (!opts.include_docs) {\n\t      delete change.doc;\n\t    } else if (!opts.attachments) {\n\t      for (var att in change.doc._attachments) {\n\t        /* istanbul ignore else */\n\t        if (change.doc._attachments.hasOwnProperty(att)) {\n\t          change.doc._attachments[att].stub = true;\n\t        }\n\t      }\n\t    }\n\t    return true;\n\t  };\n\t}\n\n\tfunction flatten(arrs) {\n\t  var res = [];\n\t  for (var i = 0, len = arrs.length; i < len; i++) {\n\t    res = res.concat(arrs[i]);\n\t  }\n\t  return res;\n\t}\n\n\t// shim for Function.prototype.name,\n\n\t// Determine id an ID is valid\n\t//   - invalid IDs begin with an underescore that does not begin '_design' or\n\t//     '_local'\n\t//   - any other string value is a valid id\n\t// Returns the specific error object for each case\n\tfunction invalidIdError(id) {\n\t  var err;\n\t  if (!id) {\n\t    err = createError(MISSING_ID);\n\t  } else if (typeof id !== 'string') {\n\t    err = createError(INVALID_ID);\n\t  } else if (/^_/.test(id) && !(/^_(design|local)/).test(id)) {\n\t    err = createError(RESERVED_ID);\n\t  }\n\t  if (err) {\n\t    throw err;\n\t  }\n\t}\n\n\t// Checks if a PouchDB object is \"remote\" or not. This is\n\n\tfunction isRemote(db) {\n\t  if (typeof db._remote === 'boolean') {\n\t    return db._remote;\n\t  }\n\t  /* istanbul ignore next */\n\t  if (typeof db.type === 'function') {\n\t    guardedConsole('warn',\n\t      'db.type() is deprecated and will be removed in ' +\n\t      'a future version of PouchDB');\n\t    return db.type() === 'http';\n\t  }\n\t  /* istanbul ignore next */\n\t  return false;\n\t}\n\n\tfunction listenerCount$1(ee, type) {\n\t  return 'listenerCount' in ee ? ee.listenerCount(type) :\n\t                                 EventEmitter.listenerCount(ee, type);\n\t}\n\n\tfunction parseDesignDocFunctionName(s) {\n\t  if (!s) {\n\t    return null;\n\t  }\n\t  var parts = s.split('/');\n\t  if (parts.length === 2) {\n\t    return parts;\n\t  }\n\t  if (parts.length === 1) {\n\t    return [s, s];\n\t  }\n\t  return null;\n\t}\n\n\tfunction normalizeDesignDocFunctionName(s) {\n\t  var normalized = parseDesignDocFunctionName(s);\n\t  return normalized ? normalized.join('/') : null;\n\t}\n\n\t// originally parseUri 1.2.2, now patched by us\n\t// (c) Steven Levithan <stevenlevithan.com>\n\t// MIT License\n\tvar keys = [\"source\", \"protocol\", \"authority\", \"userInfo\", \"user\", \"password\",\n\t    \"host\", \"port\", \"relative\", \"path\", \"directory\", \"file\", \"query\", \"anchor\"];\n\tvar qName =\"queryKey\";\n\tvar qParser = /(?:^|&)([^&=]*)=?([^&]*)/g;\n\n\t// use the \"loose\" parser\n\t/* eslint maxlen: 0, no-useless-escape: 0 */\n\tvar parser = /^(?:(?![^:@]+:[^:@\\/]*@)([^:\\/?#.]+):)?(?:\\/\\/)?((?:(([^:@]*)(?::([^:@]*))?)?@)?([^:\\/?#]*)(?::(\\d*))?)(((\\/(?:[^?#](?![^?#\\/]*\\.[^?#\\/.]+(?:[?#]|$)))*\\/?)?([^?#\\/]*))(?:\\?([^#]*))?(?:#(.*))?)/;\n\n\tfunction parseUri(str) {\n\t  var m = parser.exec(str);\n\t  var uri = {};\n\t  var i = 14;\n\n\t  while (i--) {\n\t    var key = keys[i];\n\t    var value = m[i] || \"\";\n\t    var encoded = ['user', 'password'].indexOf(key) !== -1;\n\t    uri[key] = encoded ? decodeURIComponent(value) : value;\n\t  }\n\n\t  uri[qName] = {};\n\t  uri[keys[12]].replace(qParser, function ($0, $1, $2) {\n\t    if ($1) {\n\t      uri[qName][$1] = $2;\n\t    }\n\t  });\n\n\t  return uri;\n\t}\n\n\t// Based on https://github.com/alexdavid/scope-eval v0.0.3\n\t// (source: https://unpkg.com/scope-eval@0.0.3/scope_eval.js)\n\t// This is basically just a wrapper around new Function()\n\n\tfunction scopeEval(source, scope) {\n\t  var keys = [];\n\t  var values = [];\n\t  for (var key in scope) {\n\t    if (scope.hasOwnProperty(key)) {\n\t      keys.push(key);\n\t      values.push(scope[key]);\n\t    }\n\t  }\n\t  keys.push(source);\n\t  return Function.apply(null, keys).apply(null, values);\n\t}\n\n\t// this is essentially the \"update sugar\" function from daleharvey/pouchdb#1388\n\t// the diffFun tells us what delta to apply to the doc.  it either returns\n\t// the doc, or false if it doesn't need to do an update after all\n\tfunction upsert(db, docId, diffFun) {\n\t  return new Promise(function (fulfill, reject) {\n\t    db.get(docId, function (err, doc) {\n\t      if (err) {\n\t        /* istanbul ignore next */\n\t        if (err.status !== 404) {\n\t          return reject(err);\n\t        }\n\t        doc = {};\n\t      }\n\n\t      // the user might change the _rev, so save it for posterity\n\t      var docRev = doc._rev;\n\t      var newDoc = diffFun(doc);\n\n\t      if (!newDoc) {\n\t        // if the diffFun returns falsy, we short-circuit as\n\t        // an optimization\n\t        return fulfill({updated: false, rev: docRev});\n\t      }\n\n\t      // users aren't allowed to modify these values,\n\t      // so reset them here\n\t      newDoc._id = docId;\n\t      newDoc._rev = docRev;\n\t      fulfill(tryAndPut(db, newDoc, diffFun));\n\t    });\n\t  });\n\t}\n\n\tfunction tryAndPut(db, doc, diffFun) {\n\t  return db.put(doc).then(function (res) {\n\t    return {\n\t      updated: true,\n\t      rev: res.rev\n\t    };\n\t  }, function (err) {\n\t    /* istanbul ignore next */\n\t    if (err.status !== 409) {\n\t      throw err;\n\t    }\n\t    return upsert(db, doc._id, diffFun);\n\t  });\n\t}\n\n\tvar thisAtob = function (str) {\n\t  return atob(str);\n\t};\n\n\tvar thisBtoa = function (str) {\n\t  return btoa(str);\n\t};\n\n\t// Abstracts constructing a Blob object, so it also works in older\n\t// browsers that don't support the native Blob constructor (e.g.\n\t// old QtWebKit versions, Android < 4.4).\n\tfunction createBlob(parts, properties) {\n\t  /* global BlobBuilder,MSBlobBuilder,MozBlobBuilder,WebKitBlobBuilder */\n\t  parts = parts || [];\n\t  properties = properties || {};\n\t  try {\n\t    return new Blob(parts, properties);\n\t  } catch (e) {\n\t    if (e.name !== \"TypeError\") {\n\t      throw e;\n\t    }\n\t    var Builder = typeof BlobBuilder !== 'undefined' ? BlobBuilder :\n\t                  typeof MSBlobBuilder !== 'undefined' ? MSBlobBuilder :\n\t                  typeof MozBlobBuilder !== 'undefined' ? MozBlobBuilder :\n\t                  WebKitBlobBuilder;\n\t    var builder = new Builder();\n\t    for (var i = 0; i < parts.length; i += 1) {\n\t      builder.append(parts[i]);\n\t    }\n\t    return builder.getBlob(properties.type);\n\t  }\n\t}\n\n\t// From http://stackoverflow.com/questions/14967647/ (continues on next line)\n\t// encode-decode-image-with-base64-breaks-image (2013-04-21)\n\tfunction binaryStringToArrayBuffer(bin) {\n\t  var length = bin.length;\n\t  var buf = new ArrayBuffer(length);\n\t  var arr = new Uint8Array(buf);\n\t  for (var i = 0; i < length; i++) {\n\t    arr[i] = bin.charCodeAt(i);\n\t  }\n\t  return buf;\n\t}\n\n\tfunction binStringToBluffer(binString, type) {\n\t  return createBlob([binaryStringToArrayBuffer(binString)], {type: type});\n\t}\n\n\tfunction b64ToBluffer(b64, type) {\n\t  return binStringToBluffer(thisAtob(b64), type);\n\t}\n\n\t//Can't find original post, but this is close\n\t//http://stackoverflow.com/questions/6965107/ (continues on next line)\n\t//converting-between-strings-and-arraybuffers\n\tfunction arrayBufferToBinaryString(buffer) {\n\t  var binary = '';\n\t  var bytes = new Uint8Array(buffer);\n\t  var length = bytes.byteLength;\n\t  for (var i = 0; i < length; i++) {\n\t    binary += String.fromCharCode(bytes[i]);\n\t  }\n\t  return binary;\n\t}\n\n\t// shim for browsers that don't support it\n\tfunction readAsBinaryString(blob, callback) {\n\t  var reader = new FileReader();\n\t  var hasBinaryString = typeof reader.readAsBinaryString === 'function';\n\t  reader.onloadend = function (e) {\n\t    var result = e.target.result || '';\n\t    if (hasBinaryString) {\n\t      return callback(result);\n\t    }\n\t    callback(arrayBufferToBinaryString(result));\n\t  };\n\t  if (hasBinaryString) {\n\t    reader.readAsBinaryString(blob);\n\t  } else {\n\t    reader.readAsArrayBuffer(blob);\n\t  }\n\t}\n\n\tfunction blobToBinaryString(blobOrBuffer, callback) {\n\t  readAsBinaryString(blobOrBuffer, function (bin) {\n\t    callback(bin);\n\t  });\n\t}\n\n\tfunction blobToBase64(blobOrBuffer, callback) {\n\t  blobToBinaryString(blobOrBuffer, function (base64) {\n\t    callback(thisBtoa(base64));\n\t  });\n\t}\n\n\t// simplified API. universal browser support is assumed\n\tfunction readAsArrayBuffer(blob, callback) {\n\t  var reader = new FileReader();\n\t  reader.onloadend = function (e) {\n\t    var result = e.target.result || new ArrayBuffer(0);\n\t    callback(result);\n\t  };\n\t  reader.readAsArrayBuffer(blob);\n\t}\n\n\t// this is not used in the browser\n\n\tvar setImmediateShim = global.setImmediate || global.setTimeout;\n\tvar MD5_CHUNK_SIZE = 32768;\n\n\tfunction rawToBase64(raw) {\n\t  return thisBtoa(raw);\n\t}\n\n\tfunction sliceBlob(blob, start, end) {\n\t  if (blob.webkitSlice) {\n\t    return blob.webkitSlice(start, end);\n\t  }\n\t  return blob.slice(start, end);\n\t}\n\n\tfunction appendBlob(buffer, blob, start, end, callback) {\n\t  if (start > 0 || end < blob.size) {\n\t    // only slice blob if we really need to\n\t    blob = sliceBlob(blob, start, end);\n\t  }\n\t  readAsArrayBuffer(blob, function (arrayBuffer) {\n\t    buffer.append(arrayBuffer);\n\t    callback();\n\t  });\n\t}\n\n\tfunction appendString(buffer, string, start, end, callback) {\n\t  if (start > 0 || end < string.length) {\n\t    // only create a substring if we really need to\n\t    string = string.substring(start, end);\n\t  }\n\t  buffer.appendBinary(string);\n\t  callback();\n\t}\n\n\tfunction binaryMd5(data, callback) {\n\t  var inputIsString = typeof data === 'string';\n\t  var len = inputIsString ? data.length : data.size;\n\t  var chunkSize = Math.min(MD5_CHUNK_SIZE, len);\n\t  var chunks = Math.ceil(len / chunkSize);\n\t  var currentChunk = 0;\n\t  var buffer = inputIsString ? new sparkMd5() : new sparkMd5.ArrayBuffer();\n\n\t  var append = inputIsString ? appendString : appendBlob;\n\n\t  function next() {\n\t    setImmediateShim(loadNextChunk);\n\t  }\n\n\t  function done() {\n\t    var raw = buffer.end(true);\n\t    var base64 = rawToBase64(raw);\n\t    callback(base64);\n\t    buffer.destroy();\n\t  }\n\n\t  function loadNextChunk() {\n\t    var start = currentChunk * chunkSize;\n\t    var end = start + chunkSize;\n\t    currentChunk++;\n\t    if (currentChunk < chunks) {\n\t      append(buffer, data, start, end, next);\n\t    } else {\n\t      append(buffer, data, start, end, done);\n\t    }\n\t  }\n\t  loadNextChunk();\n\t}\n\n\tfunction stringMd5(string) {\n\t  return sparkMd5.hash(string);\n\t}\n\n\tfunction rev(doc, deterministic_revs) {\n\t  var clonedDoc = clone(doc);\n\t  if (!deterministic_revs) {\n\t    return uuid_1.v4().replace(/-/g, '').toLowerCase();\n\t  }\n\n\t  delete clonedDoc._rev_tree;\n\t  return stringMd5(JSON.stringify(clonedDoc));\n\t}\n\n\tvar uuid$1 = uuid_1.v4;\n\n\t// We fetch all leafs of the revision tree, and sort them based on tree length\n\t// and whether they were deleted, undeleted documents with the longest revision\n\t// tree (most edits) win\n\t// The final sort algorithm is slightly documented in a sidebar here:\n\t// http://guide.couchdb.org/draft/conflicts.html\n\tfunction winningRev(metadata) {\n\t  var winningId;\n\t  var winningPos;\n\t  var winningDeleted;\n\t  var toVisit = metadata.rev_tree.slice();\n\t  var node;\n\t  while ((node = toVisit.pop())) {\n\t    var tree = node.ids;\n\t    var branches = tree[2];\n\t    var pos = node.pos;\n\t    if (branches.length) { // non-leaf\n\t      for (var i = 0, len = branches.length; i < len; i++) {\n\t        toVisit.push({pos: pos + 1, ids: branches[i]});\n\t      }\n\t      continue;\n\t    }\n\t    var deleted = !!tree[1].deleted;\n\t    var id = tree[0];\n\t    // sort by deleted, then pos, then id\n\t    if (!winningId || (winningDeleted !== deleted ? winningDeleted :\n\t        winningPos !== pos ? winningPos < pos : winningId < id)) {\n\t      winningId = id;\n\t      winningPos = pos;\n\t      winningDeleted = deleted;\n\t    }\n\t  }\n\n\t  return winningPos + '-' + winningId;\n\t}\n\n\t// Pretty much all below can be combined into a higher order function to\n\t// traverse revisions\n\t// The return value from the callback will be passed as context to all\n\t// children of that node\n\tfunction traverseRevTree(revs, callback) {\n\t  var toVisit = revs.slice();\n\n\t  var node;\n\t  while ((node = toVisit.pop())) {\n\t    var pos = node.pos;\n\t    var tree = node.ids;\n\t    var branches = tree[2];\n\t    var newCtx =\n\t      callback(branches.length === 0, pos, tree[0], node.ctx, tree[1]);\n\t    for (var i = 0, len = branches.length; i < len; i++) {\n\t      toVisit.push({pos: pos + 1, ids: branches[i], ctx: newCtx});\n\t    }\n\t  }\n\t}\n\n\tfunction sortByPos(a, b) {\n\t  return a.pos - b.pos;\n\t}\n\n\tfunction collectLeaves(revs) {\n\t  var leaves = [];\n\t  traverseRevTree(revs, function (isLeaf, pos, id, acc, opts) {\n\t    if (isLeaf) {\n\t      leaves.push({rev: pos + \"-\" + id, pos: pos, opts: opts});\n\t    }\n\t  });\n\t  leaves.sort(sortByPos).reverse();\n\t  for (var i = 0, len = leaves.length; i < len; i++) {\n\t    delete leaves[i].pos;\n\t  }\n\t  return leaves;\n\t}\n\n\t// returns revs of all conflicts that is leaves such that\n\t// 1. are not deleted and\n\t// 2. are different than winning revision\n\tfunction collectConflicts(metadata) {\n\t  var win = winningRev(metadata);\n\t  var leaves = collectLeaves(metadata.rev_tree);\n\t  var conflicts = [];\n\t  for (var i = 0, len = leaves.length; i < len; i++) {\n\t    var leaf = leaves[i];\n\t    if (leaf.rev !== win && !leaf.opts.deleted) {\n\t      conflicts.push(leaf.rev);\n\t    }\n\t  }\n\t  return conflicts;\n\t}\n\n\t// compact a tree by marking its non-leafs as missing,\n\t// and return a list of revs to delete\n\tfunction compactTree(metadata) {\n\t  var revs = [];\n\t  traverseRevTree(metadata.rev_tree, function (isLeaf, pos,\n\t                                               revHash, ctx, opts) {\n\t    if (opts.status === 'available' && !isLeaf) {\n\t      revs.push(pos + '-' + revHash);\n\t      opts.status = 'missing';\n\t    }\n\t  });\n\t  return revs;\n\t}\n\n\t// build up a list of all the paths to the leafs in this revision tree\n\tfunction rootToLeaf(revs) {\n\t  var paths = [];\n\t  var toVisit = revs.slice();\n\t  var node;\n\t  while ((node = toVisit.pop())) {\n\t    var pos = node.pos;\n\t    var tree = node.ids;\n\t    var id = tree[0];\n\t    var opts = tree[1];\n\t    var branches = tree[2];\n\t    var isLeaf = branches.length === 0;\n\n\t    var history = node.history ? node.history.slice() : [];\n\t    history.push({id: id, opts: opts});\n\t    if (isLeaf) {\n\t      paths.push({pos: (pos + 1 - history.length), ids: history});\n\t    }\n\t    for (var i = 0, len = branches.length; i < len; i++) {\n\t      toVisit.push({pos: pos + 1, ids: branches[i], history: history});\n\t    }\n\t  }\n\t  return paths.reverse();\n\t}\n\n\t// for a better overview of what this is doing, read:\n\n\tfunction sortByPos$1(a, b) {\n\t  return a.pos - b.pos;\n\t}\n\n\t// classic binary search\n\tfunction binarySearch(arr, item, comparator) {\n\t  var low = 0;\n\t  var high = arr.length;\n\t  var mid;\n\t  while (low < high) {\n\t    mid = (low + high) >>> 1;\n\t    if (comparator(arr[mid], item) < 0) {\n\t      low = mid + 1;\n\t    } else {\n\t      high = mid;\n\t    }\n\t  }\n\t  return low;\n\t}\n\n\t// assuming the arr is sorted, insert the item in the proper place\n\tfunction insertSorted(arr, item, comparator) {\n\t  var idx = binarySearch(arr, item, comparator);\n\t  arr.splice(idx, 0, item);\n\t}\n\n\t// Turn a path as a flat array into a tree with a single branch.\n\t// If any should be stemmed from the beginning of the array, that's passed\n\t// in as the second argument\n\tfunction pathToTree(path, numStemmed) {\n\t  var root;\n\t  var leaf;\n\t  for (var i = numStemmed, len = path.length; i < len; i++) {\n\t    var node = path[i];\n\t    var currentLeaf = [node.id, node.opts, []];\n\t    if (leaf) {\n\t      leaf[2].push(currentLeaf);\n\t      leaf = currentLeaf;\n\t    } else {\n\t      root = leaf = currentLeaf;\n\t    }\n\t  }\n\t  return root;\n\t}\n\n\t// compare the IDs of two trees\n\tfunction compareTree(a, b) {\n\t  return a[0] < b[0] ? -1 : 1;\n\t}\n\n\t// Merge two trees together\n\t// The roots of tree1 and tree2 must be the same revision\n\tfunction mergeTree(in_tree1, in_tree2) {\n\t  var queue = [{tree1: in_tree1, tree2: in_tree2}];\n\t  var conflicts = false;\n\t  while (queue.length > 0) {\n\t    var item = queue.pop();\n\t    var tree1 = item.tree1;\n\t    var tree2 = item.tree2;\n\n\t    if (tree1[1].status || tree2[1].status) {\n\t      tree1[1].status =\n\t        (tree1[1].status ===  'available' ||\n\t        tree2[1].status === 'available') ? 'available' : 'missing';\n\t    }\n\n\t    for (var i = 0; i < tree2[2].length; i++) {\n\t      if (!tree1[2][0]) {\n\t        conflicts = 'new_leaf';\n\t        tree1[2][0] = tree2[2][i];\n\t        continue;\n\t      }\n\n\t      var merged = false;\n\t      for (var j = 0; j < tree1[2].length; j++) {\n\t        if (tree1[2][j][0] === tree2[2][i][0]) {\n\t          queue.push({tree1: tree1[2][j], tree2: tree2[2][i]});\n\t          merged = true;\n\t        }\n\t      }\n\t      if (!merged) {\n\t        conflicts = 'new_branch';\n\t        insertSorted(tree1[2], tree2[2][i], compareTree);\n\t      }\n\t    }\n\t  }\n\t  return {conflicts: conflicts, tree: in_tree1};\n\t}\n\n\tfunction doMerge(tree, path, dontExpand) {\n\t  var restree = [];\n\t  var conflicts = false;\n\t  var merged = false;\n\t  var res;\n\n\t  if (!tree.length) {\n\t    return {tree: [path], conflicts: 'new_leaf'};\n\t  }\n\n\t  for (var i = 0, len = tree.length; i < len; i++) {\n\t    var branch = tree[i];\n\t    if (branch.pos === path.pos && branch.ids[0] === path.ids[0]) {\n\t      // Paths start at the same position and have the same root, so they need\n\t      // merged\n\t      res = mergeTree(branch.ids, path.ids);\n\t      restree.push({pos: branch.pos, ids: res.tree});\n\t      conflicts = conflicts || res.conflicts;\n\t      merged = true;\n\t    } else if (dontExpand !== true) {\n\t      // The paths start at a different position, take the earliest path and\n\t      // traverse up until it as at the same point from root as the path we\n\t      // want to merge.  If the keys match we return the longer path with the\n\t      // other merged After stemming we dont want to expand the trees\n\n\t      var t1 = branch.pos < path.pos ? branch : path;\n\t      var t2 = branch.pos < path.pos ? path : branch;\n\t      var diff = t2.pos - t1.pos;\n\n\t      var candidateParents = [];\n\n\t      var trees = [];\n\t      trees.push({ids: t1.ids, diff: diff, parent: null, parentIdx: null});\n\t      while (trees.length > 0) {\n\t        var item = trees.pop();\n\t        if (item.diff === 0) {\n\t          if (item.ids[0] === t2.ids[0]) {\n\t            candidateParents.push(item);\n\t          }\n\t          continue;\n\t        }\n\t        var elements = item.ids[2];\n\t        for (var j = 0, elementsLen = elements.length; j < elementsLen; j++) {\n\t          trees.push({\n\t            ids: elements[j],\n\t            diff: item.diff - 1,\n\t            parent: item.ids,\n\t            parentIdx: j\n\t          });\n\t        }\n\t      }\n\n\t      var el = candidateParents[0];\n\n\t      if (!el) {\n\t        restree.push(branch);\n\t      } else {\n\t        res = mergeTree(el.ids, t2.ids);\n\t        el.parent[2][el.parentIdx] = res.tree;\n\t        restree.push({pos: t1.pos, ids: t1.ids});\n\t        conflicts = conflicts || res.conflicts;\n\t        merged = true;\n\t      }\n\t    } else {\n\t      restree.push(branch);\n\t    }\n\t  }\n\n\t  // We didnt find\n\t  if (!merged) {\n\t    restree.push(path);\n\t  }\n\n\t  restree.sort(sortByPos$1);\n\n\t  return {\n\t    tree: restree,\n\t    conflicts: conflicts || 'internal_node'\n\t  };\n\t}\n\n\t// To ensure we dont grow the revision tree infinitely, we stem old revisions\n\tfunction stem(tree, depth) {\n\t  // First we break out the tree into a complete list of root to leaf paths\n\t  var paths = rootToLeaf(tree);\n\t  var stemmedRevs;\n\n\t  var result;\n\t  for (var i = 0, len = paths.length; i < len; i++) {\n\t    // Then for each path, we cut off the start of the path based on the\n\t    // `depth` to stem to, and generate a new set of flat trees\n\t    var path = paths[i];\n\t    var stemmed = path.ids;\n\t    var node;\n\t    if (stemmed.length > depth) {\n\t      // only do the stemming work if we actually need to stem\n\t      if (!stemmedRevs) {\n\t        stemmedRevs = {}; // avoid allocating this object unnecessarily\n\t      }\n\t      var numStemmed = stemmed.length - depth;\n\t      node = {\n\t        pos: path.pos + numStemmed,\n\t        ids: pathToTree(stemmed, numStemmed)\n\t      };\n\n\t      for (var s = 0; s < numStemmed; s++) {\n\t        var rev = (path.pos + s) + '-' + stemmed[s].id;\n\t        stemmedRevs[rev] = true;\n\t      }\n\t    } else { // no need to actually stem\n\t      node = {\n\t        pos: path.pos,\n\t        ids: pathToTree(stemmed, 0)\n\t      };\n\t    }\n\n\t    // Then we remerge all those flat trees together, ensuring that we dont\n\t    // connect trees that would go beyond the depth limit\n\t    if (result) {\n\t      result = doMerge(result, node, true).tree;\n\t    } else {\n\t      result = [node];\n\t    }\n\t  }\n\n\t  // this is memory-heavy per Chrome profiler, avoid unless we actually stemmed\n\t  if (stemmedRevs) {\n\t    traverseRevTree(result, function (isLeaf, pos, revHash) {\n\t      // some revisions may have been removed in a branch but not in another\n\t      delete stemmedRevs[pos + '-' + revHash];\n\t    });\n\t  }\n\n\t  return {\n\t    tree: result,\n\t    revs: stemmedRevs ? Object.keys(stemmedRevs) : []\n\t  };\n\t}\n\n\tfunction merge(tree, path, depth) {\n\t  var newTree = doMerge(tree, path);\n\t  var stemmed = stem(newTree.tree, depth);\n\t  return {\n\t    tree: stemmed.tree,\n\t    stemmedRevs: stemmed.revs,\n\t    conflicts: newTree.conflicts\n\t  };\n\t}\n\n\t// return true if a rev exists in the rev tree, false otherwise\n\tfunction revExists(revs, rev) {\n\t  var toVisit = revs.slice();\n\t  var splitRev = rev.split('-');\n\t  var targetPos = parseInt(splitRev[0], 10);\n\t  var targetId = splitRev[1];\n\n\t  var node;\n\t  while ((node = toVisit.pop())) {\n\t    if (node.pos === targetPos && node.ids[0] === targetId) {\n\t      return true;\n\t    }\n\t    var branches = node.ids[2];\n\t    for (var i = 0, len = branches.length; i < len; i++) {\n\t      toVisit.push({pos: node.pos + 1, ids: branches[i]});\n\t    }\n\t  }\n\t  return false;\n\t}\n\n\tfunction getTrees(node) {\n\t  return node.ids;\n\t}\n\n\t// check if a specific revision of a doc has been deleted\n\t//  - metadata: the metadata object from the doc store\n\t//  - rev: (optional) the revision to check. defaults to winning revision\n\tfunction isDeleted(metadata, rev) {\n\t  if (!rev) {\n\t    rev = winningRev(metadata);\n\t  }\n\t  var id = rev.substring(rev.indexOf('-') + 1);\n\t  var toVisit = metadata.rev_tree.map(getTrees);\n\n\t  var tree;\n\t  while ((tree = toVisit.pop())) {\n\t    if (tree[0] === id) {\n\t      return !!tree[1].deleted;\n\t    }\n\t    toVisit = toVisit.concat(tree[2]);\n\t  }\n\t}\n\n\tfunction isLocalId(id) {\n\t  return (/^_local/).test(id);\n\t}\n\n\t// returns the current leaf node for a given revision\n\tfunction latest(rev, metadata) {\n\t  var toVisit = metadata.rev_tree.slice();\n\t  var node;\n\t  while ((node = toVisit.pop())) {\n\t    var pos = node.pos;\n\t    var tree = node.ids;\n\t    var id = tree[0];\n\t    var opts = tree[1];\n\t    var branches = tree[2];\n\t    var isLeaf = branches.length === 0;\n\n\t    var history = node.history ? node.history.slice() : [];\n\t    history.push({id: id, pos: pos, opts: opts});\n\n\t    if (isLeaf) {\n\t      for (var i = 0, len = history.length; i < len; i++) {\n\t        var historyNode = history[i];\n\t        var historyRev = historyNode.pos + '-' + historyNode.id;\n\n\t        if (historyRev === rev) {\n\t          // return the rev of this leaf\n\t          return pos + '-' + id;\n\t        }\n\t      }\n\t    }\n\n\t    for (var j = 0, l = branches.length; j < l; j++) {\n\t      toVisit.push({pos: pos + 1, ids: branches[j], history: history});\n\t    }\n\t  }\n\n\t  /* istanbul ignore next */\n\t  throw new Error('Unable to resolve latest revision for id ' + metadata.id + ', rev ' + rev);\n\t}\n\n\tinherits$2(Changes$1, EventEmitter);\n\n\tfunction tryCatchInChangeListener(self, change, pending, lastSeq) {\n\t  // isolate try/catches to avoid V8 deoptimizations\n\t  try {\n\t    self.emit('change', change, pending, lastSeq);\n\t  } catch (e) {\n\t    guardedConsole('error', 'Error in .on(\"change\", function):', e);\n\t  }\n\t}\n\n\tfunction Changes$1(db, opts, callback) {\n\t  EventEmitter.call(this);\n\t  var self = this;\n\t  this.db = db;\n\t  opts = opts ? clone(opts) : {};\n\t  var complete = opts.complete = once(function (err, resp) {\n\t    if (err) {\n\t      if (listenerCount$1(self, 'error') > 0) {\n\t        self.emit('error', err);\n\t      }\n\t    } else {\n\t      self.emit('complete', resp);\n\t    }\n\t    self.removeAllListeners();\n\t    db.removeListener('destroyed', onDestroy);\n\t  });\n\t  if (callback) {\n\t    self.on('complete', function (resp) {\n\t      callback(null, resp);\n\t    });\n\t    self.on('error', callback);\n\t  }\n\t  function onDestroy() {\n\t    self.cancel();\n\t  }\n\t  db.once('destroyed', onDestroy);\n\n\t  opts.onChange = function (change, pending, lastSeq) {\n\t    /* istanbul ignore if */\n\t    if (self.isCancelled) {\n\t      return;\n\t    }\n\t    tryCatchInChangeListener(self, change, pending, lastSeq);\n\t  };\n\n\t  var promise = new Promise(function (fulfill, reject) {\n\t    opts.complete = function (err, res) {\n\t      if (err) {\n\t        reject(err);\n\t      } else {\n\t        fulfill(res);\n\t      }\n\t    };\n\t  });\n\t  self.once('cancel', function () {\n\t    db.removeListener('destroyed', onDestroy);\n\t    opts.complete(null, {status: 'cancelled'});\n\t  });\n\t  this.then = promise.then.bind(promise);\n\t  this['catch'] = promise['catch'].bind(promise);\n\t  this.then(function (result) {\n\t    complete(null, result);\n\t  }, complete);\n\n\n\n\t  if (!db.taskqueue.isReady) {\n\t    db.taskqueue.addTask(function (failed) {\n\t      if (failed) {\n\t        opts.complete(failed);\n\t      } else if (self.isCancelled) {\n\t        self.emit('cancel');\n\t      } else {\n\t        self.validateChanges(opts);\n\t      }\n\t    });\n\t  } else {\n\t    self.validateChanges(opts);\n\t  }\n\t}\n\tChanges$1.prototype.cancel = function () {\n\t  this.isCancelled = true;\n\t  if (this.db.taskqueue.isReady) {\n\t    this.emit('cancel');\n\t  }\n\t};\n\tfunction processChange(doc, metadata, opts) {\n\t  var changeList = [{rev: doc._rev}];\n\t  if (opts.style === 'all_docs') {\n\t    changeList = collectLeaves(metadata.rev_tree)\n\t    .map(function (x) { return {rev: x.rev}; });\n\t  }\n\t  var change = {\n\t    id: metadata.id,\n\t    changes: changeList,\n\t    doc: doc\n\t  };\n\n\t  if (isDeleted(metadata, doc._rev)) {\n\t    change.deleted = true;\n\t  }\n\t  if (opts.conflicts) {\n\t    change.doc._conflicts = collectConflicts(metadata);\n\t    if (!change.doc._conflicts.length) {\n\t      delete change.doc._conflicts;\n\t    }\n\t  }\n\t  return change;\n\t}\n\n\tChanges$1.prototype.validateChanges = function (opts) {\n\t  var callback = opts.complete;\n\t  var self = this;\n\n\t  /* istanbul ignore else */\n\t  if (PouchDB._changesFilterPlugin) {\n\t    PouchDB._changesFilterPlugin.validate(opts, function (err) {\n\t      if (err) {\n\t        return callback(err);\n\t      }\n\t      self.doChanges(opts);\n\t    });\n\t  } else {\n\t    self.doChanges(opts);\n\t  }\n\t};\n\n\tChanges$1.prototype.doChanges = function (opts) {\n\t  var self = this;\n\t  var callback = opts.complete;\n\n\t  opts = clone(opts);\n\t  if ('live' in opts && !('continuous' in opts)) {\n\t    opts.continuous = opts.live;\n\t  }\n\t  opts.processChange = processChange;\n\n\t  if (opts.since === 'latest') {\n\t    opts.since = 'now';\n\t  }\n\t  if (!opts.since) {\n\t    opts.since = 0;\n\t  }\n\t  if (opts.since === 'now') {\n\t    this.db.info().then(function (info) {\n\t      /* istanbul ignore if */\n\t      if (self.isCancelled) {\n\t        callback(null, {status: 'cancelled'});\n\t        return;\n\t      }\n\t      opts.since = info.update_seq;\n\t      self.doChanges(opts);\n\t    }, callback);\n\t    return;\n\t  }\n\n\t  /* istanbul ignore else */\n\t  if (PouchDB._changesFilterPlugin) {\n\t    PouchDB._changesFilterPlugin.normalize(opts);\n\t    if (PouchDB._changesFilterPlugin.shouldFilter(this, opts)) {\n\t      return PouchDB._changesFilterPlugin.filter(this, opts);\n\t    }\n\t  } else {\n\t    ['doc_ids', 'filter', 'selector', 'view'].forEach(function (key) {\n\t      if (key in opts) {\n\t        guardedConsole('warn',\n\t          'The \"' + key + '\" option was passed in to changes/replicate, ' +\n\t          'but pouchdb-changes-filter plugin is not installed, so it ' +\n\t          'was ignored. Please install the plugin to enable filtering.'\n\t        );\n\t      }\n\t    });\n\t  }\n\n\t  if (!('descending' in opts)) {\n\t    opts.descending = false;\n\t  }\n\n\t  // 0 and 1 should return 1 document\n\t  opts.limit = opts.limit === 0 ? 1 : opts.limit;\n\t  opts.complete = callback;\n\t  var newPromise = this.db._changes(opts);\n\t  /* istanbul ignore else */\n\t  if (newPromise && typeof newPromise.cancel === 'function') {\n\t    var cancel = self.cancel;\n\t    self.cancel = argsarray(function (args) {\n\t      newPromise.cancel();\n\t      cancel.apply(this, args);\n\t    });\n\t  }\n\t};\n\n\t/*\n\t * A generic pouch adapter\n\t */\n\n\tfunction compare(left, right) {\n\t  return left < right ? -1 : left > right ? 1 : 0;\n\t}\n\n\t// Wrapper for functions that call the bulkdocs api with a single doc,\n\t// if the first result is an error, return an error\n\tfunction yankError(callback, docId) {\n\t  return function (err, results) {\n\t    if (err || (results[0] && results[0].error)) {\n\t      err = err || results[0];\n\t      err.docId = docId;\n\t      callback(err);\n\t    } else {\n\t      callback(null, results.length ? results[0]  : results);\n\t    }\n\t  };\n\t}\n\n\t// clean docs given to us by the user\n\tfunction cleanDocs(docs) {\n\t  for (var i = 0; i < docs.length; i++) {\n\t    var doc = docs[i];\n\t    if (doc._deleted) {\n\t      delete doc._attachments; // ignore atts for deleted docs\n\t    } else if (doc._attachments) {\n\t      // filter out extraneous keys from _attachments\n\t      var atts = Object.keys(doc._attachments);\n\t      for (var j = 0; j < atts.length; j++) {\n\t        var att = atts[j];\n\t        doc._attachments[att] = pick(doc._attachments[att],\n\t          ['data', 'digest', 'content_type', 'length', 'revpos', 'stub']);\n\t      }\n\t    }\n\t  }\n\t}\n\n\t// compare two docs, first by _id then by _rev\n\tfunction compareByIdThenRev(a, b) {\n\t  var idCompare = compare(a._id, b._id);\n\t  if (idCompare !== 0) {\n\t    return idCompare;\n\t  }\n\t  var aStart = a._revisions ? a._revisions.start : 0;\n\t  var bStart = b._revisions ? b._revisions.start : 0;\n\t  return compare(aStart, bStart);\n\t}\n\n\t// for every node in a revision tree computes its distance from the closest\n\t// leaf\n\tfunction computeHeight(revs) {\n\t  var height = {};\n\t  var edges = [];\n\t  traverseRevTree(revs, function (isLeaf, pos, id, prnt) {\n\t    var rev$$1 = pos + \"-\" + id;\n\t    if (isLeaf) {\n\t      height[rev$$1] = 0;\n\t    }\n\t    if (prnt !== undefined) {\n\t      edges.push({from: prnt, to: rev$$1});\n\t    }\n\t    return rev$$1;\n\t  });\n\n\t  edges.reverse();\n\t  edges.forEach(function (edge) {\n\t    if (height[edge.from] === undefined) {\n\t      height[edge.from] = 1 + height[edge.to];\n\t    } else {\n\t      height[edge.from] = Math.min(height[edge.from], 1 + height[edge.to]);\n\t    }\n\t  });\n\t  return height;\n\t}\n\n\tfunction allDocsKeysParse(opts) {\n\t  var keys =  ('limit' in opts) ?\n\t    opts.keys.slice(opts.skip, opts.limit + opts.skip) :\n\t    (opts.skip > 0) ? opts.keys.slice(opts.skip) : opts.keys;\n\t  opts.keys = keys;\n\t  opts.skip = 0;\n\t  delete opts.limit;\n\t  if (opts.descending) {\n\t    keys.reverse();\n\t    opts.descending = false;\n\t  }\n\t}\n\n\t// all compaction is done in a queue, to avoid attaching\n\t// too many listeners at once\n\tfunction doNextCompaction(self) {\n\t  var task = self._compactionQueue[0];\n\t  var opts = task.opts;\n\t  var callback = task.callback;\n\t  self.get('_local/compaction').catch(function () {\n\t    return false;\n\t  }).then(function (doc) {\n\t    if (doc && doc.last_seq) {\n\t      opts.last_seq = doc.last_seq;\n\t    }\n\t    self._compact(opts, function (err, res) {\n\t      /* istanbul ignore if */\n\t      if (err) {\n\t        callback(err);\n\t      } else {\n\t        callback(null, res);\n\t      }\n\t      lib(function () {\n\t        self._compactionQueue.shift();\n\t        if (self._compactionQueue.length) {\n\t          doNextCompaction(self);\n\t        }\n\t      });\n\t    });\n\t  });\n\t}\n\n\tfunction attachmentNameError(name) {\n\t  if (name.charAt(0) === '_') {\n\t    return name + ' is not a valid attachment name, attachment ' +\n\t      'names cannot start with \\'_\\'';\n\t  }\n\t  return false;\n\t}\n\n\tinherits$2(AbstractPouchDB, EventEmitter);\n\n\tfunction AbstractPouchDB() {\n\t  EventEmitter.call(this);\n\n\t  // re-bind prototyped methods\n\t  for (var p in AbstractPouchDB.prototype) {\n\t    if (typeof this[p] === 'function') {\n\t      this[p] = this[p].bind(this);\n\t    }\n\t  }\n\t}\n\n\tAbstractPouchDB.prototype.post =\n\t  adapterFun('post', function (doc, opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  if (typeof doc !== 'object' || Array.isArray(doc)) {\n\t    return callback(createError(NOT_AN_OBJECT));\n\t  }\n\t  this.bulkDocs({docs: [doc]}, opts, yankError(callback, doc._id));\n\t});\n\n\tAbstractPouchDB.prototype.put = adapterFun('put', function (doc, opts, cb) {\n\t  if (typeof opts === 'function') {\n\t    cb = opts;\n\t    opts = {};\n\t  }\n\t  if (typeof doc !== 'object' || Array.isArray(doc)) {\n\t    return cb(createError(NOT_AN_OBJECT));\n\t  }\n\t  invalidIdError(doc._id);\n\t  if (isLocalId(doc._id) && typeof this._putLocal === 'function') {\n\t    if (doc._deleted) {\n\t      return this._removeLocal(doc, cb);\n\t    } else {\n\t      return this._putLocal(doc, cb);\n\t    }\n\t  }\n\t  var self = this;\n\t  if (opts.force && doc._rev) {\n\t    transformForceOptionToNewEditsOption();\n\t    putDoc(function (err) {\n\t      var result = err ? null : {ok: true, id: doc._id, rev: doc._rev};\n\t      cb(err, result);\n\t    });\n\t  } else {\n\t    putDoc(cb);\n\t  }\n\n\t  function transformForceOptionToNewEditsOption() {\n\t    var parts = doc._rev.split('-');\n\t    var oldRevId = parts[1];\n\t    var oldRevNum = parseInt(parts[0], 10);\n\n\t    var newRevNum = oldRevNum + 1;\n\t    var newRevId = rev();\n\n\t    doc._revisions = {\n\t      start: newRevNum,\n\t      ids: [newRevId, oldRevId]\n\t    };\n\t    doc._rev = newRevNum + '-' + newRevId;\n\t    opts.new_edits = false;\n\t  }\n\t  function putDoc(next) {\n\t    if (typeof self._put === 'function' && opts.new_edits !== false) {\n\t      self._put(doc, opts, next);\n\t    } else {\n\t      self.bulkDocs({docs: [doc]}, opts, yankError(next, doc._id));\n\t    }\n\t  }\n\t});\n\n\tAbstractPouchDB.prototype.putAttachment =\n\t  adapterFun('putAttachment', function (docId, attachmentId, rev$$1,\n\t                                              blob, type) {\n\t  var api = this;\n\t  if (typeof type === 'function') {\n\t    type = blob;\n\t    blob = rev$$1;\n\t    rev$$1 = null;\n\t  }\n\t  // Lets fix in https://github.com/pouchdb/pouchdb/issues/3267\n\t  /* istanbul ignore if */\n\t  if (typeof type === 'undefined') {\n\t    type = blob;\n\t    blob = rev$$1;\n\t    rev$$1 = null;\n\t  }\n\t  if (!type) {\n\t    guardedConsole('warn', 'Attachment', attachmentId, 'on document', docId, 'is missing content_type');\n\t  }\n\n\t  function createAttachment(doc) {\n\t    var prevrevpos = '_rev' in doc ? parseInt(doc._rev, 10) : 0;\n\t    doc._attachments = doc._attachments || {};\n\t    doc._attachments[attachmentId] = {\n\t      content_type: type,\n\t      data: blob,\n\t      revpos: ++prevrevpos\n\t    };\n\t    return api.put(doc);\n\t  }\n\n\t  return api.get(docId).then(function (doc) {\n\t    if (doc._rev !== rev$$1) {\n\t      throw createError(REV_CONFLICT);\n\t    }\n\n\t    return createAttachment(doc);\n\t  }, function (err) {\n\t     // create new doc\n\t    /* istanbul ignore else */\n\t    if (err.reason === MISSING_DOC.message) {\n\t      return createAttachment({_id: docId});\n\t    } else {\n\t      throw err;\n\t    }\n\t  });\n\t});\n\n\tAbstractPouchDB.prototype.removeAttachment =\n\t  adapterFun('removeAttachment', function (docId, attachmentId, rev$$1,\n\t                                                 callback) {\n\t  var self = this;\n\t  self.get(docId, function (err, obj) {\n\t    /* istanbul ignore if */\n\t    if (err) {\n\t      callback(err);\n\t      return;\n\t    }\n\t    if (obj._rev !== rev$$1) {\n\t      callback(createError(REV_CONFLICT));\n\t      return;\n\t    }\n\t    /* istanbul ignore if */\n\t    if (!obj._attachments) {\n\t      return callback();\n\t    }\n\t    delete obj._attachments[attachmentId];\n\t    if (Object.keys(obj._attachments).length === 0) {\n\t      delete obj._attachments;\n\t    }\n\t    self.put(obj, callback);\n\t  });\n\t});\n\n\tAbstractPouchDB.prototype.remove =\n\t  adapterFun('remove', function (docOrId, optsOrRev, opts, callback) {\n\t  var doc;\n\t  if (typeof optsOrRev === 'string') {\n\t    // id, rev, opts, callback style\n\t    doc = {\n\t      _id: docOrId,\n\t      _rev: optsOrRev\n\t    };\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t  } else {\n\t    // doc, opts, callback style\n\t    doc = docOrId;\n\t    if (typeof optsOrRev === 'function') {\n\t      callback = optsOrRev;\n\t      opts = {};\n\t    } else {\n\t      callback = opts;\n\t      opts = optsOrRev;\n\t    }\n\t  }\n\t  opts = opts || {};\n\t  opts.was_delete = true;\n\t  var newDoc = {_id: doc._id, _rev: (doc._rev || opts.rev)};\n\t  newDoc._deleted = true;\n\t  if (isLocalId(newDoc._id) && typeof this._removeLocal === 'function') {\n\t    return this._removeLocal(doc, callback);\n\t  }\n\t  this.bulkDocs({docs: [newDoc]}, opts, yankError(callback, newDoc._id));\n\t});\n\n\tAbstractPouchDB.prototype.revsDiff =\n\t  adapterFun('revsDiff', function (req, opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  var ids = Object.keys(req);\n\n\t  if (!ids.length) {\n\t    return callback(null, {});\n\t  }\n\n\t  var count = 0;\n\t  var missing = new ExportedMap();\n\n\t  function addToMissing(id, revId) {\n\t    if (!missing.has(id)) {\n\t      missing.set(id, {missing: []});\n\t    }\n\t    missing.get(id).missing.push(revId);\n\t  }\n\n\t  function processDoc(id, rev_tree) {\n\t    // Is this fast enough? Maybe we should switch to a set simulated by a map\n\t    var missingForId = req[id].slice(0);\n\t    traverseRevTree(rev_tree, function (isLeaf, pos, revHash, ctx,\n\t      opts) {\n\t        var rev$$1 = pos + '-' + revHash;\n\t        var idx = missingForId.indexOf(rev$$1);\n\t        if (idx === -1) {\n\t          return;\n\t        }\n\n\t        missingForId.splice(idx, 1);\n\t        /* istanbul ignore if */\n\t        if (opts.status !== 'available') {\n\t          addToMissing(id, rev$$1);\n\t        }\n\t      });\n\n\t    // Traversing the tree is synchronous, so now `missingForId` contains\n\t    // revisions that were not found in the tree\n\t    missingForId.forEach(function (rev$$1) {\n\t      addToMissing(id, rev$$1);\n\t    });\n\t  }\n\n\t  ids.map(function (id) {\n\t    this._getRevisionTree(id, function (err, rev_tree) {\n\t      if (err && err.status === 404 && err.message === 'missing') {\n\t        missing.set(id, {missing: req[id]});\n\t      } else if (err) {\n\t        /* istanbul ignore next */\n\t        return callback(err);\n\t      } else {\n\t        processDoc(id, rev_tree);\n\t      }\n\n\t      if (++count === ids.length) {\n\t        // convert LazyMap to object\n\t        var missingObj = {};\n\t        missing.forEach(function (value, key) {\n\t          missingObj[key] = value;\n\t        });\n\t        return callback(null, missingObj);\n\t      }\n\t    });\n\t  }, this);\n\t});\n\n\t// _bulk_get API for faster replication, as described in\n\t// https://github.com/apache/couchdb-chttpd/pull/33\n\t// At the \"abstract\" level, it will just run multiple get()s in\n\t// parallel, because this isn't much of a performance cost\n\t// for local databases (except the cost of multiple transactions, which is\n\t// small). The http adapter overrides this in order\n\t// to do a more efficient single HTTP request.\n\tAbstractPouchDB.prototype.bulkGet =\n\t  adapterFun('bulkGet', function (opts, callback) {\n\t  bulkGet(this, opts, callback);\n\t});\n\n\t// compact one document and fire callback\n\t// by compacting we mean removing all revisions which\n\t// are further from the leaf in revision tree than max_height\n\tAbstractPouchDB.prototype.compactDocument =\n\t  adapterFun('compactDocument', function (docId, maxHeight, callback) {\n\t  var self = this;\n\t  this._getRevisionTree(docId, function (err, revTree) {\n\t    /* istanbul ignore if */\n\t    if (err) {\n\t      return callback(err);\n\t    }\n\t    var height = computeHeight(revTree);\n\t    var candidates = [];\n\t    var revs = [];\n\t    Object.keys(height).forEach(function (rev$$1) {\n\t      if (height[rev$$1] > maxHeight) {\n\t        candidates.push(rev$$1);\n\t      }\n\t    });\n\n\t    traverseRevTree(revTree, function (isLeaf, pos, revHash, ctx, opts) {\n\t      var rev$$1 = pos + '-' + revHash;\n\t      if (opts.status === 'available' && candidates.indexOf(rev$$1) !== -1) {\n\t        revs.push(rev$$1);\n\t      }\n\t    });\n\t    self._doCompaction(docId, revs, callback);\n\t  });\n\t});\n\n\t// compact the whole database using single document\n\t// compaction\n\tAbstractPouchDB.prototype.compact =\n\t  adapterFun('compact', function (opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\n\t  var self = this;\n\t  opts = opts || {};\n\n\t  self._compactionQueue = self._compactionQueue || [];\n\t  self._compactionQueue.push({opts: opts, callback: callback});\n\t  if (self._compactionQueue.length === 1) {\n\t    doNextCompaction(self);\n\t  }\n\t});\n\tAbstractPouchDB.prototype._compact = function (opts, callback) {\n\t  var self = this;\n\t  var changesOpts = {\n\t    return_docs: false,\n\t    last_seq: opts.last_seq || 0\n\t  };\n\t  var promises = [];\n\n\t  function onChange(row) {\n\t    promises.push(self.compactDocument(row.id, 0));\n\t  }\n\t  function onComplete(resp) {\n\t    var lastSeq = resp.last_seq;\n\t    Promise.all(promises).then(function () {\n\t      return upsert(self, '_local/compaction', function deltaFunc(doc) {\n\t        if (!doc.last_seq || doc.last_seq < lastSeq) {\n\t          doc.last_seq = lastSeq;\n\t          return doc;\n\t        }\n\t        return false; // somebody else got here first, don't update\n\t      });\n\t    }).then(function () {\n\t      callback(null, {ok: true});\n\t    }).catch(callback);\n\t  }\n\t  self.changes(changesOpts)\n\t    .on('change', onChange)\n\t    .on('complete', onComplete)\n\t    .on('error', callback);\n\t};\n\n\t/* Begin api wrappers. Specific functionality to storage belongs in the\n\t   _[method] */\n\tAbstractPouchDB.prototype.get = adapterFun('get', function (id, opts, cb) {\n\t  if (typeof opts === 'function') {\n\t    cb = opts;\n\t    opts = {};\n\t  }\n\t  if (typeof id !== 'string') {\n\t    return cb(createError(INVALID_ID));\n\t  }\n\t  if (isLocalId(id) && typeof this._getLocal === 'function') {\n\t    return this._getLocal(id, cb);\n\t  }\n\t  var leaves = [], self = this;\n\n\t  function finishOpenRevs() {\n\t    var result = [];\n\t    var count = leaves.length;\n\t    /* istanbul ignore if */\n\t    if (!count) {\n\t      return cb(null, result);\n\t    }\n\n\t    // order with open_revs is unspecified\n\t    leaves.forEach(function (leaf) {\n\t      self.get(id, {\n\t        rev: leaf,\n\t        revs: opts.revs,\n\t        latest: opts.latest,\n\t        attachments: opts.attachments,\n\t        binary: opts.binary\n\t      }, function (err, doc) {\n\t        if (!err) {\n\t          // using latest=true can produce duplicates\n\t          var existing;\n\t          for (var i = 0, l = result.length; i < l; i++) {\n\t            if (result[i].ok && result[i].ok._rev === doc._rev) {\n\t              existing = true;\n\t              break;\n\t            }\n\t          }\n\t          if (!existing) {\n\t            result.push({ok: doc});\n\t          }\n\t        } else {\n\t          result.push({missing: leaf});\n\t        }\n\t        count--;\n\t        if (!count) {\n\t          cb(null, result);\n\t        }\n\t      });\n\t    });\n\t  }\n\n\t  if (opts.open_revs) {\n\t    if (opts.open_revs === \"all\") {\n\t      this._getRevisionTree(id, function (err, rev_tree) {\n\t        /* istanbul ignore if */\n\t        if (err) {\n\t          return cb(err);\n\t        }\n\t        leaves = collectLeaves(rev_tree).map(function (leaf) {\n\t          return leaf.rev;\n\t        });\n\t        finishOpenRevs();\n\t      });\n\t    } else {\n\t      if (Array.isArray(opts.open_revs)) {\n\t        leaves = opts.open_revs;\n\t        for (var i = 0; i < leaves.length; i++) {\n\t          var l = leaves[i];\n\t          // looks like it's the only thing couchdb checks\n\t          if (!(typeof (l) === \"string\" && /^\\d+-/.test(l))) {\n\t            return cb(createError(INVALID_REV));\n\t          }\n\t        }\n\t        finishOpenRevs();\n\t      } else {\n\t        return cb(createError(UNKNOWN_ERROR, 'function_clause'));\n\t      }\n\t    }\n\t    return; // open_revs does not like other options\n\t  }\n\n\t  return this._get(id, opts, function (err, result) {\n\t    if (err) {\n\t      err.docId = id;\n\t      return cb(err);\n\t    }\n\n\t    var doc = result.doc;\n\t    var metadata = result.metadata;\n\t    var ctx = result.ctx;\n\n\t    if (opts.conflicts) {\n\t      var conflicts = collectConflicts(metadata);\n\t      if (conflicts.length) {\n\t        doc._conflicts = conflicts;\n\t      }\n\t    }\n\n\t    if (isDeleted(metadata, doc._rev)) {\n\t      doc._deleted = true;\n\t    }\n\n\t    if (opts.revs || opts.revs_info) {\n\t      var splittedRev = doc._rev.split('-');\n\t      var revNo       = parseInt(splittedRev[0], 10);\n\t      var revHash     = splittedRev[1];\n\n\t      var paths = rootToLeaf(metadata.rev_tree);\n\t      var path = null;\n\n\t      for (var i = 0; i < paths.length; i++) {\n\t        var currentPath = paths[i];\n\t        var hashIndex = currentPath.ids.map(function (x) { return x.id; })\n\t          .indexOf(revHash);\n\t        var hashFoundAtRevPos = hashIndex === (revNo - 1);\n\n\t        if (hashFoundAtRevPos || (!path && hashIndex !== -1)) {\n\t          path = currentPath;\n\t        }\n\t      }\n\n\t      /* istanbul ignore if */\n\t      if (!path) {\n\t        err = new Error('invalid rev tree');\n\t        err.docId = id;\n\t        return cb(err);\n\t      }\n\n\t      var indexOfRev = path.ids.map(function (x) { return x.id; })\n\t        .indexOf(doc._rev.split('-')[1]) + 1;\n\t      var howMany = path.ids.length - indexOfRev;\n\t      path.ids.splice(indexOfRev, howMany);\n\t      path.ids.reverse();\n\n\t      if (opts.revs) {\n\t        doc._revisions = {\n\t          start: (path.pos + path.ids.length) - 1,\n\t          ids: path.ids.map(function (rev$$1) {\n\t            return rev$$1.id;\n\t          })\n\t        };\n\t      }\n\t      if (opts.revs_info) {\n\t        var pos =  path.pos + path.ids.length;\n\t        doc._revs_info = path.ids.map(function (rev$$1) {\n\t          pos--;\n\t          return {\n\t            rev: pos + '-' + rev$$1.id,\n\t            status: rev$$1.opts.status\n\t          };\n\t        });\n\t      }\n\t    }\n\n\t    if (opts.attachments && doc._attachments) {\n\t      var attachments = doc._attachments;\n\t      var count = Object.keys(attachments).length;\n\t      if (count === 0) {\n\t        return cb(null, doc);\n\t      }\n\t      Object.keys(attachments).forEach(function (key) {\n\t        this._getAttachment(doc._id, key, attachments[key], {\n\t          // Previously the revision handling was done in adapter.js\n\t          // getAttachment, however since idb-next doesnt we need to\n\t          // pass the rev through\n\t          rev: doc._rev,\n\t          binary: opts.binary,\n\t          ctx: ctx\n\t        }, function (err, data) {\n\t          var att = doc._attachments[key];\n\t          att.data = data;\n\t          delete att.stub;\n\t          delete att.length;\n\t          if (!--count) {\n\t            cb(null, doc);\n\t          }\n\t        });\n\t      }, self);\n\t    } else {\n\t      if (doc._attachments) {\n\t        for (var key in doc._attachments) {\n\t          /* istanbul ignore else */\n\t          if (doc._attachments.hasOwnProperty(key)) {\n\t            doc._attachments[key].stub = true;\n\t          }\n\t        }\n\t      }\n\t      cb(null, doc);\n\t    }\n\t  });\n\t});\n\n\t// TODO: I dont like this, it forces an extra read for every\n\t// attachment read and enforces a confusing api between\n\t// adapter.js and the adapter implementation\n\tAbstractPouchDB.prototype.getAttachment =\n\t  adapterFun('getAttachment', function (docId, attachmentId, opts, callback) {\n\t  var self = this;\n\t  if (opts instanceof Function) {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  this._get(docId, opts, function (err, res) {\n\t    if (err) {\n\t      return callback(err);\n\t    }\n\t    if (res.doc._attachments && res.doc._attachments[attachmentId]) {\n\t      opts.ctx = res.ctx;\n\t      opts.binary = true;\n\t      self._getAttachment(docId, attachmentId,\n\t                          res.doc._attachments[attachmentId], opts, callback);\n\t    } else {\n\t      return callback(createError(MISSING_DOC));\n\t    }\n\t  });\n\t});\n\n\tAbstractPouchDB.prototype.allDocs =\n\t  adapterFun('allDocs', function (opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  opts.skip = typeof opts.skip !== 'undefined' ? opts.skip : 0;\n\t  if (opts.start_key) {\n\t    opts.startkey = opts.start_key;\n\t  }\n\t  if (opts.end_key) {\n\t    opts.endkey = opts.end_key;\n\t  }\n\t  if ('keys' in opts) {\n\t    if (!Array.isArray(opts.keys)) {\n\t      return callback(new TypeError('options.keys must be an array'));\n\t    }\n\t    var incompatibleOpt =\n\t      ['startkey', 'endkey', 'key'].filter(function (incompatibleOpt) {\n\t      return incompatibleOpt in opts;\n\t    })[0];\n\t    if (incompatibleOpt) {\n\t      callback(createError(QUERY_PARSE_ERROR,\n\t        'Query parameter `' + incompatibleOpt +\n\t        '` is not compatible with multi-get'\n\t      ));\n\t      return;\n\t    }\n\t    if (!isRemote(this)) {\n\t      allDocsKeysParse(opts);\n\t      if (opts.keys.length === 0) {\n\t        return this._allDocs({limit: 0}, callback);\n\t      }\n\t    }\n\t  }\n\n\t  return this._allDocs(opts, callback);\n\t});\n\n\tAbstractPouchDB.prototype.changes = function (opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\n\t  opts = opts || {};\n\n\t  // By default set return_docs to false if the caller has opts.live = true,\n\t  // this will prevent us from collecting the set of changes indefinitely\n\t  // resulting in growing memory\n\t  opts.return_docs = ('return_docs' in opts) ? opts.return_docs : !opts.live;\n\n\t  return new Changes$1(this, opts, callback);\n\t};\n\n\tAbstractPouchDB.prototype.close = adapterFun('close', function (callback) {\n\t  this._closed = true;\n\t  this.emit('closed');\n\t  return this._close(callback);\n\t});\n\n\tAbstractPouchDB.prototype.info = adapterFun('info', function (callback) {\n\t  var self = this;\n\t  this._info(function (err, info) {\n\t    if (err) {\n\t      return callback(err);\n\t    }\n\t    // assume we know better than the adapter, unless it informs us\n\t    info.db_name = info.db_name || self.name;\n\t    info.auto_compaction = !!(self.auto_compaction && !isRemote(self));\n\t    info.adapter = self.adapter;\n\t    callback(null, info);\n\t  });\n\t});\n\n\tAbstractPouchDB.prototype.id = adapterFun('id', function (callback) {\n\t  return this._id(callback);\n\t});\n\n\t/* istanbul ignore next */\n\tAbstractPouchDB.prototype.type = function () {\n\t  return (typeof this._type === 'function') ? this._type() : this.adapter;\n\t};\n\n\tAbstractPouchDB.prototype.bulkDocs =\n\t  adapterFun('bulkDocs', function (req, opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\n\t  opts = opts || {};\n\n\t  if (Array.isArray(req)) {\n\t    req = {\n\t      docs: req\n\t    };\n\t  }\n\n\t  if (!req || !req.docs || !Array.isArray(req.docs)) {\n\t    return callback(createError(MISSING_BULK_DOCS));\n\t  }\n\n\t  for (var i = 0; i < req.docs.length; ++i) {\n\t    if (typeof req.docs[i] !== 'object' || Array.isArray(req.docs[i])) {\n\t      return callback(createError(NOT_AN_OBJECT));\n\t    }\n\t  }\n\n\t  var attachmentError;\n\t  req.docs.forEach(function (doc) {\n\t    if (doc._attachments) {\n\t      Object.keys(doc._attachments).forEach(function (name) {\n\t        attachmentError = attachmentError || attachmentNameError(name);\n\t        if (!doc._attachments[name].content_type) {\n\t          guardedConsole('warn', 'Attachment', name, 'on document', doc._id, 'is missing content_type');\n\t        }\n\t      });\n\t    }\n\t  });\n\n\t  if (attachmentError) {\n\t    return callback(createError(BAD_REQUEST, attachmentError));\n\t  }\n\n\t  if (!('new_edits' in opts)) {\n\t    if ('new_edits' in req) {\n\t      opts.new_edits = req.new_edits;\n\t    } else {\n\t      opts.new_edits = true;\n\t    }\n\t  }\n\n\t  var adapter = this;\n\t  if (!opts.new_edits && !isRemote(adapter)) {\n\t    // ensure revisions of the same doc are sorted, so that\n\t    // the local adapter processes them correctly (#2935)\n\t    req.docs.sort(compareByIdThenRev);\n\t  }\n\n\t  cleanDocs(req.docs);\n\n\t  // in the case of conflicts, we want to return the _ids to the user\n\t  // however, the underlying adapter may destroy the docs array, so\n\t  // create a copy here\n\t  var ids = req.docs.map(function (doc) {\n\t    return doc._id;\n\t  });\n\n\t  return this._bulkDocs(req, opts, function (err, res) {\n\t    if (err) {\n\t      return callback(err);\n\t    }\n\t    if (!opts.new_edits) {\n\t      // this is what couch does when new_edits is false\n\t      res = res.filter(function (x) {\n\t        return x.error;\n\t      });\n\t    }\n\t    // add ids for error/conflict responses (not required for CouchDB)\n\t    if (!isRemote(adapter)) {\n\t      for (var i = 0, l = res.length; i < l; i++) {\n\t        res[i].id = res[i].id || ids[i];\n\t      }\n\t    }\n\n\t    callback(null, res);\n\t  });\n\t});\n\n\tAbstractPouchDB.prototype.registerDependentDatabase =\n\t  adapterFun('registerDependentDatabase', function (dependentDb,\n\t                                                          callback) {\n\t  var depDB = new this.constructor(dependentDb, this.__opts);\n\n\t  function diffFun(doc) {\n\t    doc.dependentDbs = doc.dependentDbs || {};\n\t    if (doc.dependentDbs[dependentDb]) {\n\t      return false; // no update required\n\t    }\n\t    doc.dependentDbs[dependentDb] = true;\n\t    return doc;\n\t  }\n\t  upsert(this, '_local/_pouch_dependentDbs', diffFun)\n\t    .then(function () {\n\t      callback(null, {db: depDB});\n\t    }).catch(callback);\n\t});\n\n\tAbstractPouchDB.prototype.destroy =\n\t  adapterFun('destroy', function (opts, callback) {\n\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\n\t  var self = this;\n\t  var usePrefix = 'use_prefix' in self ? self.use_prefix : true;\n\n\t  function destroyDb() {\n\t    // call destroy method of the particular adaptor\n\t    self._destroy(opts, function (err, resp) {\n\t      if (err) {\n\t        return callback(err);\n\t      }\n\t      self._destroyed = true;\n\t      self.emit('destroyed');\n\t      callback(null, resp || { 'ok': true });\n\t    });\n\t  }\n\n\t  if (isRemote(self)) {\n\t    // no need to check for dependent DBs if it's a remote DB\n\t    return destroyDb();\n\t  }\n\n\t  self.get('_local/_pouch_dependentDbs', function (err, localDoc) {\n\t    if (err) {\n\t      /* istanbul ignore if */\n\t      if (err.status !== 404) {\n\t        return callback(err);\n\t      } else { // no dependencies\n\t        return destroyDb();\n\t      }\n\t    }\n\t    var dependentDbs = localDoc.dependentDbs;\n\t    var PouchDB = self.constructor;\n\t    var deletedMap = Object.keys(dependentDbs).map(function (name) {\n\t      // use_prefix is only false in the browser\n\t      /* istanbul ignore next */\n\t      var trueName = usePrefix ?\n\t        name.replace(new RegExp('^' + PouchDB.prefix), '') : name;\n\t      return new PouchDB(trueName, self.__opts).destroy();\n\t    });\n\t    Promise.all(deletedMap).then(destroyDb, callback);\n\t  });\n\t});\n\n\tfunction TaskQueue() {\n\t  this.isReady = false;\n\t  this.failed = false;\n\t  this.queue = [];\n\t}\n\n\tTaskQueue.prototype.execute = function () {\n\t  var fun;\n\t  if (this.failed) {\n\t    while ((fun = this.queue.shift())) {\n\t      fun(this.failed);\n\t    }\n\t  } else {\n\t    while ((fun = this.queue.shift())) {\n\t      fun();\n\t    }\n\t  }\n\t};\n\n\tTaskQueue.prototype.fail = function (err) {\n\t  this.failed = err;\n\t  this.execute();\n\t};\n\n\tTaskQueue.prototype.ready = function (db) {\n\t  this.isReady = true;\n\t  this.db = db;\n\t  this.execute();\n\t};\n\n\tTaskQueue.prototype.addTask = function (fun) {\n\t  this.queue.push(fun);\n\t  if (this.failed) {\n\t    this.execute();\n\t  }\n\t};\n\n\tfunction parseAdapter(name, opts) {\n\t  var match = name.match(/([a-z-]*):\\/\\/(.*)/);\n\t  if (match) {\n\t    // the http adapter expects the fully qualified name\n\t    return {\n\t      name: /https?/.test(match[1]) ? match[1] + '://' + match[2] : match[2],\n\t      adapter: match[1]\n\t    };\n\t  }\n\n\t  var adapters = PouchDB.adapters;\n\t  var preferredAdapters = PouchDB.preferredAdapters;\n\t  var prefix = PouchDB.prefix;\n\t  var adapterName = opts.adapter;\n\n\t  if (!adapterName) { // automatically determine adapter\n\t    for (var i = 0; i < preferredAdapters.length; ++i) {\n\t      adapterName = preferredAdapters[i];\n\t      // check for browsers that have been upgraded from websql-only to websql+idb\n\t      /* istanbul ignore if */\n\t      if (adapterName === 'idb' && 'websql' in adapters &&\n\t          hasLocalStorage() && localStorage['_pouch__websqldb_' + prefix + name]) {\n\t        // log it, because this can be confusing during development\n\t        guardedConsole('log', 'PouchDB is downgrading \"' + name + '\" to WebSQL to' +\n\t          ' avoid data loss, because it was already opened with WebSQL.');\n\t        continue; // keep using websql to avoid user data loss\n\t      }\n\t      break;\n\t    }\n\t  }\n\n\t  var adapter = adapters[adapterName];\n\n\t  // if adapter is invalid, then an error will be thrown later\n\t  var usePrefix = (adapter && 'use_prefix' in adapter) ?\n\t    adapter.use_prefix : true;\n\n\t  return {\n\t    name: usePrefix ? (prefix + name) : name,\n\t    adapter: adapterName\n\t  };\n\t}\n\n\t// OK, so here's the deal. Consider this code:\n\t//     var db1 = new PouchDB('foo');\n\t//     var db2 = new PouchDB('foo');\n\t//     db1.destroy();\n\t// ^ these two both need to emit 'destroyed' events,\n\t// as well as the PouchDB constructor itself.\n\t// So we have one db object (whichever one got destroy() called on it)\n\t// responsible for emitting the initial event, which then gets emitted\n\t// by the constructor, which then broadcasts it to any other dbs\n\t// that may have been created with the same name.\n\tfunction prepareForDestruction(self) {\n\n\t  function onDestroyed(from_constructor) {\n\t    self.removeListener('closed', onClosed);\n\t    if (!from_constructor) {\n\t      self.constructor.emit('destroyed', self.name);\n\t    }\n\t  }\n\n\t  function onClosed() {\n\t    self.removeListener('destroyed', onDestroyed);\n\t    self.constructor.emit('unref', self);\n\t  }\n\n\t  self.once('destroyed', onDestroyed);\n\t  self.once('closed', onClosed);\n\t  self.constructor.emit('ref', self);\n\t}\n\n\tinherits$2(PouchDB, AbstractPouchDB);\n\tfunction PouchDB(name, opts) {\n\t  // In Node our test suite only tests this for PouchAlt unfortunately\n\t  /* istanbul ignore if */\n\t  if (!(this instanceof PouchDB)) {\n\t    return new PouchDB(name, opts);\n\t  }\n\n\t  var self = this;\n\t  opts = opts || {};\n\n\t  if (name && typeof name === 'object') {\n\t    opts = name;\n\t    name = opts.name;\n\t    delete opts.name;\n\t  }\n\n\t  if (opts.deterministic_revs === undefined) {\n\t    opts.deterministic_revs = true;\n\t  }\n\n\t  this.__opts = opts = clone(opts);\n\n\t  self.auto_compaction = opts.auto_compaction;\n\t  self.prefix = PouchDB.prefix;\n\n\t  if (typeof name !== 'string') {\n\t    throw new Error('Missing/invalid DB name');\n\t  }\n\n\t  var prefixedName = (opts.prefix || '') + name;\n\t  var backend = parseAdapter(prefixedName, opts);\n\n\t  opts.name = backend.name;\n\t  opts.adapter = opts.adapter || backend.adapter;\n\n\t  self.name = name;\n\t  self._adapter = opts.adapter;\n\t  PouchDB.emit('debug', ['adapter', 'Picked adapter: ', opts.adapter]);\n\n\t  if (!PouchDB.adapters[opts.adapter] ||\n\t      !PouchDB.adapters[opts.adapter].valid()) {\n\t    throw new Error('Invalid Adapter: ' + opts.adapter);\n\t  }\n\n\t  AbstractPouchDB.call(self);\n\t  self.taskqueue = new TaskQueue();\n\n\t  self.adapter = opts.adapter;\n\n\t  PouchDB.adapters[opts.adapter].call(self, opts, function (err) {\n\t    if (err) {\n\t      return self.taskqueue.fail(err);\n\t    }\n\t    prepareForDestruction(self);\n\n\t    self.emit('created', self);\n\t    PouchDB.emit('created', self.name);\n\t    self.taskqueue.ready(self);\n\t  });\n\n\t}\n\n\t// AbortController was introduced quite a while after fetch and\n\t// isnt required for PouchDB to function so polyfill if needed\n\tvar a = (typeof AbortController !== 'undefined')\n\t    ? AbortController\n\t    : function () { return {abort: function () {}}; };\n\n\tvar f$1 = fetch;\n\tvar h = Headers;\n\n\tPouchDB.adapters = {};\n\tPouchDB.preferredAdapters = [];\n\n\tPouchDB.prefix = '_pouch_';\n\n\tvar eventEmitter = new EventEmitter();\n\n\tfunction setUpEventEmitter(Pouch) {\n\t  Object.keys(EventEmitter.prototype).forEach(function (key) {\n\t    if (typeof EventEmitter.prototype[key] === 'function') {\n\t      Pouch[key] = eventEmitter[key].bind(eventEmitter);\n\t    }\n\t  });\n\n\t  // these are created in constructor.js, and allow us to notify each DB with\n\t  // the same name that it was destroyed, via the constructor object\n\t  var destructListeners = Pouch._destructionListeners = new ExportedMap();\n\n\t  Pouch.on('ref', function onConstructorRef(db) {\n\t    if (!destructListeners.has(db.name)) {\n\t      destructListeners.set(db.name, []);\n\t    }\n\t    destructListeners.get(db.name).push(db);\n\t  });\n\n\t  Pouch.on('unref', function onConstructorUnref(db) {\n\t    if (!destructListeners.has(db.name)) {\n\t      return;\n\t    }\n\t    var dbList = destructListeners.get(db.name);\n\t    var pos = dbList.indexOf(db);\n\t    if (pos < 0) {\n\t      /* istanbul ignore next */\n\t      return;\n\t    }\n\t    dbList.splice(pos, 1);\n\t    if (dbList.length > 1) {\n\t      /* istanbul ignore next */\n\t      destructListeners.set(db.name, dbList);\n\t    } else {\n\t      destructListeners.delete(db.name);\n\t    }\n\t  });\n\n\t  Pouch.on('destroyed', function onConstructorDestroyed(name) {\n\t    if (!destructListeners.has(name)) {\n\t      return;\n\t    }\n\t    var dbList = destructListeners.get(name);\n\t    destructListeners.delete(name);\n\t    dbList.forEach(function (db) {\n\t      db.emit('destroyed',true);\n\t    });\n\t  });\n\t}\n\n\tsetUpEventEmitter(PouchDB);\n\n\tPouchDB.adapter = function (id, obj, addToPreferredAdapters) {\n\t  /* istanbul ignore else */\n\t  if (obj.valid()) {\n\t    PouchDB.adapters[id] = obj;\n\t    if (addToPreferredAdapters) {\n\t      PouchDB.preferredAdapters.push(id);\n\t    }\n\t  }\n\t};\n\n\tPouchDB.plugin = function (obj) {\n\t  if (typeof obj === 'function') { // function style for plugins\n\t    obj(PouchDB);\n\t  } else if (typeof obj !== 'object' || Object.keys(obj).length === 0) {\n\t    throw new Error('Invalid plugin: got \"' + obj + '\", expected an object or a function');\n\t  } else {\n\t    Object.keys(obj).forEach(function (id) { // object style for plugins\n\t      PouchDB.prototype[id] = obj[id];\n\t    });\n\t  }\n\t  if (this.__defaults) {\n\t    PouchDB.__defaults = $inject_Object_assign({}, this.__defaults);\n\t  }\n\t  return PouchDB;\n\t};\n\n\tPouchDB.defaults = function (defaultOpts) {\n\t  function PouchAlt(name, opts) {\n\t    if (!(this instanceof PouchAlt)) {\n\t      return new PouchAlt(name, opts);\n\t    }\n\n\t    opts = opts || {};\n\n\t    if (name && typeof name === 'object') {\n\t      opts = name;\n\t      name = opts.name;\n\t      delete opts.name;\n\t    }\n\n\t    opts = $inject_Object_assign({}, PouchAlt.__defaults, opts);\n\t    PouchDB.call(this, name, opts);\n\t  }\n\n\t  inherits$2(PouchAlt, PouchDB);\n\n\t  PouchAlt.preferredAdapters = PouchDB.preferredAdapters.slice();\n\t  Object.keys(PouchDB).forEach(function (key) {\n\t    if (!(key in PouchAlt)) {\n\t      PouchAlt[key] = PouchDB[key];\n\t    }\n\t  });\n\n\t  // make default options transitive\n\t  // https://github.com/pouchdb/pouchdb/issues/5922\n\t  PouchAlt.__defaults = $inject_Object_assign({}, this.__defaults, defaultOpts);\n\n\t  return PouchAlt;\n\t};\n\n\tPouchDB.fetch = function (url, opts) {\n\t  return f$1(url, opts);\n\t};\n\n\t// managed automatically by set-version.js\n\tvar version = \"7.2.1\";\n\n\t// this would just be \"return doc[field]\", but fields\n\t// can be \"deep\" due to dot notation\n\tfunction getFieldFromDoc(doc, parsedField) {\n\t  var value = doc;\n\t  for (var i = 0, len = parsedField.length; i < len; i++) {\n\t    var key = parsedField[i];\n\t    value = value[key];\n\t    if (!value) {\n\t      break;\n\t    }\n\t  }\n\t  return value;\n\t}\n\n\tfunction compare$1(left, right) {\n\t  return left < right ? -1 : left > right ? 1 : 0;\n\t}\n\n\t// Converts a string in dot notation to an array of its components, with backslash escaping\n\tfunction parseField(fieldName) {\n\t  // fields may be deep (e.g. \"foo.bar.baz\"), so parse\n\t  var fields = [];\n\t  var current = '';\n\t  for (var i = 0, len = fieldName.length; i < len; i++) {\n\t    var ch = fieldName[i];\n\t    if (ch === '.') {\n\t      if (i > 0 && fieldName[i - 1] === '\\\\') { // escaped delimiter\n\t        current = current.substring(0, current.length - 1) + '.';\n\t      } else { // not escaped, so delimiter\n\t        fields.push(current);\n\t        current = '';\n\t      }\n\t    } else { // normal character\n\t      current += ch;\n\t    }\n\t  }\n\t  fields.push(current);\n\t  return fields;\n\t}\n\n\tvar combinationFields = ['$or', '$nor', '$not'];\n\tfunction isCombinationalField(field) {\n\t  return combinationFields.indexOf(field) > -1;\n\t}\n\n\tfunction getKey(obj) {\n\t  return Object.keys(obj)[0];\n\t}\n\n\tfunction getValue(obj) {\n\t  return obj[getKey(obj)];\n\t}\n\n\n\t// flatten an array of selectors joined by an $and operator\n\tfunction mergeAndedSelectors(selectors) {\n\n\t  // sort to ensure that e.g. if the user specified\n\t  // $and: [{$gt: 'a'}, {$gt: 'b'}], then it's collapsed into\n\t  // just {$gt: 'b'}\n\t  var res = {};\n\n\t  selectors.forEach(function (selector) {\n\t    Object.keys(selector).forEach(function (field) {\n\t      var matcher = selector[field];\n\t      if (typeof matcher !== 'object') {\n\t        matcher = {$eq: matcher};\n\t      }\n\n\t      if (isCombinationalField(field)) {\n\t        if (matcher instanceof Array) {\n\t          res[field] = matcher.map(function (m) {\n\t            return mergeAndedSelectors([m]);\n\t          });\n\t        } else {\n\t          res[field] = mergeAndedSelectors([matcher]);\n\t        }\n\t      } else {\n\t        var fieldMatchers = res[field] = res[field] || {};\n\t        Object.keys(matcher).forEach(function (operator) {\n\t          var value = matcher[operator];\n\n\t          if (operator === '$gt' || operator === '$gte') {\n\t            return mergeGtGte(operator, value, fieldMatchers);\n\t          } else if (operator === '$lt' || operator === '$lte') {\n\t            return mergeLtLte(operator, value, fieldMatchers);\n\t          } else if (operator === '$ne') {\n\t            return mergeNe(value, fieldMatchers);\n\t          } else if (operator === '$eq') {\n\t            return mergeEq(value, fieldMatchers);\n\t          }\n\t          fieldMatchers[operator] = value;\n\t        });\n\t      }\n\t    });\n\t  });\n\n\t  return res;\n\t}\n\n\n\n\t// collapse logically equivalent gt/gte values\n\tfunction mergeGtGte(operator, value, fieldMatchers) {\n\t  if (typeof fieldMatchers.$eq !== 'undefined') {\n\t    return; // do nothing\n\t  }\n\t  if (typeof fieldMatchers.$gte !== 'undefined') {\n\t    if (operator === '$gte') {\n\t      if (value > fieldMatchers.$gte) { // more specificity\n\t        fieldMatchers.$gte = value;\n\t      }\n\t    } else { // operator === '$gt'\n\t      if (value >= fieldMatchers.$gte) { // more specificity\n\t        delete fieldMatchers.$gte;\n\t        fieldMatchers.$gt = value;\n\t      }\n\t    }\n\t  } else if (typeof fieldMatchers.$gt !== 'undefined') {\n\t    if (operator === '$gte') {\n\t      if (value > fieldMatchers.$gt) { // more specificity\n\t        delete fieldMatchers.$gt;\n\t        fieldMatchers.$gte = value;\n\t      }\n\t    } else { // operator === '$gt'\n\t      if (value > fieldMatchers.$gt) { // more specificity\n\t        fieldMatchers.$gt = value;\n\t      }\n\t    }\n\t  } else {\n\t    fieldMatchers[operator] = value;\n\t  }\n\t}\n\n\t// collapse logically equivalent lt/lte values\n\tfunction mergeLtLte(operator, value, fieldMatchers) {\n\t  if (typeof fieldMatchers.$eq !== 'undefined') {\n\t    return; // do nothing\n\t  }\n\t  if (typeof fieldMatchers.$lte !== 'undefined') {\n\t    if (operator === '$lte') {\n\t      if (value < fieldMatchers.$lte) { // more specificity\n\t        fieldMatchers.$lte = value;\n\t      }\n\t    } else { // operator === '$gt'\n\t      if (value <= fieldMatchers.$lte) { // more specificity\n\t        delete fieldMatchers.$lte;\n\t        fieldMatchers.$lt = value;\n\t      }\n\t    }\n\t  } else if (typeof fieldMatchers.$lt !== 'undefined') {\n\t    if (operator === '$lte') {\n\t      if (value < fieldMatchers.$lt) { // more specificity\n\t        delete fieldMatchers.$lt;\n\t        fieldMatchers.$lte = value;\n\t      }\n\t    } else { // operator === '$gt'\n\t      if (value < fieldMatchers.$lt) { // more specificity\n\t        fieldMatchers.$lt = value;\n\t      }\n\t    }\n\t  } else {\n\t    fieldMatchers[operator] = value;\n\t  }\n\t}\n\n\t// combine $ne values into one array\n\tfunction mergeNe(value, fieldMatchers) {\n\t  if ('$ne' in fieldMatchers) {\n\t    // there are many things this could \"not\" be\n\t    fieldMatchers.$ne.push(value);\n\t  } else { // doesn't exist yet\n\t    fieldMatchers.$ne = [value];\n\t  }\n\t}\n\n\t// add $eq into the mix\n\tfunction mergeEq(value, fieldMatchers) {\n\t  // these all have less specificity than the $eq\n\t  // TODO: check for user errors here\n\t  delete fieldMatchers.$gt;\n\t  delete fieldMatchers.$gte;\n\t  delete fieldMatchers.$lt;\n\t  delete fieldMatchers.$lte;\n\t  delete fieldMatchers.$ne;\n\t  fieldMatchers.$eq = value;\n\t}\n\n\t//#7458: execute function mergeAndedSelectors on nested $and\n\tfunction mergeAndedSelectorsNested(obj) {\n\t    for (var prop in obj) {\n\t        if (Array.isArray(obj)) {\n\t            for (var i in obj) {\n\t                if (obj[i]['$and']) {\n\t                    obj[i] = mergeAndedSelectors(obj[i]['$and']);\n\t                }\n\t            }\n\t        }\n\t        var value = obj[prop];\n\t        if (typeof value === 'object') {\n\t            mergeAndedSelectorsNested(value); // <- recursive call\n\t        }\n\t    }\n\t    return obj;\n\t}\n\n\t//#7458: determine id $and is present in selector (at any level)\n\tfunction isAndInSelector(obj, isAnd) {\n\t    for (var prop in obj) {\n\t        if (prop === '$and') {\n\t            isAnd = true;\n\t        }\n\t        var value = obj[prop];\n\t        if (typeof value === 'object') {\n\t            isAnd = isAndInSelector(value, isAnd); // <- recursive call\n\t        }\n\t    }\n\t    return isAnd;\n\t}\n\n\t//\n\t// normalize the selector\n\t//\n\tfunction massageSelector(input) {\n\t  var result = clone(input);\n\t  var wasAnded = false;\n\t    //#7458: if $and is present in selector (at any level) merge nested $and\n\t    if (isAndInSelector(result, false)) {\n\t        result = mergeAndedSelectorsNested(result);\n\t        if ('$and' in result) {\n\t            result = mergeAndedSelectors(result['$and']);\n\t        }\n\t        wasAnded = true;\n\t    }\n\n\t  ['$or', '$nor'].forEach(function (orOrNor) {\n\t    if (orOrNor in result) {\n\t      // message each individual selector\n\t      // e.g. {foo: 'bar'} becomes {foo: {$eq: 'bar'}}\n\t      result[orOrNor].forEach(function (subSelector) {\n\t        var fields = Object.keys(subSelector);\n\t        for (var i = 0; i < fields.length; i++) {\n\t          var field = fields[i];\n\t          var matcher = subSelector[field];\n\t          if (typeof matcher !== 'object' || matcher === null) {\n\t            subSelector[field] = {$eq: matcher};\n\t          }\n\t        }\n\t      });\n\t    }\n\t  });\n\n\t  if ('$not' in result) {\n\t    //This feels a little like forcing, but it will work for now,\n\t    //I would like to come back to this and make the merging of selectors a little more generic\n\t    result['$not'] = mergeAndedSelectors([result['$not']]);\n\t  }\n\n\t  var fields = Object.keys(result);\n\n\t  for (var i = 0; i < fields.length; i++) {\n\t    var field = fields[i];\n\t    var matcher = result[field];\n\n\t    if (typeof matcher !== 'object' || matcher === null) {\n\t      matcher = {$eq: matcher};\n\t    } else if ('$ne' in matcher && !wasAnded) {\n\t      // I put these in an array, since there may be more than one\n\t      // but in the \"mergeAnded\" operation, I already take care of that\n\t      matcher.$ne = [matcher.$ne];\n\t    }\n\t    result[field] = matcher;\n\t  }\n\n\t  return result;\n\t}\n\n\tfunction pad$1(str, padWith, upToLength) {\n\t  var padding = '';\n\t  var targetLength = upToLength - str.length;\n\t  /* istanbul ignore next */\n\t  while (padding.length < targetLength) {\n\t    padding += padWith;\n\t  }\n\t  return padding;\n\t}\n\n\tfunction padLeft(str, padWith, upToLength) {\n\t  var padding = pad$1(str, padWith, upToLength);\n\t  return padding + str;\n\t}\n\n\tvar MIN_MAGNITUDE = -324; // verified by -Number.MIN_VALUE\n\tvar MAGNITUDE_DIGITS = 3; // ditto\n\tvar SEP = ''; // set to '_' for easier debugging \n\n\tfunction collate(a, b) {\n\n\t  if (a === b) {\n\t    return 0;\n\t  }\n\n\t  a = normalizeKey(a);\n\t  b = normalizeKey(b);\n\n\t  var ai = collationIndex(a);\n\t  var bi = collationIndex(b);\n\t  if ((ai - bi) !== 0) {\n\t    return ai - bi;\n\t  }\n\t  switch (typeof a) {\n\t    case 'number':\n\t      return a - b;\n\t    case 'boolean':\n\t      return a < b ? -1 : 1;\n\t    case 'string':\n\t      return stringCollate(a, b);\n\t  }\n\t  return Array.isArray(a) ? arrayCollate(a, b) : objectCollate(a, b);\n\t}\n\n\t// couch considers null/NaN/Infinity/-Infinity === undefined,\n\t// for the purposes of mapreduce indexes. also, dates get stringified.\n\tfunction normalizeKey(key) {\n\t  switch (typeof key) {\n\t    case 'undefined':\n\t      return null;\n\t    case 'number':\n\t      if (key === Infinity || key === -Infinity || isNaN(key)) {\n\t        return null;\n\t      }\n\t      return key;\n\t    case 'object':\n\t      var origKey = key;\n\t      if (Array.isArray(key)) {\n\t        var len = key.length;\n\t        key = new Array(len);\n\t        for (var i = 0; i < len; i++) {\n\t          key[i] = normalizeKey(origKey[i]);\n\t        }\n\t      /* istanbul ignore next */\n\t      } else if (key instanceof Date) {\n\t        return key.toJSON();\n\t      } else if (key !== null) { // generic object\n\t        key = {};\n\t        for (var k in origKey) {\n\t          if (origKey.hasOwnProperty(k)) {\n\t            var val = origKey[k];\n\t            if (typeof val !== 'undefined') {\n\t              key[k] = normalizeKey(val);\n\t            }\n\t          }\n\t        }\n\t      }\n\t  }\n\t  return key;\n\t}\n\n\tfunction indexify(key) {\n\t  if (key !== null) {\n\t    switch (typeof key) {\n\t      case 'boolean':\n\t        return key ? 1 : 0;\n\t      case 'number':\n\t        return numToIndexableString(key);\n\t      case 'string':\n\t        // We've to be sure that key does not contain \\u0000\n\t        // Do order-preserving replacements:\n\t        // 0 -> 1, 1\n\t        // 1 -> 1, 2\n\t        // 2 -> 2, 2\n\t        /* eslint-disable no-control-regex */\n\t        return key\n\t          .replace(/\\u0002/g, '\\u0002\\u0002')\n\t          .replace(/\\u0001/g, '\\u0001\\u0002')\n\t          .replace(/\\u0000/g, '\\u0001\\u0001');\n\t        /* eslint-enable no-control-regex */\n\t      case 'object':\n\t        var isArray = Array.isArray(key);\n\t        var arr = isArray ? key : Object.keys(key);\n\t        var i = -1;\n\t        var len = arr.length;\n\t        var result = '';\n\t        if (isArray) {\n\t          while (++i < len) {\n\t            result += toIndexableString(arr[i]);\n\t          }\n\t        } else {\n\t          while (++i < len) {\n\t            var objKey = arr[i];\n\t            result += toIndexableString(objKey) +\n\t                toIndexableString(key[objKey]);\n\t          }\n\t        }\n\t        return result;\n\t    }\n\t  }\n\t  return '';\n\t}\n\n\t// convert the given key to a string that would be appropriate\n\t// for lexical sorting, e.g. within a database, where the\n\t// sorting is the same given by the collate() function.\n\tfunction toIndexableString(key) {\n\t  var zero = '\\u0000';\n\t  key = normalizeKey(key);\n\t  return collationIndex(key) + SEP + indexify(key) + zero;\n\t}\n\n\tfunction parseNumber(str, i) {\n\t  var originalIdx = i;\n\t  var num;\n\t  var zero = str[i] === '1';\n\t  if (zero) {\n\t    num = 0;\n\t    i++;\n\t  } else {\n\t    var neg = str[i] === '0';\n\t    i++;\n\t    var numAsString = '';\n\t    var magAsString = str.substring(i, i + MAGNITUDE_DIGITS);\n\t    var magnitude = parseInt(magAsString, 10) + MIN_MAGNITUDE;\n\t    /* istanbul ignore next */\n\t    if (neg) {\n\t      magnitude = -magnitude;\n\t    }\n\t    i += MAGNITUDE_DIGITS;\n\t    while (true) {\n\t      var ch = str[i];\n\t      if (ch === '\\u0000') {\n\t        break;\n\t      } else {\n\t        numAsString += ch;\n\t      }\n\t      i++;\n\t    }\n\t    numAsString = numAsString.split('.');\n\t    if (numAsString.length === 1) {\n\t      num = parseInt(numAsString, 10);\n\t    } else {\n\t      /* istanbul ignore next */\n\t      num = parseFloat(numAsString[0] + '.' + numAsString[1]);\n\t    }\n\t    /* istanbul ignore next */\n\t    if (neg) {\n\t      num = num - 10;\n\t    }\n\t    /* istanbul ignore next */\n\t    if (magnitude !== 0) {\n\t      // parseFloat is more reliable than pow due to rounding errors\n\t      // e.g. Number.MAX_VALUE would return Infinity if we did\n\t      // num * Math.pow(10, magnitude);\n\t      num = parseFloat(num + 'e' + magnitude);\n\t    }\n\t  }\n\t  return {num: num, length : i - originalIdx};\n\t}\n\n\t// move up the stack while parsing\n\t// this function moved outside of parseIndexableString for performance\n\tfunction pop$1(stack, metaStack) {\n\t  var obj = stack.pop();\n\n\t  if (metaStack.length) {\n\t    var lastMetaElement = metaStack[metaStack.length - 1];\n\t    if (obj === lastMetaElement.element) {\n\t      // popping a meta-element, e.g. an object whose value is another object\n\t      metaStack.pop();\n\t      lastMetaElement = metaStack[metaStack.length - 1];\n\t    }\n\t    var element = lastMetaElement.element;\n\t    var lastElementIndex = lastMetaElement.index;\n\t    if (Array.isArray(element)) {\n\t      element.push(obj);\n\t    } else if (lastElementIndex === stack.length - 2) { // obj with key+value\n\t      var key = stack.pop();\n\t      element[key] = obj;\n\t    } else {\n\t      stack.push(obj); // obj with key only\n\t    }\n\t  }\n\t}\n\n\tfunction parseIndexableString(str) {\n\t  var stack = [];\n\t  var metaStack = []; // stack for arrays and objects\n\t  var i = 0;\n\n\t  /*eslint no-constant-condition: [\"error\", { \"checkLoops\": false }]*/\n\t  while (true) {\n\t    var collationIndex = str[i++];\n\t    if (collationIndex === '\\u0000') {\n\t      if (stack.length === 1) {\n\t        return stack.pop();\n\t      } else {\n\t        pop$1(stack, metaStack);\n\t        continue;\n\t      }\n\t    }\n\t    switch (collationIndex) {\n\t      case '1':\n\t        stack.push(null);\n\t        break;\n\t      case '2':\n\t        stack.push(str[i] === '1');\n\t        i++;\n\t        break;\n\t      case '3':\n\t        var parsedNum = parseNumber(str, i);\n\t        stack.push(parsedNum.num);\n\t        i += parsedNum.length;\n\t        break;\n\t      case '4':\n\t        var parsedStr = '';\n\t        /*eslint no-constant-condition: [\"error\", { \"checkLoops\": false }]*/\n\t        while (true) {\n\t          var ch = str[i];\n\t          if (ch === '\\u0000') {\n\t            break;\n\t          }\n\t          parsedStr += ch;\n\t          i++;\n\t        }\n\t        // perform the reverse of the order-preserving replacement\n\t        // algorithm (see above)\n\t        /* eslint-disable no-control-regex */\n\t        parsedStr = parsedStr.replace(/\\u0001\\u0001/g, '\\u0000')\n\t          .replace(/\\u0001\\u0002/g, '\\u0001')\n\t          .replace(/\\u0002\\u0002/g, '\\u0002');\n\t        /* eslint-enable no-control-regex */\n\t        stack.push(parsedStr);\n\t        break;\n\t      case '5':\n\t        var arrayElement = { element: [], index: stack.length };\n\t        stack.push(arrayElement.element);\n\t        metaStack.push(arrayElement);\n\t        break;\n\t      case '6':\n\t        var objElement = { element: {}, index: stack.length };\n\t        stack.push(objElement.element);\n\t        metaStack.push(objElement);\n\t        break;\n\t      /* istanbul ignore next */\n\t      default:\n\t        throw new Error(\n\t          'bad collationIndex or unexpectedly reached end of input: ' +\n\t            collationIndex);\n\t    }\n\t  }\n\t}\n\n\tfunction arrayCollate(a, b) {\n\t  var len = Math.min(a.length, b.length);\n\t  for (var i = 0; i < len; i++) {\n\t    var sort = collate(a[i], b[i]);\n\t    if (sort !== 0) {\n\t      return sort;\n\t    }\n\t  }\n\t  return (a.length === b.length) ? 0 :\n\t    (a.length > b.length) ? 1 : -1;\n\t}\n\tfunction stringCollate(a, b) {\n\t  // See: https://github.com/daleharvey/pouchdb/issues/40\n\t  // This is incompatible with the CouchDB implementation, but its the\n\t  // best we can do for now\n\t  return (a === b) ? 0 : ((a > b) ? 1 : -1);\n\t}\n\tfunction objectCollate(a, b) {\n\t  var ak = Object.keys(a), bk = Object.keys(b);\n\t  var len = Math.min(ak.length, bk.length);\n\t  for (var i = 0; i < len; i++) {\n\t    // First sort the keys\n\t    var sort = collate(ak[i], bk[i]);\n\t    if (sort !== 0) {\n\t      return sort;\n\t    }\n\t    // if the keys are equal sort the values\n\t    sort = collate(a[ak[i]], b[bk[i]]);\n\t    if (sort !== 0) {\n\t      return sort;\n\t    }\n\n\t  }\n\t  return (ak.length === bk.length) ? 0 :\n\t    (ak.length > bk.length) ? 1 : -1;\n\t}\n\t// The collation is defined by erlangs ordered terms\n\t// the atoms null, true, false come first, then numbers, strings,\n\t// arrays, then objects\n\t// null/undefined/NaN/Infinity/-Infinity are all considered null\n\tfunction collationIndex(x) {\n\t  var id = ['boolean', 'number', 'string', 'object'];\n\t  var idx = id.indexOf(typeof x);\n\t  //false if -1 otherwise true, but fast!!!!1\n\t  if (~idx) {\n\t    if (x === null) {\n\t      return 1;\n\t    }\n\t    if (Array.isArray(x)) {\n\t      return 5;\n\t    }\n\t    return idx < 3 ? (idx + 2) : (idx + 3);\n\t  }\n\t  /* istanbul ignore next */\n\t  if (Array.isArray(x)) {\n\t    return 5;\n\t  }\n\t}\n\n\t// conversion:\n\t// x yyy zz...zz\n\t// x = 0 for negative, 1 for 0, 2 for positive\n\t// y = exponent (for negative numbers negated) moved so that it's >= 0\n\t// z = mantisse\n\tfunction numToIndexableString(num) {\n\n\t  if (num === 0) {\n\t    return '1';\n\t  }\n\n\t  // convert number to exponential format for easier and\n\t  // more succinct string sorting\n\t  var expFormat = num.toExponential().split(/e\\+?/);\n\t  var magnitude = parseInt(expFormat[1], 10);\n\n\t  var neg = num < 0;\n\n\t  var result = neg ? '0' : '2';\n\n\t  // first sort by magnitude\n\t  // it's easier if all magnitudes are positive\n\t  var magForComparison = ((neg ? -magnitude : magnitude) - MIN_MAGNITUDE);\n\t  var magString = padLeft((magForComparison).toString(), '0', MAGNITUDE_DIGITS);\n\n\t  result += SEP + magString;\n\n\t  // then sort by the factor\n\t  var factor = Math.abs(parseFloat(expFormat[0])); // [1..10)\n\t  /* istanbul ignore next */\n\t  if (neg) { // for negative reverse ordering\n\t    factor = 10 - factor;\n\t  }\n\n\t  var factorStr = factor.toFixed(20);\n\n\t  // strip zeros from the end\n\t  factorStr = factorStr.replace(/\\.?0+$/, '');\n\n\t  result += SEP + factorStr;\n\n\t  return result;\n\t}\n\n\t// create a comparator based on the sort object\n\tfunction createFieldSorter(sort) {\n\n\t  function getFieldValuesAsArray(doc) {\n\t    return sort.map(function (sorting) {\n\t      var fieldName = getKey(sorting);\n\t      var parsedField = parseField(fieldName);\n\t      var docFieldValue = getFieldFromDoc(doc, parsedField);\n\t      return docFieldValue;\n\t    });\n\t  }\n\n\t  return function (aRow, bRow) {\n\t    var aFieldValues = getFieldValuesAsArray(aRow.doc);\n\t    var bFieldValues = getFieldValuesAsArray(bRow.doc);\n\t    var collation = collate(aFieldValues, bFieldValues);\n\t    if (collation !== 0) {\n\t      return collation;\n\t    }\n\t    // this is what mango seems to do\n\t    return compare$1(aRow.doc._id, bRow.doc._id);\n\t  };\n\t}\n\n\tfunction filterInMemoryFields(rows, requestDef, inMemoryFields) {\n\t  rows = rows.filter(function (row) {\n\t    return rowFilter(row.doc, requestDef.selector, inMemoryFields);\n\t  });\n\n\t  if (requestDef.sort) {\n\t    // in-memory sort\n\t    var fieldSorter = createFieldSorter(requestDef.sort);\n\t    rows = rows.sort(fieldSorter);\n\t    if (typeof requestDef.sort[0] !== 'string' &&\n\t        getValue(requestDef.sort[0]) === 'desc') {\n\t      rows = rows.reverse();\n\t    }\n\t  }\n\n\t  if ('limit' in requestDef || 'skip' in requestDef) {\n\t    // have to do the limit in-memory\n\t    var skip = requestDef.skip || 0;\n\t    var limit = ('limit' in requestDef ? requestDef.limit : rows.length) + skip;\n\t    rows = rows.slice(skip, limit);\n\t  }\n\t  return rows;\n\t}\n\n\tfunction rowFilter(doc, selector, inMemoryFields) {\n\t  return inMemoryFields.every(function (field) {\n\t    var matcher = selector[field];\n\t    var parsedField = parseField(field);\n\t    var docFieldValue = getFieldFromDoc(doc, parsedField);\n\t    if (isCombinationalField(field)) {\n\t      return matchCominationalSelector(field, matcher, doc);\n\t    }\n\n\t    return matchSelector(matcher, doc, parsedField, docFieldValue);\n\t  });\n\t}\n\n\tfunction matchSelector(matcher, doc, parsedField, docFieldValue) {\n\t  if (!matcher) {\n\t    // no filtering necessary; this field is just needed for sorting\n\t    return true;\n\t  }\n\n\t  // is matcher an object, if so continue recursion\n\t  if (typeof matcher === 'object') {\n\t    return Object.keys(matcher).every(function (userOperator) {\n\t      var userValue = matcher[userOperator];\n\t      return match(userOperator, doc, userValue, parsedField, docFieldValue);\n\t    });\n\t  }\n\n\t  // no more depth, No need to recurse further\n\t  return matcher === docFieldValue;\n\t}\n\n\tfunction matchCominationalSelector(field, matcher, doc) {\n\n\t  if (field === '$or') {\n\t    return matcher.some(function (orMatchers) {\n\t      return rowFilter(doc, orMatchers, Object.keys(orMatchers));\n\t    });\n\t  }\n\n\t  if (field === '$not') {\n\t    return !rowFilter(doc, matcher, Object.keys(matcher));\n\t  }\n\n\t  //`$nor`\n\t  return !matcher.find(function (orMatchers) {\n\t    return rowFilter(doc, orMatchers, Object.keys(orMatchers));\n\t  });\n\n\t}\n\n\tfunction match(userOperator, doc, userValue, parsedField, docFieldValue) {\n\t  if (!matchers[userOperator]) {\n\t    throw new Error('unknown operator \"' + userOperator +\n\t      '\" - should be one of $eq, $lte, $lt, $gt, $gte, $exists, $ne, $in, ' +\n\t      '$nin, $size, $mod, $regex, $elemMatch, $type, $allMatch or $all');\n\t  }\n\t  return matchers[userOperator](doc, userValue, parsedField, docFieldValue);\n\t}\n\n\tfunction fieldExists(docFieldValue) {\n\t  return typeof docFieldValue !== 'undefined' && docFieldValue !== null;\n\t}\n\n\tfunction fieldIsNotUndefined(docFieldValue) {\n\t  return typeof docFieldValue !== 'undefined';\n\t}\n\n\tfunction modField(docFieldValue, userValue) {\n\t  var divisor = userValue[0];\n\t  var mod = userValue[1];\n\t  if (divisor === 0) {\n\t    throw new Error('Bad divisor, cannot divide by zero');\n\t  }\n\n\t  if (parseInt(divisor, 10) !== divisor ) {\n\t    throw new Error('Divisor is not an integer');\n\t  }\n\n\t  if (parseInt(mod, 10) !== mod ) {\n\t    throw new Error('Modulus is not an integer');\n\t  }\n\n\t  if (parseInt(docFieldValue, 10) !== docFieldValue) {\n\t    return false;\n\t  }\n\n\t  return docFieldValue % divisor === mod;\n\t}\n\n\tfunction arrayContainsValue(docFieldValue, userValue) {\n\t  return userValue.some(function (val) {\n\t    if (docFieldValue instanceof Array) {\n\t      return docFieldValue.indexOf(val) > -1;\n\t    }\n\n\t    return docFieldValue === val;\n\t  });\n\t}\n\n\tfunction arrayContainsAllValues(docFieldValue, userValue) {\n\t  return userValue.every(function (val) {\n\t    return docFieldValue.indexOf(val) > -1;\n\t  });\n\t}\n\n\tfunction arraySize(docFieldValue, userValue) {\n\t  return docFieldValue.length === userValue;\n\t}\n\n\tfunction regexMatch(docFieldValue, userValue) {\n\t  var re = new RegExp(userValue);\n\n\t  return re.test(docFieldValue);\n\t}\n\n\tfunction typeMatch(docFieldValue, userValue) {\n\n\t  switch (userValue) {\n\t    case 'null':\n\t      return docFieldValue === null;\n\t    case 'boolean':\n\t      return typeof (docFieldValue) === 'boolean';\n\t    case 'number':\n\t      return typeof (docFieldValue) === 'number';\n\t    case 'string':\n\t      return typeof (docFieldValue) === 'string';\n\t    case 'array':\n\t      return docFieldValue instanceof Array;\n\t    case 'object':\n\t      return ({}).toString.call(docFieldValue) === '[object Object]';\n\t  }\n\n\t  throw new Error(userValue + ' not supported as a type.' +\n\t                  'Please use one of object, string, array, number, boolean or null.');\n\n\t}\n\n\tvar matchers = {\n\n\t  '$elemMatch': function (doc, userValue, parsedField, docFieldValue) {\n\t    if (!Array.isArray(docFieldValue)) {\n\t      return false;\n\t    }\n\n\t    if (docFieldValue.length === 0) {\n\t      return false;\n\t    }\n\n\t    if (typeof docFieldValue[0] === 'object') {\n\t      return docFieldValue.some(function (val) {\n\t        return rowFilter(val, userValue, Object.keys(userValue));\n\t      });\n\t    }\n\n\t    return docFieldValue.some(function (val) {\n\t      return matchSelector(userValue, doc, parsedField, val);\n\t    });\n\t  },\n\n\t  '$allMatch': function (doc, userValue, parsedField, docFieldValue) {\n\t    if (!Array.isArray(docFieldValue)) {\n\t      return false;\n\t    }\n\n\t    /* istanbul ignore next */\n\t    if (docFieldValue.length === 0) {\n\t      return false;\n\t    }\n\n\t    if (typeof docFieldValue[0] === 'object') {\n\t      return docFieldValue.every(function (val) {\n\t        return rowFilter(val, userValue, Object.keys(userValue));\n\t      });\n\t    }\n\n\t    return docFieldValue.every(function (val) {\n\t      return matchSelector(userValue, doc, parsedField, val);\n\t    });\n\t  },\n\n\t  '$eq': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldIsNotUndefined(docFieldValue) && collate(docFieldValue, userValue) === 0;\n\t  },\n\n\t  '$gte': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldIsNotUndefined(docFieldValue) && collate(docFieldValue, userValue) >= 0;\n\t  },\n\n\t  '$gt': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldIsNotUndefined(docFieldValue) && collate(docFieldValue, userValue) > 0;\n\t  },\n\n\t  '$lte': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldIsNotUndefined(docFieldValue) && collate(docFieldValue, userValue) <= 0;\n\t  },\n\n\t  '$lt': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldIsNotUndefined(docFieldValue) && collate(docFieldValue, userValue) < 0;\n\t  },\n\n\t  '$exists': function (doc, userValue, parsedField, docFieldValue) {\n\t    //a field that is null is still considered to exist\n\t    if (userValue) {\n\t      return fieldIsNotUndefined(docFieldValue);\n\t    }\n\n\t    return !fieldIsNotUndefined(docFieldValue);\n\t  },\n\n\t  '$mod': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldExists(docFieldValue) && modField(docFieldValue, userValue);\n\t  },\n\n\t  '$ne': function (doc, userValue, parsedField, docFieldValue) {\n\t    return userValue.every(function (neValue) {\n\t      return collate(docFieldValue, neValue) !== 0;\n\t    });\n\t  },\n\t  '$in': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldExists(docFieldValue) && arrayContainsValue(docFieldValue, userValue);\n\t  },\n\n\t  '$nin': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldExists(docFieldValue) && !arrayContainsValue(docFieldValue, userValue);\n\t  },\n\n\t  '$size': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldExists(docFieldValue) && arraySize(docFieldValue, userValue);\n\t  },\n\n\t  '$all': function (doc, userValue, parsedField, docFieldValue) {\n\t    return Array.isArray(docFieldValue) && arrayContainsAllValues(docFieldValue, userValue);\n\t  },\n\n\t  '$regex': function (doc, userValue, parsedField, docFieldValue) {\n\t    return fieldExists(docFieldValue) && regexMatch(docFieldValue, userValue);\n\t  },\n\n\t  '$type': function (doc, userValue, parsedField, docFieldValue) {\n\t    return typeMatch(docFieldValue, userValue);\n\t  }\n\t};\n\n\t// return true if the given doc matches the supplied selector\n\tfunction matchesSelector(doc, selector) {\n\t  /* istanbul ignore if */\n\t  if (typeof selector !== 'object') {\n\t    // match the CouchDB error message\n\t    throw new Error('Selector error: expected a JSON object');\n\t  }\n\n\t  selector = massageSelector(selector);\n\t  var row = {\n\t    'doc': doc\n\t  };\n\n\t  var rowsMatched = filterInMemoryFields([row], { 'selector': selector }, Object.keys(selector));\n\t  return rowsMatched && rowsMatched.length === 1;\n\t}\n\n\tfunction evalFilter(input) {\n\t  return scopeEval('\"use strict\";\\nreturn ' + input + ';', {});\n\t}\n\n\tfunction evalView(input) {\n\t  var code = [\n\t    'return function(doc) {',\n\t    '  \"use strict\";',\n\t    '  var emitted = false;',\n\t    '  var emit = function (a, b) {',\n\t    '    emitted = true;',\n\t    '  };',\n\t    '  var view = ' + input + ';',\n\t    '  view(doc);',\n\t    '  if (emitted) {',\n\t    '    return true;',\n\t    '  }',\n\t    '};'\n\t  ].join('\\n');\n\n\t  return scopeEval(code, {});\n\t}\n\n\tfunction validate(opts, callback) {\n\t  if (opts.selector) {\n\t    if (opts.filter && opts.filter !== '_selector') {\n\t      var filterName = typeof opts.filter === 'string' ?\n\t        opts.filter : 'function';\n\t      return callback(new Error('selector invalid for filter \"' + filterName + '\"'));\n\t    }\n\t  }\n\t  callback();\n\t}\n\n\tfunction normalize(opts) {\n\t  if (opts.view && !opts.filter) {\n\t    opts.filter = '_view';\n\t  }\n\n\t  if (opts.selector && !opts.filter) {\n\t    opts.filter = '_selector';\n\t  }\n\n\t  if (opts.filter && typeof opts.filter === 'string') {\n\t    if (opts.filter === '_view') {\n\t      opts.view = normalizeDesignDocFunctionName(opts.view);\n\t    } else {\n\t      opts.filter = normalizeDesignDocFunctionName(opts.filter);\n\t    }\n\t  }\n\t}\n\n\tfunction shouldFilter(changesHandler, opts) {\n\t  return opts.filter && typeof opts.filter === 'string' &&\n\t    !opts.doc_ids && !isRemote(changesHandler.db);\n\t}\n\n\tfunction filter(changesHandler, opts) {\n\t  var callback = opts.complete;\n\t  if (opts.filter === '_view') {\n\t    if (!opts.view || typeof opts.view !== 'string') {\n\t      var err = createError(BAD_REQUEST,\n\t        '`view` filter parameter not found or invalid.');\n\t      return callback(err);\n\t    }\n\t    // fetch a view from a design doc, make it behave like a filter\n\t    var viewName = parseDesignDocFunctionName(opts.view);\n\t    changesHandler.db.get('_design/' + viewName[0], function (err, ddoc) {\n\t      /* istanbul ignore if */\n\t      if (changesHandler.isCancelled) {\n\t        return callback(null, {status: 'cancelled'});\n\t      }\n\t      /* istanbul ignore next */\n\t      if (err) {\n\t        return callback(generateErrorFromResponse(err));\n\t      }\n\t      var mapFun = ddoc && ddoc.views && ddoc.views[viewName[1]] &&\n\t        ddoc.views[viewName[1]].map;\n\t      if (!mapFun) {\n\t        return callback(createError(MISSING_DOC,\n\t          (ddoc.views ? 'missing json key: ' + viewName[1] :\n\t            'missing json key: views')));\n\t      }\n\t      opts.filter = evalView(mapFun);\n\t      changesHandler.doChanges(opts);\n\t    });\n\t  } else if (opts.selector) {\n\t    opts.filter = function (doc) {\n\t      return matchesSelector(doc, opts.selector);\n\t    };\n\t    changesHandler.doChanges(opts);\n\t  } else {\n\t    // fetch a filter from a design doc\n\t    var filterName = parseDesignDocFunctionName(opts.filter);\n\t    changesHandler.db.get('_design/' + filterName[0], function (err, ddoc) {\n\t      /* istanbul ignore if */\n\t      if (changesHandler.isCancelled) {\n\t        return callback(null, {status: 'cancelled'});\n\t      }\n\t      /* istanbul ignore next */\n\t      if (err) {\n\t        return callback(generateErrorFromResponse(err));\n\t      }\n\t      var filterFun = ddoc && ddoc.filters && ddoc.filters[filterName[1]];\n\t      if (!filterFun) {\n\t        return callback(createError(MISSING_DOC,\n\t          ((ddoc && ddoc.filters) ? 'missing json key: ' + filterName[1]\n\t            : 'missing json key: filters')));\n\t      }\n\t      opts.filter = evalFilter(filterFun);\n\t      changesHandler.doChanges(opts);\n\t    });\n\t  }\n\t}\n\n\tfunction applyChangesFilterPlugin(PouchDB) {\n\t  PouchDB._changesFilterPlugin = {\n\t    validate: validate,\n\t    normalize: normalize,\n\t    shouldFilter: shouldFilter,\n\t    filter: filter\n\t  };\n\t}\n\n\t// TODO: remove from pouchdb-core (breaking)\n\tPouchDB.plugin(applyChangesFilterPlugin);\n\n\tPouchDB.version = version;\n\n\tfunction toObject(array) {\n\t  return array.reduce(function (obj, item) {\n\t    obj[item] = true;\n\t    return obj;\n\t  }, {});\n\t}\n\t// List of top level reserved words for doc\n\tvar reservedWords = toObject([\n\t  '_id',\n\t  '_rev',\n\t  '_attachments',\n\t  '_deleted',\n\t  '_revisions',\n\t  '_revs_info',\n\t  '_conflicts',\n\t  '_deleted_conflicts',\n\t  '_local_seq',\n\t  '_rev_tree',\n\t  //replication documents\n\t  '_replication_id',\n\t  '_replication_state',\n\t  '_replication_state_time',\n\t  '_replication_state_reason',\n\t  '_replication_stats',\n\t  // Specific to Couchbase Sync Gateway\n\t  '_removed'\n\t]);\n\n\t// List of reserved words that should end up the document\n\tvar dataWords = toObject([\n\t  '_attachments',\n\t  //replication documents\n\t  '_replication_id',\n\t  '_replication_state',\n\t  '_replication_state_time',\n\t  '_replication_state_reason',\n\t  '_replication_stats'\n\t]);\n\n\tfunction parseRevisionInfo(rev$$1) {\n\t  if (!/^\\d+-/.test(rev$$1)) {\n\t    return createError(INVALID_REV);\n\t  }\n\t  var idx = rev$$1.indexOf('-');\n\t  var left = rev$$1.substring(0, idx);\n\t  var right = rev$$1.substring(idx + 1);\n\t  return {\n\t    prefix: parseInt(left, 10),\n\t    id: right\n\t  };\n\t}\n\n\tfunction makeRevTreeFromRevisions(revisions, opts) {\n\t  var pos = revisions.start - revisions.ids.length + 1;\n\n\t  var revisionIds = revisions.ids;\n\t  var ids = [revisionIds[0], opts, []];\n\n\t  for (var i = 1, len = revisionIds.length; i < len; i++) {\n\t    ids = [revisionIds[i], {status: 'missing'}, [ids]];\n\t  }\n\n\t  return [{\n\t    pos: pos,\n\t    ids: ids\n\t  }];\n\t}\n\n\t// Preprocess documents, parse their revisions, assign an id and a\n\t// revision for new writes that are missing them, etc\n\tfunction parseDoc(doc, newEdits, dbOpts) {\n\t  if (!dbOpts) {\n\t    dbOpts = {\n\t      deterministic_revs: true\n\t    };\n\t  }\n\n\t  var nRevNum;\n\t  var newRevId;\n\t  var revInfo;\n\t  var opts = {status: 'available'};\n\t  if (doc._deleted) {\n\t    opts.deleted = true;\n\t  }\n\n\t  if (newEdits) {\n\t    if (!doc._id) {\n\t      doc._id = uuid$1();\n\t    }\n\t    newRevId = rev(doc, dbOpts.deterministic_revs);\n\t    if (doc._rev) {\n\t      revInfo = parseRevisionInfo(doc._rev);\n\t      if (revInfo.error) {\n\t        return revInfo;\n\t      }\n\t      doc._rev_tree = [{\n\t        pos: revInfo.prefix,\n\t        ids: [revInfo.id, {status: 'missing'}, [[newRevId, opts, []]]]\n\t      }];\n\t      nRevNum = revInfo.prefix + 1;\n\t    } else {\n\t      doc._rev_tree = [{\n\t        pos: 1,\n\t        ids : [newRevId, opts, []]\n\t      }];\n\t      nRevNum = 1;\n\t    }\n\t  } else {\n\t    if (doc._revisions) {\n\t      doc._rev_tree = makeRevTreeFromRevisions(doc._revisions, opts);\n\t      nRevNum = doc._revisions.start;\n\t      newRevId = doc._revisions.ids[0];\n\t    }\n\t    if (!doc._rev_tree) {\n\t      revInfo = parseRevisionInfo(doc._rev);\n\t      if (revInfo.error) {\n\t        return revInfo;\n\t      }\n\t      nRevNum = revInfo.prefix;\n\t      newRevId = revInfo.id;\n\t      doc._rev_tree = [{\n\t        pos: nRevNum,\n\t        ids: [newRevId, opts, []]\n\t      }];\n\t    }\n\t  }\n\n\t  invalidIdError(doc._id);\n\n\t  doc._rev = nRevNum + '-' + newRevId;\n\n\t  var result = {metadata : {}, data : {}};\n\t  for (var key in doc) {\n\t    /* istanbul ignore else */\n\t    if (Object.prototype.hasOwnProperty.call(doc, key)) {\n\t      var specialKey = key[0] === '_';\n\t      if (specialKey && !reservedWords[key]) {\n\t        var error = createError(DOC_VALIDATION, key);\n\t        error.message = DOC_VALIDATION.message + ': ' + key;\n\t        throw error;\n\t      } else if (specialKey && !dataWords[key]) {\n\t        result.metadata[key.slice(1)] = doc[key];\n\t      } else {\n\t        result.data[key] = doc[key];\n\t      }\n\t    }\n\t  }\n\t  return result;\n\t}\n\n\tfunction parseBase64(data) {\n\t  try {\n\t    return thisAtob(data);\n\t  } catch (e) {\n\t    var err = createError(BAD_ARG,\n\t      'Attachment is not a valid base64 string');\n\t    return {error: err};\n\t  }\n\t}\n\n\tfunction preprocessString(att, blobType, callback) {\n\t  var asBinary = parseBase64(att.data);\n\t  if (asBinary.error) {\n\t    return callback(asBinary.error);\n\t  }\n\n\t  att.length = asBinary.length;\n\t  if (blobType === 'blob') {\n\t    att.data = binStringToBluffer(asBinary, att.content_type);\n\t  } else if (blobType === 'base64') {\n\t    att.data = thisBtoa(asBinary);\n\t  } else { // binary\n\t    att.data = asBinary;\n\t  }\n\t  binaryMd5(asBinary, function (result) {\n\t    att.digest = 'md5-' + result;\n\t    callback();\n\t  });\n\t}\n\n\tfunction preprocessBlob(att, blobType, callback) {\n\t  binaryMd5(att.data, function (md5) {\n\t    att.digest = 'md5-' + md5;\n\t    // size is for blobs (browser), length is for buffers (node)\n\t    att.length = att.data.size || att.data.length || 0;\n\t    if (blobType === 'binary') {\n\t      blobToBinaryString(att.data, function (binString) {\n\t        att.data = binString;\n\t        callback();\n\t      });\n\t    } else if (blobType === 'base64') {\n\t      blobToBase64(att.data, function (b64) {\n\t        att.data = b64;\n\t        callback();\n\t      });\n\t    } else {\n\t      callback();\n\t    }\n\t  });\n\t}\n\n\tfunction preprocessAttachment(att, blobType, callback) {\n\t  if (att.stub) {\n\t    return callback();\n\t  }\n\t  if (typeof att.data === 'string') { // input is a base64 string\n\t    preprocessString(att, blobType, callback);\n\t  } else { // input is a blob\n\t    preprocessBlob(att, blobType, callback);\n\t  }\n\t}\n\n\tfunction preprocessAttachments(docInfos, blobType, callback) {\n\n\t  if (!docInfos.length) {\n\t    return callback();\n\t  }\n\n\t  var docv = 0;\n\t  var overallErr;\n\n\t  docInfos.forEach(function (docInfo) {\n\t    var attachments = docInfo.data && docInfo.data._attachments ?\n\t      Object.keys(docInfo.data._attachments) : [];\n\t    var recv = 0;\n\n\t    if (!attachments.length) {\n\t      return done();\n\t    }\n\n\t    function processedAttachment(err) {\n\t      overallErr = err;\n\t      recv++;\n\t      if (recv === attachments.length) {\n\t        done();\n\t      }\n\t    }\n\n\t    for (var key in docInfo.data._attachments) {\n\t      if (docInfo.data._attachments.hasOwnProperty(key)) {\n\t        preprocessAttachment(docInfo.data._attachments[key],\n\t          blobType, processedAttachment);\n\t      }\n\t    }\n\t  });\n\n\t  function done() {\n\t    docv++;\n\t    if (docInfos.length === docv) {\n\t      if (overallErr) {\n\t        callback(overallErr);\n\t      } else {\n\t        callback();\n\t      }\n\t    }\n\t  }\n\t}\n\n\tfunction updateDoc(revLimit, prev, docInfo, results,\n\t                   i, cb, writeDoc, newEdits) {\n\n\t  if (revExists(prev.rev_tree, docInfo.metadata.rev) && !newEdits) {\n\t    results[i] = docInfo;\n\t    return cb();\n\t  }\n\n\t  // sometimes this is pre-calculated. historically not always\n\t  var previousWinningRev = prev.winningRev || winningRev(prev);\n\t  var previouslyDeleted = 'deleted' in prev ? prev.deleted :\n\t    isDeleted(prev, previousWinningRev);\n\t  var deleted = 'deleted' in docInfo.metadata ? docInfo.metadata.deleted :\n\t    isDeleted(docInfo.metadata);\n\t  var isRoot = /^1-/.test(docInfo.metadata.rev);\n\n\t  if (previouslyDeleted && !deleted && newEdits && isRoot) {\n\t    var newDoc = docInfo.data;\n\t    newDoc._rev = previousWinningRev;\n\t    newDoc._id = docInfo.metadata.id;\n\t    docInfo = parseDoc(newDoc, newEdits);\n\t  }\n\n\t  var merged = merge(prev.rev_tree, docInfo.metadata.rev_tree[0], revLimit);\n\n\t  var inConflict = newEdits && ((\n\t    (previouslyDeleted && deleted && merged.conflicts !== 'new_leaf') ||\n\t    (!previouslyDeleted && merged.conflicts !== 'new_leaf') ||\n\t    (previouslyDeleted && !deleted && merged.conflicts === 'new_branch')));\n\n\t  if (inConflict) {\n\t    var err = createError(REV_CONFLICT);\n\t    results[i] = err;\n\t    return cb();\n\t  }\n\n\t  var newRev = docInfo.metadata.rev;\n\t  docInfo.metadata.rev_tree = merged.tree;\n\t  docInfo.stemmedRevs = merged.stemmedRevs || [];\n\t  /* istanbul ignore else */\n\t  if (prev.rev_map) {\n\t    docInfo.metadata.rev_map = prev.rev_map; // used only by leveldb\n\t  }\n\n\t  // recalculate\n\t  var winningRev$$1 = winningRev(docInfo.metadata);\n\t  var winningRevIsDeleted = isDeleted(docInfo.metadata, winningRev$$1);\n\n\t  // calculate the total number of documents that were added/removed,\n\t  // from the perspective of total_rows/doc_count\n\t  var delta = (previouslyDeleted === winningRevIsDeleted) ? 0 :\n\t    previouslyDeleted < winningRevIsDeleted ? -1 : 1;\n\n\t  var newRevIsDeleted;\n\t  if (newRev === winningRev$$1) {\n\t    // if the new rev is the same as the winning rev, we can reuse that value\n\t    newRevIsDeleted = winningRevIsDeleted;\n\t  } else {\n\t    // if they're not the same, then we need to recalculate\n\t    newRevIsDeleted = isDeleted(docInfo.metadata, newRev);\n\t  }\n\n\t  writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted,\n\t    true, delta, i, cb);\n\t}\n\n\tfunction rootIsMissing(docInfo) {\n\t  return docInfo.metadata.rev_tree[0].ids[1].status === 'missing';\n\t}\n\n\tfunction processDocs(revLimit, docInfos, api, fetchedDocs, tx, results,\n\t                     writeDoc, opts, overallCallback) {\n\n\t  // Default to 1000 locally\n\t  revLimit = revLimit || 1000;\n\n\t  function insertDoc(docInfo, resultsIdx, callback) {\n\t    // Cant insert new deleted documents\n\t    var winningRev$$1 = winningRev(docInfo.metadata);\n\t    var deleted = isDeleted(docInfo.metadata, winningRev$$1);\n\t    if ('was_delete' in opts && deleted) {\n\t      results[resultsIdx] = createError(MISSING_DOC, 'deleted');\n\t      return callback();\n\t    }\n\n\t    // 4712 - detect whether a new document was inserted with a _rev\n\t    var inConflict = newEdits && rootIsMissing(docInfo);\n\n\t    if (inConflict) {\n\t      var err = createError(REV_CONFLICT);\n\t      results[resultsIdx] = err;\n\t      return callback();\n\t    }\n\n\t    var delta = deleted ? 0 : 1;\n\n\t    writeDoc(docInfo, winningRev$$1, deleted, deleted, false,\n\t      delta, resultsIdx, callback);\n\t  }\n\n\t  var newEdits = opts.new_edits;\n\t  var idsToDocs = new ExportedMap();\n\n\t  var docsDone = 0;\n\t  var docsToDo = docInfos.length;\n\n\t  function checkAllDocsDone() {\n\t    if (++docsDone === docsToDo && overallCallback) {\n\t      overallCallback();\n\t    }\n\t  }\n\n\t  docInfos.forEach(function (currentDoc, resultsIdx) {\n\n\t    if (currentDoc._id && isLocalId(currentDoc._id)) {\n\t      var fun = currentDoc._deleted ? '_removeLocal' : '_putLocal';\n\t      api[fun](currentDoc, {ctx: tx}, function (err, res) {\n\t        results[resultsIdx] = err || res;\n\t        checkAllDocsDone();\n\t      });\n\t      return;\n\t    }\n\n\t    var id = currentDoc.metadata.id;\n\t    if (idsToDocs.has(id)) {\n\t      docsToDo--; // duplicate\n\t      idsToDocs.get(id).push([currentDoc, resultsIdx]);\n\t    } else {\n\t      idsToDocs.set(id, [[currentDoc, resultsIdx]]);\n\t    }\n\t  });\n\n\t  // in the case of new_edits, the user can provide multiple docs\n\t  // with the same id. these need to be processed sequentially\n\t  idsToDocs.forEach(function (docs, id) {\n\t    var numDone = 0;\n\n\t    function docWritten() {\n\t      if (++numDone < docs.length) {\n\t        nextDoc();\n\t      } else {\n\t        checkAllDocsDone();\n\t      }\n\t    }\n\t    function nextDoc() {\n\t      var value = docs[numDone];\n\t      var currentDoc = value[0];\n\t      var resultsIdx = value[1];\n\n\t      if (fetchedDocs.has(id)) {\n\t        updateDoc(revLimit, fetchedDocs.get(id), currentDoc, results,\n\t          resultsIdx, docWritten, writeDoc, newEdits);\n\t      } else {\n\t        // Ensure stemming applies to new writes as well\n\t        var merged = merge([], currentDoc.metadata.rev_tree[0], revLimit);\n\t        currentDoc.metadata.rev_tree = merged.tree;\n\t        currentDoc.stemmedRevs = merged.stemmedRevs || [];\n\t        insertDoc(currentDoc, resultsIdx, docWritten);\n\t      }\n\t    }\n\t    nextDoc();\n\t  });\n\t}\n\n\t// IndexedDB requires a versioned database structure, so we use the\n\t// version here to manage migrations.\n\tvar ADAPTER_VERSION = 5;\n\n\t// The object stores created for each database\n\t// DOC_STORE stores the document meta data, its revision history and state\n\t// Keyed by document id\n\tvar DOC_STORE = 'document-store';\n\t// BY_SEQ_STORE stores a particular version of a document, keyed by its\n\t// sequence id\n\tvar BY_SEQ_STORE = 'by-sequence';\n\t// Where we store attachments\n\tvar ATTACH_STORE = 'attach-store';\n\t// Where we store many-to-many relations\n\t// between attachment digests and seqs\n\tvar ATTACH_AND_SEQ_STORE = 'attach-seq-store';\n\n\t// Where we store database-wide meta data in a single record\n\t// keyed by id: META_STORE\n\tvar META_STORE = 'meta-store';\n\t// Where we store local documents\n\tvar LOCAL_STORE = 'local-store';\n\t// Where we detect blob support\n\tvar DETECT_BLOB_SUPPORT_STORE = 'detect-blob-support';\n\n\tfunction safeJsonParse(str) {\n\t  // This try/catch guards against stack overflow errors.\n\t  // JSON.parse() is faster than vuvuzela.parse() but vuvuzela\n\t  // cannot overflow.\n\t  try {\n\t    return JSON.parse(str);\n\t  } catch (e) {\n\t    /* istanbul ignore next */\n\t    return vuvuzela.parse(str);\n\t  }\n\t}\n\n\tfunction safeJsonStringify(json) {\n\t  try {\n\t    return JSON.stringify(json);\n\t  } catch (e) {\n\t    /* istanbul ignore next */\n\t    return vuvuzela.stringify(json);\n\t  }\n\t}\n\n\tfunction idbError(callback) {\n\t  return function (evt) {\n\t    var message = 'unknown_error';\n\t    if (evt.target && evt.target.error) {\n\t      message = evt.target.error.name || evt.target.error.message;\n\t    }\n\t    callback(createError(IDB_ERROR, message, evt.type));\n\t  };\n\t}\n\n\t// Unfortunately, the metadata has to be stringified\n\t// when it is put into the database, because otherwise\n\t// IndexedDB can throw errors for deeply-nested objects.\n\t// Originally we just used JSON.parse/JSON.stringify; now\n\t// we use this custom vuvuzela library that avoids recursion.\n\t// If we could do it all over again, we'd probably use a\n\t// format for the revision trees other than JSON.\n\tfunction encodeMetadata(metadata, winningRev, deleted) {\n\t  return {\n\t    data: safeJsonStringify(metadata),\n\t    winningRev: winningRev,\n\t    deletedOrLocal: deleted ? '1' : '0',\n\t    seq: metadata.seq, // highest seq for this doc\n\t    id: metadata.id\n\t  };\n\t}\n\n\tfunction decodeMetadata(storedObject) {\n\t  if (!storedObject) {\n\t    return null;\n\t  }\n\t  var metadata = safeJsonParse(storedObject.data);\n\t  metadata.winningRev = storedObject.winningRev;\n\t  metadata.deleted = storedObject.deletedOrLocal === '1';\n\t  metadata.seq = storedObject.seq;\n\t  return metadata;\n\t}\n\n\t// read the doc back out from the database. we don't store the\n\t// _id or _rev because we already have _doc_id_rev.\n\tfunction decodeDoc(doc) {\n\t  if (!doc) {\n\t    return doc;\n\t  }\n\t  var idx = doc._doc_id_rev.lastIndexOf(':');\n\t  doc._id = doc._doc_id_rev.substring(0, idx - 1);\n\t  doc._rev = doc._doc_id_rev.substring(idx + 1);\n\t  delete doc._doc_id_rev;\n\t  return doc;\n\t}\n\n\t// Read a blob from the database, encoding as necessary\n\t// and translating from base64 if the IDB doesn't support\n\t// native Blobs\n\tfunction readBlobData(body, type, asBlob, callback) {\n\t  if (asBlob) {\n\t    if (!body) {\n\t      callback(createBlob([''], {type: type}));\n\t    } else if (typeof body !== 'string') { // we have blob support\n\t      callback(body);\n\t    } else { // no blob support\n\t      callback(b64ToBluffer(body, type));\n\t    }\n\t  } else { // as base64 string\n\t    if (!body) {\n\t      callback('');\n\t    } else if (typeof body !== 'string') { // we have blob support\n\t      readAsBinaryString(body, function (binary) {\n\t        callback(thisBtoa(binary));\n\t      });\n\t    } else { // no blob support\n\t      callback(body);\n\t    }\n\t  }\n\t}\n\n\tfunction fetchAttachmentsIfNecessary(doc, opts, txn, cb) {\n\t  var attachments = Object.keys(doc._attachments || {});\n\t  if (!attachments.length) {\n\t    return cb && cb();\n\t  }\n\t  var numDone = 0;\n\n\t  function checkDone() {\n\t    if (++numDone === attachments.length && cb) {\n\t      cb();\n\t    }\n\t  }\n\n\t  function fetchAttachment(doc, att) {\n\t    var attObj = doc._attachments[att];\n\t    var digest = attObj.digest;\n\t    var req = txn.objectStore(ATTACH_STORE).get(digest);\n\t    req.onsuccess = function (e) {\n\t      attObj.body = e.target.result.body;\n\t      checkDone();\n\t    };\n\t  }\n\n\t  attachments.forEach(function (att) {\n\t    if (opts.attachments && opts.include_docs) {\n\t      fetchAttachment(doc, att);\n\t    } else {\n\t      doc._attachments[att].stub = true;\n\t      checkDone();\n\t    }\n\t  });\n\t}\n\n\t// IDB-specific postprocessing necessary because\n\t// we don't know whether we stored a true Blob or\n\t// a base64-encoded string, and if it's a Blob it\n\t// needs to be read outside of the transaction context\n\tfunction postProcessAttachments(results, asBlob) {\n\t  return Promise.all(results.map(function (row) {\n\t    if (row.doc && row.doc._attachments) {\n\t      var attNames = Object.keys(row.doc._attachments);\n\t      return Promise.all(attNames.map(function (att) {\n\t        var attObj = row.doc._attachments[att];\n\t        if (!('body' in attObj)) { // already processed\n\t          return;\n\t        }\n\t        var body = attObj.body;\n\t        var type = attObj.content_type;\n\t        return new Promise(function (resolve) {\n\t          readBlobData(body, type, asBlob, function (data) {\n\t            row.doc._attachments[att] = $inject_Object_assign(\n\t              pick(attObj, ['digest', 'content_type']),\n\t              {data: data}\n\t            );\n\t            resolve();\n\t          });\n\t        });\n\t      }));\n\t    }\n\t  }));\n\t}\n\n\tfunction compactRevs(revs, docId, txn) {\n\n\t  var possiblyOrphanedDigests = [];\n\t  var seqStore = txn.objectStore(BY_SEQ_STORE);\n\t  var attStore = txn.objectStore(ATTACH_STORE);\n\t  var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n\t  var count = revs.length;\n\n\t  function checkDone() {\n\t    count--;\n\t    if (!count) { // done processing all revs\n\t      deleteOrphanedAttachments();\n\t    }\n\t  }\n\n\t  function deleteOrphanedAttachments() {\n\t    if (!possiblyOrphanedDigests.length) {\n\t      return;\n\t    }\n\t    possiblyOrphanedDigests.forEach(function (digest) {\n\t      var countReq = attAndSeqStore.index('digestSeq').count(\n\t        IDBKeyRange.bound(\n\t          digest + '::', digest + '::\\uffff', false, false));\n\t      countReq.onsuccess = function (e) {\n\t        var count = e.target.result;\n\t        if (!count) {\n\t          // orphaned\n\t          attStore.delete(digest);\n\t        }\n\t      };\n\t    });\n\t  }\n\n\t  revs.forEach(function (rev$$1) {\n\t    var index = seqStore.index('_doc_id_rev');\n\t    var key = docId + \"::\" + rev$$1;\n\t    index.getKey(key).onsuccess = function (e) {\n\t      var seq = e.target.result;\n\t      if (typeof seq !== 'number') {\n\t        return checkDone();\n\t      }\n\t      seqStore.delete(seq);\n\n\t      var cursor = attAndSeqStore.index('seq')\n\t        .openCursor(IDBKeyRange.only(seq));\n\n\t      cursor.onsuccess = function (event) {\n\t        var cursor = event.target.result;\n\t        if (cursor) {\n\t          var digest = cursor.value.digestSeq.split('::')[0];\n\t          possiblyOrphanedDigests.push(digest);\n\t          attAndSeqStore.delete(cursor.primaryKey);\n\t          cursor.continue();\n\t        } else { // done\n\t          checkDone();\n\t        }\n\t      };\n\t    };\n\t  });\n\t}\n\n\tfunction openTransactionSafely(idb, stores, mode) {\n\t  try {\n\t    return {\n\t      txn: idb.transaction(stores, mode)\n\t    };\n\t  } catch (err) {\n\t    return {\n\t      error: err\n\t    };\n\t  }\n\t}\n\n\tvar changesHandler = new Changes();\n\n\tfunction idbBulkDocs(dbOpts, req, opts, api, idb, callback) {\n\t  var docInfos = req.docs;\n\t  var txn;\n\t  var docStore;\n\t  var bySeqStore;\n\t  var attachStore;\n\t  var attachAndSeqStore;\n\t  var metaStore;\n\t  var docInfoError;\n\t  var metaDoc;\n\n\t  for (var i = 0, len = docInfos.length; i < len; i++) {\n\t    var doc = docInfos[i];\n\t    if (doc._id && isLocalId(doc._id)) {\n\t      continue;\n\t    }\n\t    doc = docInfos[i] = parseDoc(doc, opts.new_edits, dbOpts);\n\t    if (doc.error && !docInfoError) {\n\t      docInfoError = doc;\n\t    }\n\t  }\n\n\t  if (docInfoError) {\n\t    return callback(docInfoError);\n\t  }\n\n\t  var allDocsProcessed = false;\n\t  var docCountDelta = 0;\n\t  var results = new Array(docInfos.length);\n\t  var fetchedDocs = new ExportedMap();\n\t  var preconditionErrored = false;\n\t  var blobType = api._meta.blobSupport ? 'blob' : 'base64';\n\n\t  preprocessAttachments(docInfos, blobType, function (err) {\n\t    if (err) {\n\t      return callback(err);\n\t    }\n\t    startTransaction();\n\t  });\n\n\t  function startTransaction() {\n\n\t    var stores = [\n\t      DOC_STORE, BY_SEQ_STORE,\n\t      ATTACH_STORE,\n\t      LOCAL_STORE, ATTACH_AND_SEQ_STORE,\n\t      META_STORE\n\t    ];\n\t    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n\t    if (txnResult.error) {\n\t      return callback(txnResult.error);\n\t    }\n\t    txn = txnResult.txn;\n\t    txn.onabort = idbError(callback);\n\t    txn.ontimeout = idbError(callback);\n\t    txn.oncomplete = complete;\n\t    docStore = txn.objectStore(DOC_STORE);\n\t    bySeqStore = txn.objectStore(BY_SEQ_STORE);\n\t    attachStore = txn.objectStore(ATTACH_STORE);\n\t    attachAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n\t    metaStore = txn.objectStore(META_STORE);\n\n\t    metaStore.get(META_STORE).onsuccess = function (e) {\n\t      metaDoc = e.target.result;\n\t      updateDocCountIfReady();\n\t    };\n\n\t    verifyAttachments(function (err) {\n\t      if (err) {\n\t        preconditionErrored = true;\n\t        return callback(err);\n\t      }\n\t      fetchExistingDocs();\n\t    });\n\t  }\n\n\t  function onAllDocsProcessed() {\n\t    allDocsProcessed = true;\n\t    updateDocCountIfReady();\n\t  }\n\n\t  function idbProcessDocs() {\n\t    processDocs(dbOpts.revs_limit, docInfos, api, fetchedDocs,\n\t                txn, results, writeDoc, opts, onAllDocsProcessed);\n\t  }\n\n\t  function updateDocCountIfReady() {\n\t    if (!metaDoc || !allDocsProcessed) {\n\t      return;\n\t    }\n\t    // caching the docCount saves a lot of time in allDocs() and\n\t    // info(), which is why we go to all the trouble of doing this\n\t    metaDoc.docCount += docCountDelta;\n\t    metaStore.put(metaDoc);\n\t  }\n\n\t  function fetchExistingDocs() {\n\n\t    if (!docInfos.length) {\n\t      return;\n\t    }\n\n\t    var numFetched = 0;\n\n\t    function checkDone() {\n\t      if (++numFetched === docInfos.length) {\n\t        idbProcessDocs();\n\t      }\n\t    }\n\n\t    function readMetadata(event) {\n\t      var metadata = decodeMetadata(event.target.result);\n\n\t      if (metadata) {\n\t        fetchedDocs.set(metadata.id, metadata);\n\t      }\n\t      checkDone();\n\t    }\n\n\t    for (var i = 0, len = docInfos.length; i < len; i++) {\n\t      var docInfo = docInfos[i];\n\t      if (docInfo._id && isLocalId(docInfo._id)) {\n\t        checkDone(); // skip local docs\n\t        continue;\n\t      }\n\t      var req = docStore.get(docInfo.metadata.id);\n\t      req.onsuccess = readMetadata;\n\t    }\n\t  }\n\n\t  function complete() {\n\t    if (preconditionErrored) {\n\t      return;\n\t    }\n\n\t    changesHandler.notify(api._meta.name);\n\t    callback(null, results);\n\t  }\n\n\t  function verifyAttachment(digest, callback) {\n\n\t    var req = attachStore.get(digest);\n\t    req.onsuccess = function (e) {\n\t      if (!e.target.result) {\n\t        var err = createError(MISSING_STUB,\n\t          'unknown stub attachment with digest ' +\n\t          digest);\n\t        err.status = 412;\n\t        callback(err);\n\t      } else {\n\t        callback();\n\t      }\n\t    };\n\t  }\n\n\t  function verifyAttachments(finish) {\n\n\n\t    var digests = [];\n\t    docInfos.forEach(function (docInfo) {\n\t      if (docInfo.data && docInfo.data._attachments) {\n\t        Object.keys(docInfo.data._attachments).forEach(function (filename) {\n\t          var att = docInfo.data._attachments[filename];\n\t          if (att.stub) {\n\t            digests.push(att.digest);\n\t          }\n\t        });\n\t      }\n\t    });\n\t    if (!digests.length) {\n\t      return finish();\n\t    }\n\t    var numDone = 0;\n\t    var err;\n\n\t    function checkDone() {\n\t      if (++numDone === digests.length) {\n\t        finish(err);\n\t      }\n\t    }\n\t    digests.forEach(function (digest) {\n\t      verifyAttachment(digest, function (attErr) {\n\t        if (attErr && !err) {\n\t          err = attErr;\n\t        }\n\t        checkDone();\n\t      });\n\t    });\n\t  }\n\n\t  function writeDoc(docInfo, winningRev$$1, winningRevIsDeleted, newRevIsDeleted,\n\t                    isUpdate, delta, resultsIdx, callback) {\n\n\t    docInfo.metadata.winningRev = winningRev$$1;\n\t    docInfo.metadata.deleted = winningRevIsDeleted;\n\n\t    var doc = docInfo.data;\n\t    doc._id = docInfo.metadata.id;\n\t    doc._rev = docInfo.metadata.rev;\n\n\t    if (newRevIsDeleted) {\n\t      doc._deleted = true;\n\t    }\n\n\t    var hasAttachments = doc._attachments &&\n\t      Object.keys(doc._attachments).length;\n\t    if (hasAttachments) {\n\t      return writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted,\n\t        isUpdate, resultsIdx, callback);\n\t    }\n\n\t    docCountDelta += delta;\n\t    updateDocCountIfReady();\n\n\t    finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n\t      isUpdate, resultsIdx, callback);\n\t  }\n\n\t  function finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n\t                     isUpdate, resultsIdx, callback) {\n\n\t    var doc = docInfo.data;\n\t    var metadata = docInfo.metadata;\n\n\t    doc._doc_id_rev = metadata.id + '::' + metadata.rev;\n\t    delete doc._id;\n\t    delete doc._rev;\n\n\t    function afterPutDoc(e) {\n\t      var revsToDelete = docInfo.stemmedRevs || [];\n\n\t      if (isUpdate && api.auto_compaction) {\n\t        revsToDelete = revsToDelete.concat(compactTree(docInfo.metadata));\n\t      }\n\n\t      if (revsToDelete && revsToDelete.length) {\n\t        compactRevs(revsToDelete, docInfo.metadata.id, txn);\n\t      }\n\n\t      metadata.seq = e.target.result;\n\t      // Current _rev is calculated from _rev_tree on read\n\t      // delete metadata.rev;\n\t      var metadataToStore = encodeMetadata(metadata, winningRev$$1,\n\t        winningRevIsDeleted);\n\t      var metaDataReq = docStore.put(metadataToStore);\n\t      metaDataReq.onsuccess = afterPutMetadata;\n\t    }\n\n\t    function afterPutDocError(e) {\n\t      // ConstraintError, need to update, not put (see #1638 for details)\n\t      e.preventDefault(); // avoid transaction abort\n\t      e.stopPropagation(); // avoid transaction onerror\n\t      var index = bySeqStore.index('_doc_id_rev');\n\t      var getKeyReq = index.getKey(doc._doc_id_rev);\n\t      getKeyReq.onsuccess = function (e) {\n\t        var putReq = bySeqStore.put(doc, e.target.result);\n\t        putReq.onsuccess = afterPutDoc;\n\t      };\n\t    }\n\n\t    function afterPutMetadata() {\n\t      results[resultsIdx] = {\n\t        ok: true,\n\t        id: metadata.id,\n\t        rev: metadata.rev\n\t      };\n\t      fetchedDocs.set(docInfo.metadata.id, docInfo.metadata);\n\t      insertAttachmentMappings(docInfo, metadata.seq, callback);\n\t    }\n\n\t    var putReq = bySeqStore.put(doc);\n\n\t    putReq.onsuccess = afterPutDoc;\n\t    putReq.onerror = afterPutDocError;\n\t  }\n\n\t  function writeAttachments(docInfo, winningRev$$1, winningRevIsDeleted,\n\t                            isUpdate, resultsIdx, callback) {\n\n\n\t    var doc = docInfo.data;\n\n\t    var numDone = 0;\n\t    var attachments = Object.keys(doc._attachments);\n\n\t    function collectResults() {\n\t      if (numDone === attachments.length) {\n\t        finishDoc(docInfo, winningRev$$1, winningRevIsDeleted,\n\t          isUpdate, resultsIdx, callback);\n\t      }\n\t    }\n\n\t    function attachmentSaved() {\n\t      numDone++;\n\t      collectResults();\n\t    }\n\n\t    attachments.forEach(function (key) {\n\t      var att = docInfo.data._attachments[key];\n\t      if (!att.stub) {\n\t        var data = att.data;\n\t        delete att.data;\n\t        att.revpos = parseInt(winningRev$$1, 10);\n\t        var digest = att.digest;\n\t        saveAttachment(digest, data, attachmentSaved);\n\t      } else {\n\t        numDone++;\n\t        collectResults();\n\t      }\n\t    });\n\t  }\n\n\t  // map seqs to attachment digests, which\n\t  // we will need later during compaction\n\t  function insertAttachmentMappings(docInfo, seq, callback) {\n\n\t    var attsAdded = 0;\n\t    var attsToAdd = Object.keys(docInfo.data._attachments || {});\n\n\t    if (!attsToAdd.length) {\n\t      return callback();\n\t    }\n\n\t    function checkDone() {\n\t      if (++attsAdded === attsToAdd.length) {\n\t        callback();\n\t      }\n\t    }\n\n\t    function add(att) {\n\t      var digest = docInfo.data._attachments[att].digest;\n\t      var req = attachAndSeqStore.put({\n\t        seq: seq,\n\t        digestSeq: digest + '::' + seq\n\t      });\n\n\t      req.onsuccess = checkDone;\n\t      req.onerror = function (e) {\n\t        // this callback is for a constaint error, which we ignore\n\t        // because this docid/rev has already been associated with\n\t        // the digest (e.g. when new_edits == false)\n\t        e.preventDefault(); // avoid transaction abort\n\t        e.stopPropagation(); // avoid transaction onerror\n\t        checkDone();\n\t      };\n\t    }\n\t    for (var i = 0; i < attsToAdd.length; i++) {\n\t      add(attsToAdd[i]); // do in parallel\n\t    }\n\t  }\n\n\t  function saveAttachment(digest, data, callback) {\n\n\n\t    var getKeyReq = attachStore.count(digest);\n\t    getKeyReq.onsuccess = function (e) {\n\t      var count = e.target.result;\n\t      if (count) {\n\t        return callback(); // already exists\n\t      }\n\t      var newAtt = {\n\t        digest: digest,\n\t        body: data\n\t      };\n\t      var putReq = attachStore.put(newAtt);\n\t      putReq.onsuccess = callback;\n\t    };\n\t  }\n\t}\n\n\t// Abstraction over IDBCursor and getAll()/getAllKeys() that allows us to batch our operations\n\t// while falling back to a normal IDBCursor operation on browsers that don't support getAll() or\n\t// getAllKeys(). This allows for a much faster implementation than just straight-up cursors, because\n\t// we're not processing each document one-at-a-time.\n\tfunction runBatchedCursor(objectStore, keyRange, descending, batchSize, onBatch) {\n\n\t  if (batchSize === -1) {\n\t    batchSize = 1000;\n\t  }\n\n\t  // Bail out of getAll()/getAllKeys() in the following cases:\n\t  // 1) either method is unsupported - we need both\n\t  // 2) batchSize is 1 (might as well use IDBCursor)\n\t  // 3) descending – no real way to do this via getAll()/getAllKeys()\n\n\t  var useGetAll = typeof objectStore.getAll === 'function' &&\n\t    typeof objectStore.getAllKeys === 'function' &&\n\t    batchSize > 1 && !descending;\n\n\t  var keysBatch;\n\t  var valuesBatch;\n\t  var pseudoCursor;\n\n\t  function onGetAll(e) {\n\t    valuesBatch = e.target.result;\n\t    if (keysBatch) {\n\t      onBatch(keysBatch, valuesBatch, pseudoCursor);\n\t    }\n\t  }\n\n\t  function onGetAllKeys(e) {\n\t    keysBatch = e.target.result;\n\t    if (valuesBatch) {\n\t      onBatch(keysBatch, valuesBatch, pseudoCursor);\n\t    }\n\t  }\n\n\t  function continuePseudoCursor() {\n\t    if (!keysBatch.length) { // no more results\n\t      return onBatch();\n\t    }\n\t    // fetch next batch, exclusive start\n\t    var lastKey = keysBatch[keysBatch.length - 1];\n\t    var newKeyRange;\n\t    if (keyRange && keyRange.upper) {\n\t      try {\n\t        newKeyRange = IDBKeyRange.bound(lastKey, keyRange.upper,\n\t          true, keyRange.upperOpen);\n\t      } catch (e) {\n\t        if (e.name === \"DataError\" && e.code === 0) {\n\t          return onBatch(); // we're done, startkey and endkey are equal\n\t        }\n\t      }\n\t    } else {\n\t      newKeyRange = IDBKeyRange.lowerBound(lastKey, true);\n\t    }\n\t    keyRange = newKeyRange;\n\t    keysBatch = null;\n\t    valuesBatch = null;\n\t    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n\t    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n\t  }\n\n\t  function onCursor(e) {\n\t    var cursor = e.target.result;\n\t    if (!cursor) { // done\n\t      return onBatch();\n\t    }\n\t    // regular IDBCursor acts like a batch where batch size is always 1\n\t    onBatch([cursor.key], [cursor.value], cursor);\n\t  }\n\n\t  if (useGetAll) {\n\t    pseudoCursor = {\"continue\": continuePseudoCursor};\n\t    objectStore.getAll(keyRange, batchSize).onsuccess = onGetAll;\n\t    objectStore.getAllKeys(keyRange, batchSize).onsuccess = onGetAllKeys;\n\t  } else if (descending) {\n\t    objectStore.openCursor(keyRange, 'prev').onsuccess = onCursor;\n\t  } else {\n\t    objectStore.openCursor(keyRange).onsuccess = onCursor;\n\t  }\n\t}\n\n\t// simple shim for objectStore.getAll(), falling back to IDBCursor\n\tfunction getAll(objectStore, keyRange, onSuccess) {\n\t  if (typeof objectStore.getAll === 'function') {\n\t    // use native getAll\n\t    objectStore.getAll(keyRange).onsuccess = onSuccess;\n\t    return;\n\t  }\n\t  // fall back to cursors\n\t  var values = [];\n\n\t  function onCursor(e) {\n\t    var cursor = e.target.result;\n\t    if (cursor) {\n\t      values.push(cursor.value);\n\t      cursor.continue();\n\t    } else {\n\t      onSuccess({\n\t        target: {\n\t          result: values\n\t        }\n\t      });\n\t    }\n\t  }\n\n\t  objectStore.openCursor(keyRange).onsuccess = onCursor;\n\t}\n\n\tfunction allDocsKeys(keys, docStore, onBatch) {\n\t  // It's not guaranted to be returned in right order  \n\t  var valuesBatch = new Array(keys.length);\n\t  var count = 0;\n\t  keys.forEach(function (key, index) {\n\t    docStore.get(key).onsuccess = function (event) {\n\t      if (event.target.result) {\n\t        valuesBatch[index] = event.target.result;\n\t      } else {\n\t        valuesBatch[index] = {key: key, error: 'not_found'};\n\t      }\n\t      count++;\n\t      if (count === keys.length) {\n\t        onBatch(keys, valuesBatch, {});\n\t      }\n\t    };\n\t  });\n\t}\n\n\tfunction createKeyRange(start, end, inclusiveEnd, key, descending) {\n\t  try {\n\t    if (start && end) {\n\t      if (descending) {\n\t        return IDBKeyRange.bound(end, start, !inclusiveEnd, false);\n\t      } else {\n\t        return IDBKeyRange.bound(start, end, false, !inclusiveEnd);\n\t      }\n\t    } else if (start) {\n\t      if (descending) {\n\t        return IDBKeyRange.upperBound(start);\n\t      } else {\n\t        return IDBKeyRange.lowerBound(start);\n\t      }\n\t    } else if (end) {\n\t      if (descending) {\n\t        return IDBKeyRange.lowerBound(end, !inclusiveEnd);\n\t      } else {\n\t        return IDBKeyRange.upperBound(end, !inclusiveEnd);\n\t      }\n\t    } else if (key) {\n\t      return IDBKeyRange.only(key);\n\t    }\n\t  } catch (e) {\n\t    return {error: e};\n\t  }\n\t  return null;\n\t}\n\n\tfunction idbAllDocs(opts, idb, callback) {\n\t  var start = 'startkey' in opts ? opts.startkey : false;\n\t  var end = 'endkey' in opts ? opts.endkey : false;\n\t  var key = 'key' in opts ? opts.key : false;\n\t  var keys = 'keys' in opts ? opts.keys : false; \n\t  var skip = opts.skip || 0;\n\t  var limit = typeof opts.limit === 'number' ? opts.limit : -1;\n\t  var inclusiveEnd = opts.inclusive_end !== false;\n\n\t  var keyRange ; \n\t  var keyRangeError;\n\t  if (!keys) {\n\t    keyRange = createKeyRange(start, end, inclusiveEnd, key, opts.descending);\n\t    keyRangeError = keyRange && keyRange.error;\n\t    if (keyRangeError && \n\t      !(keyRangeError.name === \"DataError\" && keyRangeError.code === 0)) {\n\t      // DataError with error code 0 indicates start is less than end, so\n\t      // can just do an empty query. Else need to throw\n\t      return callback(createError(IDB_ERROR,\n\t        keyRangeError.name, keyRangeError.message));\n\t    }\n\t  }\n\n\t  var stores = [DOC_STORE, BY_SEQ_STORE, META_STORE];\n\n\t  if (opts.attachments) {\n\t    stores.push(ATTACH_STORE);\n\t  }\n\t  var txnResult = openTransactionSafely(idb, stores, 'readonly');\n\t  if (txnResult.error) {\n\t    return callback(txnResult.error);\n\t  }\n\t  var txn = txnResult.txn;\n\t  txn.oncomplete = onTxnComplete;\n\t  txn.onabort = idbError(callback);\n\t  var docStore = txn.objectStore(DOC_STORE);\n\t  var seqStore = txn.objectStore(BY_SEQ_STORE);\n\t  var metaStore = txn.objectStore(META_STORE);\n\t  var docIdRevIndex = seqStore.index('_doc_id_rev');\n\t  var results = [];\n\t  var docCount;\n\t  var updateSeq;\n\n\t  metaStore.get(META_STORE).onsuccess = function (e) {\n\t    docCount = e.target.result.docCount;\n\t  };\n\n\t  /* istanbul ignore if */\n\t  if (opts.update_seq) {\n\t    getMaxUpdateSeq(seqStore, function (e) { \n\t      if (e.target.result && e.target.result.length > 0) {\n\t        updateSeq = e.target.result[0];\n\t      }\n\t    });\n\t  }\n\n\t  function getMaxUpdateSeq(objectStore, onSuccess) {\n\t    function onCursor(e) {\n\t      var cursor = e.target.result;\n\t      var maxKey = undefined;\n\t      if (cursor && cursor.key) {\n\t        maxKey = cursor.key;\n\t      } \n\t      return onSuccess({\n\t        target: {\n\t          result: [maxKey]\n\t        }\n\t      });\n\t    }\n\t    objectStore.openCursor(null, 'prev').onsuccess = onCursor;\n\t  }\n\n\t  // if the user specifies include_docs=true, then we don't\n\t  // want to block the main cursor while we're fetching the doc\n\t  function fetchDocAsynchronously(metadata, row, winningRev$$1) {\n\t    var key = metadata.id + \"::\" + winningRev$$1;\n\t    docIdRevIndex.get(key).onsuccess =  function onGetDoc(e) {\n\t      row.doc = decodeDoc(e.target.result) || {};\n\t      if (opts.conflicts) {\n\t        var conflicts = collectConflicts(metadata);\n\t        if (conflicts.length) {\n\t          row.doc._conflicts = conflicts;\n\t        }\n\t      }\n\t      fetchAttachmentsIfNecessary(row.doc, opts, txn);\n\t    };\n\t  }\n\n\t  function allDocsInner(winningRev$$1, metadata) {\n\t    var row = {\n\t      id: metadata.id,\n\t      key: metadata.id,\n\t      value: {\n\t        rev: winningRev$$1\n\t      }\n\t    };\n\t    var deleted = metadata.deleted;\n\t    if (deleted) {\n\t      if (keys) {\n\t        results.push(row);\n\t        // deleted docs are okay with \"keys\" requests\n\t        row.value.deleted = true;\n\t        row.doc = null;\n\t      }\n\t    } else if (skip-- <= 0) {\n\t      results.push(row);\n\t      if (opts.include_docs) {\n\t        fetchDocAsynchronously(metadata, row, winningRev$$1);\n\t      }\n\t    }\n\t  }\n\n\t  function processBatch(batchValues) {\n\t    for (var i = 0, len = batchValues.length; i < len; i++) {\n\t      if (results.length === limit) {\n\t        break;\n\t      }\n\t      var batchValue = batchValues[i];\n\t      if (batchValue.error && keys) {\n\t        // key was not found with \"keys\" requests\n\t        results.push(batchValue);\n\t        continue;\n\t      }\n\t      var metadata = decodeMetadata(batchValue);\n\t      var winningRev$$1 = metadata.winningRev;\n\t      allDocsInner(winningRev$$1, metadata);\n\t    }\n\t  }\n\n\t  function onBatch(batchKeys, batchValues, cursor) {\n\t    if (!cursor) {\n\t      return;\n\t    }\n\t    processBatch(batchValues);\n\t    if (results.length < limit) {\n\t      cursor.continue();\n\t    }\n\t  }\n\n\t  function onGetAll(e) {\n\t    var values = e.target.result;\n\t    if (opts.descending) {\n\t      values = values.reverse();\n\t    }\n\t    processBatch(values);\n\t  }\n\n\t  function onResultsReady() {\n\t    var returnVal = {\n\t      total_rows: docCount,\n\t      offset: opts.skip,\n\t      rows: results\n\t    };\n\t    \n\t    /* istanbul ignore if */\n\t    if (opts.update_seq && updateSeq !== undefined) {\n\t      returnVal.update_seq = updateSeq;\n\t    }\n\t    callback(null, returnVal);\n\t  }\n\n\t  function onTxnComplete() {\n\t    if (opts.attachments) {\n\t      postProcessAttachments(results, opts.binary).then(onResultsReady);\n\t    } else {\n\t      onResultsReady();\n\t    }\n\t  }\n\n\t  // don't bother doing any requests if start > end or limit === 0\n\t  if (keyRangeError || limit === 0) {\n\t    return;\n\t  }\n\t  if (keys) {\n\t    return allDocsKeys(opts.keys, docStore, onBatch);\n\t  }\n\t  if (limit === -1) { // just fetch everything\n\t    return getAll(docStore, keyRange, onGetAll);\n\t  }\n\t  // else do a cursor\n\t  // choose a batch size based on the skip, since we'll need to skip that many\n\t  runBatchedCursor(docStore, keyRange, opts.descending, limit + skip, onBatch);\n\t}\n\n\t//\n\t// Blobs are not supported in all versions of IndexedDB, notably\n\t// Chrome <37 and Android <5. In those versions, storing a blob will throw.\n\t//\n\t// Various other blob bugs exist in Chrome v37-42 (inclusive).\n\t// Detecting them is expensive and confusing to users, and Chrome 37-42\n\t// is at very low usage worldwide, so we do a hacky userAgent check instead.\n\t//\n\t// content-type bug: https://code.google.com/p/chromium/issues/detail?id=408120\n\t// 404 bug: https://code.google.com/p/chromium/issues/detail?id=447916\n\t// FileReader bug: https://code.google.com/p/chromium/issues/detail?id=447836\n\t//\n\tfunction checkBlobSupport(txn) {\n\t  return new Promise(function (resolve) {\n\t    var blob$$1 = createBlob(['']);\n\t    var req = txn.objectStore(DETECT_BLOB_SUPPORT_STORE).put(blob$$1, 'key');\n\n\t    req.onsuccess = function () {\n\t      var matchedChrome = navigator.userAgent.match(/Chrome\\/(\\d+)/);\n\t      var matchedEdge = navigator.userAgent.match(/Edge\\//);\n\t      // MS Edge pretends to be Chrome 42:\n\t      // https://msdn.microsoft.com/en-us/library/hh869301%28v=vs.85%29.aspx\n\t      resolve(matchedEdge || !matchedChrome ||\n\t        parseInt(matchedChrome[1], 10) >= 43);\n\t    };\n\n\t    req.onerror = txn.onabort = function (e) {\n\t      // If the transaction aborts now its due to not being able to\n\t      // write to the database, likely due to the disk being full\n\t      e.preventDefault();\n\t      e.stopPropagation();\n\t      resolve(false);\n\t    };\n\t  }).catch(function () {\n\t    return false; // error, so assume unsupported\n\t  });\n\t}\n\n\tfunction countDocs(txn, cb) {\n\t  var index = txn.objectStore(DOC_STORE).index('deletedOrLocal');\n\t  index.count(IDBKeyRange.only('0')).onsuccess = function (e) {\n\t    cb(e.target.result);\n\t  };\n\t}\n\n\t// This task queue ensures that IDB open calls are done in their own tick\n\n\tvar running = false;\n\tvar queue$1 = [];\n\n\tfunction tryCode(fun, err, res, PouchDB) {\n\t  try {\n\t    fun(err, res);\n\t  } catch (err) {\n\t    // Shouldn't happen, but in some odd cases\n\t    // IndexedDB implementations might throw a sync\n\t    // error, in which case this will at least log it.\n\t    PouchDB.emit('error', err);\n\t  }\n\t}\n\n\tfunction applyNext() {\n\t  if (running || !queue$1.length) {\n\t    return;\n\t  }\n\t  running = true;\n\t  queue$1.shift()();\n\t}\n\n\tfunction enqueueTask(action, callback, PouchDB) {\n\t  queue$1.push(function runAction() {\n\t    action(function runCallback(err, res) {\n\t      tryCode(callback, err, res, PouchDB);\n\t      running = false;\n\t      lib(function runNext() {\n\t        applyNext();\n\t      });\n\t    });\n\t  });\n\t  applyNext();\n\t}\n\n\tfunction changes(opts, api, dbName, idb) {\n\t  opts = clone(opts);\n\n\t  if (opts.continuous) {\n\t    var id = dbName + ':' + uuid$1();\n\t    changesHandler.addListener(dbName, id, api, opts);\n\t    changesHandler.notify(dbName);\n\t    return {\n\t      cancel: function () {\n\t        changesHandler.removeListener(dbName, id);\n\t      }\n\t    };\n\t  }\n\n\t  var docIds = opts.doc_ids && new ExportedSet(opts.doc_ids);\n\n\t  opts.since = opts.since || 0;\n\t  var lastSeq = opts.since;\n\n\t  var limit = 'limit' in opts ? opts.limit : -1;\n\t  if (limit === 0) {\n\t    limit = 1; // per CouchDB _changes spec\n\t  }\n\n\t  var results = [];\n\t  var numResults = 0;\n\t  var filter = filterChange(opts);\n\t  var docIdsToMetadata = new ExportedMap();\n\n\t  var txn;\n\t  var bySeqStore;\n\t  var docStore;\n\t  var docIdRevIndex;\n\n\t  function onBatch(batchKeys, batchValues, cursor) {\n\t    if (!cursor || !batchKeys.length) { // done\n\t      return;\n\t    }\n\n\t    var winningDocs = new Array(batchKeys.length);\n\t    var metadatas = new Array(batchKeys.length);\n\n\t    function processMetadataAndWinningDoc(metadata, winningDoc) {\n\t      var change = opts.processChange(winningDoc, metadata, opts);\n\t      lastSeq = change.seq = metadata.seq;\n\n\t      var filtered = filter(change);\n\t      if (typeof filtered === 'object') { // anything but true/false indicates error\n\t        return Promise.reject(filtered);\n\t      }\n\n\t      if (!filtered) {\n\t        return Promise.resolve();\n\t      }\n\t      numResults++;\n\t      if (opts.return_docs) {\n\t        results.push(change);\n\t      }\n\t      // process the attachment immediately\n\t      // for the benefit of live listeners\n\t      if (opts.attachments && opts.include_docs) {\n\t        return new Promise(function (resolve) {\n\t          fetchAttachmentsIfNecessary(winningDoc, opts, txn, function () {\n\t            postProcessAttachments([change], opts.binary).then(function () {\n\t              resolve(change);\n\t            });\n\t          });\n\t        });\n\t      } else {\n\t        return Promise.resolve(change);\n\t      }\n\t    }\n\n\t    function onBatchDone() {\n\t      var promises = [];\n\t      for (var i = 0, len = winningDocs.length; i < len; i++) {\n\t        if (numResults === limit) {\n\t          break;\n\t        }\n\t        var winningDoc = winningDocs[i];\n\t        if (!winningDoc) {\n\t          continue;\n\t        }\n\t        var metadata = metadatas[i];\n\t        promises.push(processMetadataAndWinningDoc(metadata, winningDoc));\n\t      }\n\n\t      Promise.all(promises).then(function (changes) {\n\t        for (var i = 0, len = changes.length; i < len; i++) {\n\t          if (changes[i]) {\n\t            opts.onChange(changes[i]);\n\t          }\n\t        }\n\t      }).catch(opts.complete);\n\n\t      if (numResults !== limit) {\n\t        cursor.continue();\n\t      }\n\t    }\n\n\t    // Fetch all metadatas/winningdocs from this batch in parallel, then process\n\t    // them all only once all data has been collected. This is done in parallel\n\t    // because it's faster than doing it one-at-a-time.\n\t    var numDone = 0;\n\t    batchValues.forEach(function (value, i) {\n\t      var doc = decodeDoc(value);\n\t      var seq = batchKeys[i];\n\t      fetchWinningDocAndMetadata(doc, seq, function (metadata, winningDoc) {\n\t        metadatas[i] = metadata;\n\t        winningDocs[i] = winningDoc;\n\t        if (++numDone === batchKeys.length) {\n\t          onBatchDone();\n\t        }\n\t      });\n\t    });\n\t  }\n\n\t  function onGetMetadata(doc, seq, metadata, cb) {\n\t    if (metadata.seq !== seq) {\n\t      // some other seq is later\n\t      return cb();\n\t    }\n\n\t    if (metadata.winningRev === doc._rev) {\n\t      // this is the winning doc\n\t      return cb(metadata, doc);\n\t    }\n\n\t    // fetch winning doc in separate request\n\t    var docIdRev = doc._id + '::' + metadata.winningRev;\n\t    var req = docIdRevIndex.get(docIdRev);\n\t    req.onsuccess = function (e) {\n\t      cb(metadata, decodeDoc(e.target.result));\n\t    };\n\t  }\n\n\t  function fetchWinningDocAndMetadata(doc, seq, cb) {\n\t    if (docIds && !docIds.has(doc._id)) {\n\t      return cb();\n\t    }\n\n\t    var metadata = docIdsToMetadata.get(doc._id);\n\t    if (metadata) { // cached\n\t      return onGetMetadata(doc, seq, metadata, cb);\n\t    }\n\t    // metadata not cached, have to go fetch it\n\t    docStore.get(doc._id).onsuccess = function (e) {\n\t      metadata = decodeMetadata(e.target.result);\n\t      docIdsToMetadata.set(doc._id, metadata);\n\t      onGetMetadata(doc, seq, metadata, cb);\n\t    };\n\t  }\n\n\t  function finish() {\n\t    opts.complete(null, {\n\t      results: results,\n\t      last_seq: lastSeq\n\t    });\n\t  }\n\n\t  function onTxnComplete() {\n\t    if (!opts.continuous && opts.attachments) {\n\t      // cannot guarantee that postProcessing was already done,\n\t      // so do it again\n\t      postProcessAttachments(results).then(finish);\n\t    } else {\n\t      finish();\n\t    }\n\t  }\n\n\t  var objectStores = [DOC_STORE, BY_SEQ_STORE];\n\t  if (opts.attachments) {\n\t    objectStores.push(ATTACH_STORE);\n\t  }\n\t  var txnResult = openTransactionSafely(idb, objectStores, 'readonly');\n\t  if (txnResult.error) {\n\t    return opts.complete(txnResult.error);\n\t  }\n\t  txn = txnResult.txn;\n\t  txn.onabort = idbError(opts.complete);\n\t  txn.oncomplete = onTxnComplete;\n\n\t  bySeqStore = txn.objectStore(BY_SEQ_STORE);\n\t  docStore = txn.objectStore(DOC_STORE);\n\t  docIdRevIndex = bySeqStore.index('_doc_id_rev');\n\n\t  var keyRange = (opts.since && !opts.descending) ?\n\t    IDBKeyRange.lowerBound(opts.since, true) : null;\n\n\t  runBatchedCursor(bySeqStore, keyRange, opts.descending, limit, onBatch);\n\t}\n\n\tvar cachedDBs = new ExportedMap();\n\tvar blobSupportPromise;\n\tvar openReqList = new ExportedMap();\n\n\tfunction IdbPouch(opts, callback) {\n\t  var api = this;\n\n\t  enqueueTask(function (thisCallback) {\n\t    init(api, opts, thisCallback);\n\t  }, callback, api.constructor);\n\t}\n\n\tfunction init(api, opts, callback) {\n\n\t  var dbName = opts.name;\n\n\t  var idb = null;\n\t  api._meta = null;\n\n\t  // called when creating a fresh new database\n\t  function createSchema(db) {\n\t    var docStore = db.createObjectStore(DOC_STORE, {keyPath : 'id'});\n\t    db.createObjectStore(BY_SEQ_STORE, {autoIncrement: true})\n\t      .createIndex('_doc_id_rev', '_doc_id_rev', {unique: true});\n\t    db.createObjectStore(ATTACH_STORE, {keyPath: 'digest'});\n\t    db.createObjectStore(META_STORE, {keyPath: 'id', autoIncrement: false});\n\t    db.createObjectStore(DETECT_BLOB_SUPPORT_STORE);\n\n\t    // added in v2\n\t    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {unique : false});\n\n\t    // added in v3\n\t    db.createObjectStore(LOCAL_STORE, {keyPath: '_id'});\n\n\t    // added in v4\n\t    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE,\n\t      {autoIncrement: true});\n\t    attAndSeqStore.createIndex('seq', 'seq');\n\t    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {unique: true});\n\t  }\n\n\t  // migration to version 2\n\t  // unfortunately \"deletedOrLocal\" is a misnomer now that we no longer\n\t  // store local docs in the main doc-store, but whaddyagonnado\n\t  function addDeletedOrLocalIndex(txn, callback) {\n\t    var docStore = txn.objectStore(DOC_STORE);\n\t    docStore.createIndex('deletedOrLocal', 'deletedOrLocal', {unique : false});\n\n\t    docStore.openCursor().onsuccess = function (event) {\n\t      var cursor = event.target.result;\n\t      if (cursor) {\n\t        var metadata = cursor.value;\n\t        var deleted = isDeleted(metadata);\n\t        metadata.deletedOrLocal = deleted ? \"1\" : \"0\";\n\t        docStore.put(metadata);\n\t        cursor.continue();\n\t      } else {\n\t        callback();\n\t      }\n\t    };\n\t  }\n\n\t  // migration to version 3 (part 1)\n\t  function createLocalStoreSchema(db) {\n\t    db.createObjectStore(LOCAL_STORE, {keyPath: '_id'})\n\t      .createIndex('_doc_id_rev', '_doc_id_rev', {unique: true});\n\t  }\n\n\t  // migration to version 3 (part 2)\n\t  function migrateLocalStore(txn, cb) {\n\t    var localStore = txn.objectStore(LOCAL_STORE);\n\t    var docStore = txn.objectStore(DOC_STORE);\n\t    var seqStore = txn.objectStore(BY_SEQ_STORE);\n\n\t    var cursor = docStore.openCursor();\n\t    cursor.onsuccess = function (event) {\n\t      var cursor = event.target.result;\n\t      if (cursor) {\n\t        var metadata = cursor.value;\n\t        var docId = metadata.id;\n\t        var local = isLocalId(docId);\n\t        var rev$$1 = winningRev(metadata);\n\t        if (local) {\n\t          var docIdRev = docId + \"::\" + rev$$1;\n\t          // remove all seq entries\n\t          // associated with this docId\n\t          var start = docId + \"::\";\n\t          var end = docId + \"::~\";\n\t          var index = seqStore.index('_doc_id_rev');\n\t          var range = IDBKeyRange.bound(start, end, false, false);\n\t          var seqCursor = index.openCursor(range);\n\t          seqCursor.onsuccess = function (e) {\n\t            seqCursor = e.target.result;\n\t            if (!seqCursor) {\n\t              // done\n\t              docStore.delete(cursor.primaryKey);\n\t              cursor.continue();\n\t            } else {\n\t              var data = seqCursor.value;\n\t              if (data._doc_id_rev === docIdRev) {\n\t                localStore.put(data);\n\t              }\n\t              seqStore.delete(seqCursor.primaryKey);\n\t              seqCursor.continue();\n\t            }\n\t          };\n\t        } else {\n\t          cursor.continue();\n\t        }\n\t      } else if (cb) {\n\t        cb();\n\t      }\n\t    };\n\t  }\n\n\t  // migration to version 4 (part 1)\n\t  function addAttachAndSeqStore(db) {\n\t    var attAndSeqStore = db.createObjectStore(ATTACH_AND_SEQ_STORE,\n\t      {autoIncrement: true});\n\t    attAndSeqStore.createIndex('seq', 'seq');\n\t    attAndSeqStore.createIndex('digestSeq', 'digestSeq', {unique: true});\n\t  }\n\n\t  // migration to version 4 (part 2)\n\t  function migrateAttsAndSeqs(txn, callback) {\n\t    var seqStore = txn.objectStore(BY_SEQ_STORE);\n\t    var attStore = txn.objectStore(ATTACH_STORE);\n\t    var attAndSeqStore = txn.objectStore(ATTACH_AND_SEQ_STORE);\n\n\t    // need to actually populate the table. this is the expensive part,\n\t    // so as an optimization, check first that this database even\n\t    // contains attachments\n\t    var req = attStore.count();\n\t    req.onsuccess = function (e) {\n\t      var count = e.target.result;\n\t      if (!count) {\n\t        return callback(); // done\n\t      }\n\n\t      seqStore.openCursor().onsuccess = function (e) {\n\t        var cursor = e.target.result;\n\t        if (!cursor) {\n\t          return callback(); // done\n\t        }\n\t        var doc = cursor.value;\n\t        var seq = cursor.primaryKey;\n\t        var atts = Object.keys(doc._attachments || {});\n\t        var digestMap = {};\n\t        for (var j = 0; j < atts.length; j++) {\n\t          var att = doc._attachments[atts[j]];\n\t          digestMap[att.digest] = true; // uniq digests, just in case\n\t        }\n\t        var digests = Object.keys(digestMap);\n\t        for (j = 0; j < digests.length; j++) {\n\t          var digest = digests[j];\n\t          attAndSeqStore.put({\n\t            seq: seq,\n\t            digestSeq: digest + '::' + seq\n\t          });\n\t        }\n\t        cursor.continue();\n\t      };\n\t    };\n\t  }\n\n\t  // migration to version 5\n\t  // Instead of relying on on-the-fly migration of metadata,\n\t  // this brings the doc-store to its modern form:\n\t  // - metadata.winningrev\n\t  // - metadata.seq\n\t  // - stringify the metadata when storing it\n\t  function migrateMetadata(txn) {\n\n\t    function decodeMetadataCompat(storedObject) {\n\t      if (!storedObject.data) {\n\t        // old format, when we didn't store it stringified\n\t        storedObject.deleted = storedObject.deletedOrLocal === '1';\n\t        return storedObject;\n\t      }\n\t      return decodeMetadata(storedObject);\n\t    }\n\n\t    // ensure that every metadata has a winningRev and seq,\n\t    // which was previously created on-the-fly but better to migrate\n\t    var bySeqStore = txn.objectStore(BY_SEQ_STORE);\n\t    var docStore = txn.objectStore(DOC_STORE);\n\t    var cursor = docStore.openCursor();\n\t    cursor.onsuccess = function (e) {\n\t      var cursor = e.target.result;\n\t      if (!cursor) {\n\t        return; // done\n\t      }\n\t      var metadata = decodeMetadataCompat(cursor.value);\n\n\t      metadata.winningRev = metadata.winningRev ||\n\t        winningRev(metadata);\n\n\t      function fetchMetadataSeq() {\n\t        // metadata.seq was added post-3.2.0, so if it's missing,\n\t        // we need to fetch it manually\n\t        var start = metadata.id + '::';\n\t        var end = metadata.id + '::\\uffff';\n\t        var req = bySeqStore.index('_doc_id_rev').openCursor(\n\t          IDBKeyRange.bound(start, end));\n\n\t        var metadataSeq = 0;\n\t        req.onsuccess = function (e) {\n\t          var cursor = e.target.result;\n\t          if (!cursor) {\n\t            metadata.seq = metadataSeq;\n\t            return onGetMetadataSeq();\n\t          }\n\t          var seq = cursor.primaryKey;\n\t          if (seq > metadataSeq) {\n\t            metadataSeq = seq;\n\t          }\n\t          cursor.continue();\n\t        };\n\t      }\n\n\t      function onGetMetadataSeq() {\n\t        var metadataToStore = encodeMetadata(metadata,\n\t          metadata.winningRev, metadata.deleted);\n\n\t        var req = docStore.put(metadataToStore);\n\t        req.onsuccess = function () {\n\t          cursor.continue();\n\t        };\n\t      }\n\n\t      if (metadata.seq) {\n\t        return onGetMetadataSeq();\n\t      }\n\n\t      fetchMetadataSeq();\n\t    };\n\n\t  }\n\n\t  api._remote = false;\n\t  api.type = function () {\n\t    return 'idb';\n\t  };\n\n\t  api._id = toPromise(function (callback) {\n\t    callback(null, api._meta.instanceId);\n\t  });\n\n\t  api._bulkDocs = function idb_bulkDocs(req, reqOpts, callback) {\n\t    idbBulkDocs(opts, req, reqOpts, api, idb, callback);\n\t  };\n\n\t  // First we look up the metadata in the ids database, then we fetch the\n\t  // current revision(s) from the by sequence store\n\t  api._get = function idb_get(id, opts, callback) {\n\t    var doc;\n\t    var metadata;\n\t    var err;\n\t    var txn = opts.ctx;\n\t    if (!txn) {\n\t      var txnResult = openTransactionSafely(idb,\n\t        [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n\t      if (txnResult.error) {\n\t        return callback(txnResult.error);\n\t      }\n\t      txn = txnResult.txn;\n\t    }\n\n\t    function finish() {\n\t      callback(err, {doc: doc, metadata: metadata, ctx: txn});\n\t    }\n\n\t    txn.objectStore(DOC_STORE).get(id).onsuccess = function (e) {\n\t      metadata = decodeMetadata(e.target.result);\n\t      // we can determine the result here if:\n\t      // 1. there is no such document\n\t      // 2. the document is deleted and we don't ask about specific rev\n\t      // When we ask with opts.rev we expect the answer to be either\n\t      // doc (possibly with _deleted=true) or missing error\n\t      if (!metadata) {\n\t        err = createError(MISSING_DOC, 'missing');\n\t        return finish();\n\t      }\n\n\t      var rev$$1;\n\t      if (!opts.rev) {\n\t        rev$$1 = metadata.winningRev;\n\t        var deleted = isDeleted(metadata);\n\t        if (deleted) {\n\t          err = createError(MISSING_DOC, \"deleted\");\n\t          return finish();\n\t        }\n\t      } else {\n\t        rev$$1 = opts.latest ? latest(opts.rev, metadata) : opts.rev;\n\t      }\n\n\t      var objectStore = txn.objectStore(BY_SEQ_STORE);\n\t      var key = metadata.id + '::' + rev$$1;\n\n\t      objectStore.index('_doc_id_rev').get(key).onsuccess = function (e) {\n\t        doc = e.target.result;\n\t        if (doc) {\n\t          doc = decodeDoc(doc);\n\t        }\n\t        if (!doc) {\n\t          err = createError(MISSING_DOC, 'missing');\n\t          return finish();\n\t        }\n\t        finish();\n\t      };\n\t    };\n\t  };\n\n\t  api._getAttachment = function (docId, attachId, attachment, opts, callback) {\n\t    var txn;\n\t    if (opts.ctx) {\n\t      txn = opts.ctx;\n\t    } else {\n\t      var txnResult = openTransactionSafely(idb,\n\t        [DOC_STORE, BY_SEQ_STORE, ATTACH_STORE], 'readonly');\n\t      if (txnResult.error) {\n\t        return callback(txnResult.error);\n\t      }\n\t      txn = txnResult.txn;\n\t    }\n\t    var digest = attachment.digest;\n\t    var type = attachment.content_type;\n\n\t    txn.objectStore(ATTACH_STORE).get(digest).onsuccess = function (e) {\n\t      var body = e.target.result.body;\n\t      readBlobData(body, type, opts.binary, function (blobData) {\n\t        callback(null, blobData);\n\t      });\n\t    };\n\t  };\n\n\t  api._info = function idb_info(callback) {\n\t    var updateSeq;\n\t    var docCount;\n\n\t    var txnResult = openTransactionSafely(idb, [META_STORE, BY_SEQ_STORE], 'readonly');\n\t    if (txnResult.error) {\n\t      return callback(txnResult.error);\n\t    }\n\t    var txn = txnResult.txn;\n\t    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n\t      docCount = e.target.result.docCount;\n\t    };\n\t    txn.objectStore(BY_SEQ_STORE).openCursor(null, 'prev').onsuccess = function (e) {\n\t      var cursor = e.target.result;\n\t      updateSeq = cursor ? cursor.key : 0;\n\t    };\n\n\t    txn.oncomplete = function () {\n\t      callback(null, {\n\t        doc_count: docCount,\n\t        update_seq: updateSeq,\n\t        // for debugging\n\t        idb_attachment_format: (api._meta.blobSupport ? 'binary' : 'base64')\n\t      });\n\t    };\n\t  };\n\n\t  api._allDocs = function idb_allDocs(opts, callback) {\n\t    idbAllDocs(opts, idb, callback);\n\t  };\n\n\t  api._changes = function idbChanges(opts) {\n\t    return changes(opts, api, dbName, idb);\n\t  };\n\n\t  api._close = function (callback) {\n\t    // https://developer.mozilla.org/en-US/docs/IndexedDB/IDBDatabase#close\n\t    // \"Returns immediately and closes the connection in a separate thread...\"\n\t    idb.close();\n\t    cachedDBs.delete(dbName);\n\t    callback();\n\t  };\n\n\t  api._getRevisionTree = function (docId, callback) {\n\t    var txnResult = openTransactionSafely(idb, [DOC_STORE], 'readonly');\n\t    if (txnResult.error) {\n\t      return callback(txnResult.error);\n\t    }\n\t    var txn = txnResult.txn;\n\t    var req = txn.objectStore(DOC_STORE).get(docId);\n\t    req.onsuccess = function (event) {\n\t      var doc = decodeMetadata(event.target.result);\n\t      if (!doc) {\n\t        callback(createError(MISSING_DOC));\n\t      } else {\n\t        callback(null, doc.rev_tree);\n\t      }\n\t    };\n\t  };\n\n\t  // This function removes revisions of document docId\n\t  // which are listed in revs and sets this document\n\t  // revision to to rev_tree\n\t  api._doCompaction = function (docId, revs, callback) {\n\t    var stores = [\n\t      DOC_STORE,\n\t      BY_SEQ_STORE,\n\t      ATTACH_STORE,\n\t      ATTACH_AND_SEQ_STORE\n\t    ];\n\t    var txnResult = openTransactionSafely(idb, stores, 'readwrite');\n\t    if (txnResult.error) {\n\t      return callback(txnResult.error);\n\t    }\n\t    var txn = txnResult.txn;\n\n\t    var docStore = txn.objectStore(DOC_STORE);\n\n\t    docStore.get(docId).onsuccess = function (event) {\n\t      var metadata = decodeMetadata(event.target.result);\n\t      traverseRevTree(metadata.rev_tree, function (isLeaf, pos,\n\t                                                         revHash, ctx, opts) {\n\t        var rev$$1 = pos + '-' + revHash;\n\t        if (revs.indexOf(rev$$1) !== -1) {\n\t          opts.status = 'missing';\n\t        }\n\t      });\n\t      compactRevs(revs, docId, txn);\n\t      var winningRev$$1 = metadata.winningRev;\n\t      var deleted = metadata.deleted;\n\t      txn.objectStore(DOC_STORE).put(\n\t        encodeMetadata(metadata, winningRev$$1, deleted));\n\t    };\n\t    txn.onabort = idbError(callback);\n\t    txn.oncomplete = function () {\n\t      callback();\n\t    };\n\t  };\n\n\n\t  api._getLocal = function (id, callback) {\n\t    var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readonly');\n\t    if (txnResult.error) {\n\t      return callback(txnResult.error);\n\t    }\n\t    var tx = txnResult.txn;\n\t    var req = tx.objectStore(LOCAL_STORE).get(id);\n\n\t    req.onerror = idbError(callback);\n\t    req.onsuccess = function (e) {\n\t      var doc = e.target.result;\n\t      if (!doc) {\n\t        callback(createError(MISSING_DOC));\n\t      } else {\n\t        delete doc['_doc_id_rev']; // for backwards compat\n\t        callback(null, doc);\n\t      }\n\t    };\n\t  };\n\n\t  api._putLocal = function (doc, opts, callback) {\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    delete doc._revisions; // ignore this, trust the rev\n\t    var oldRev = doc._rev;\n\t    var id = doc._id;\n\t    if (!oldRev) {\n\t      doc._rev = '0-1';\n\t    } else {\n\t      doc._rev = '0-' + (parseInt(oldRev.split('-')[1], 10) + 1);\n\t    }\n\n\t    var tx = opts.ctx;\n\t    var ret;\n\t    if (!tx) {\n\t      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n\t      if (txnResult.error) {\n\t        return callback(txnResult.error);\n\t      }\n\t      tx = txnResult.txn;\n\t      tx.onerror = idbError(callback);\n\t      tx.oncomplete = function () {\n\t        if (ret) {\n\t          callback(null, ret);\n\t        }\n\t      };\n\t    }\n\n\t    var oStore = tx.objectStore(LOCAL_STORE);\n\t    var req;\n\t    if (oldRev) {\n\t      req = oStore.get(id);\n\t      req.onsuccess = function (e) {\n\t        var oldDoc = e.target.result;\n\t        if (!oldDoc || oldDoc._rev !== oldRev) {\n\t          callback(createError(REV_CONFLICT));\n\t        } else { // update\n\t          var req = oStore.put(doc);\n\t          req.onsuccess = function () {\n\t            ret = {ok: true, id: doc._id, rev: doc._rev};\n\t            if (opts.ctx) { // return immediately\n\t              callback(null, ret);\n\t            }\n\t          };\n\t        }\n\t      };\n\t    } else { // new doc\n\t      req = oStore.add(doc);\n\t      req.onerror = function (e) {\n\t        // constraint error, already exists\n\t        callback(createError(REV_CONFLICT));\n\t        e.preventDefault(); // avoid transaction abort\n\t        e.stopPropagation(); // avoid transaction onerror\n\t      };\n\t      req.onsuccess = function () {\n\t        ret = {ok: true, id: doc._id, rev: doc._rev};\n\t        if (opts.ctx) { // return immediately\n\t          callback(null, ret);\n\t        }\n\t      };\n\t    }\n\t  };\n\n\t  api._removeLocal = function (doc, opts, callback) {\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    var tx = opts.ctx;\n\t    if (!tx) {\n\t      var txnResult = openTransactionSafely(idb, [LOCAL_STORE], 'readwrite');\n\t      if (txnResult.error) {\n\t        return callback(txnResult.error);\n\t      }\n\t      tx = txnResult.txn;\n\t      tx.oncomplete = function () {\n\t        if (ret) {\n\t          callback(null, ret);\n\t        }\n\t      };\n\t    }\n\t    var ret;\n\t    var id = doc._id;\n\t    var oStore = tx.objectStore(LOCAL_STORE);\n\t    var req = oStore.get(id);\n\n\t    req.onerror = idbError(callback);\n\t    req.onsuccess = function (e) {\n\t      var oldDoc = e.target.result;\n\t      if (!oldDoc || oldDoc._rev !== doc._rev) {\n\t        callback(createError(MISSING_DOC));\n\t      } else {\n\t        oStore.delete(id);\n\t        ret = {ok: true, id: id, rev: '0-0'};\n\t        if (opts.ctx) { // return immediately\n\t          callback(null, ret);\n\t        }\n\t      }\n\t    };\n\t  };\n\n\t  api._destroy = function (opts, callback) {\n\t    changesHandler.removeAllListeners(dbName);\n\n\t    //Close open request for \"dbName\" database to fix ie delay.\n\t    var openReq = openReqList.get(dbName);\n\t    if (openReq && openReq.result) {\n\t      openReq.result.close();\n\t      cachedDBs.delete(dbName);\n\t    }\n\t    var req = indexedDB.deleteDatabase(dbName);\n\n\t    req.onsuccess = function () {\n\t      //Remove open request from the list.\n\t      openReqList.delete(dbName);\n\t      if (hasLocalStorage() && (dbName in localStorage)) {\n\t        delete localStorage[dbName];\n\t      }\n\t      callback(null, { 'ok': true });\n\t    };\n\n\t    req.onerror = idbError(callback);\n\t  };\n\n\t  var cached = cachedDBs.get(dbName);\n\n\t  if (cached) {\n\t    idb = cached.idb;\n\t    api._meta = cached.global;\n\t    return lib(function () {\n\t      callback(null, api);\n\t    });\n\t  }\n\n\t  var req = indexedDB.open(dbName, ADAPTER_VERSION);\n\t  openReqList.set(dbName, req);\n\n\t  req.onupgradeneeded = function (e) {\n\t    var db = e.target.result;\n\t    if (e.oldVersion < 1) {\n\t      return createSchema(db); // new db, initial schema\n\t    }\n\t    // do migrations\n\n\t    var txn = e.currentTarget.transaction;\n\t    // these migrations have to be done in this function, before\n\t    // control is returned to the event loop, because IndexedDB\n\n\t    if (e.oldVersion < 3) {\n\t      createLocalStoreSchema(db); // v2 -> v3\n\t    }\n\t    if (e.oldVersion < 4) {\n\t      addAttachAndSeqStore(db); // v3 -> v4\n\t    }\n\n\t    var migrations = [\n\t      addDeletedOrLocalIndex, // v1 -> v2\n\t      migrateLocalStore,      // v2 -> v3\n\t      migrateAttsAndSeqs,     // v3 -> v4\n\t      migrateMetadata         // v4 -> v5\n\t    ];\n\n\t    var i = e.oldVersion;\n\n\t    function next() {\n\t      var migration = migrations[i - 1];\n\t      i++;\n\t      if (migration) {\n\t        migration(txn, next);\n\t      }\n\t    }\n\n\t    next();\n\t  };\n\n\t  req.onsuccess = function (e) {\n\n\t    idb = e.target.result;\n\n\t    idb.onversionchange = function () {\n\t      idb.close();\n\t      cachedDBs.delete(dbName);\n\t    };\n\n\t    idb.onabort = function (e) {\n\t      guardedConsole('error', 'Database has a global failure', e.target.error);\n\t      idb.close();\n\t      cachedDBs.delete(dbName);\n\t    };\n\n\t    // Do a few setup operations (in parallel as much as possible):\n\t    // 1. Fetch meta doc\n\t    // 2. Check blob support\n\t    // 3. Calculate docCount\n\t    // 4. Generate an instanceId if necessary\n\t    // 5. Store docCount and instanceId on meta doc\n\n\t    var txn = idb.transaction([\n\t      META_STORE,\n\t      DETECT_BLOB_SUPPORT_STORE,\n\t      DOC_STORE\n\t    ], 'readwrite');\n\n\t    var storedMetaDoc = false;\n\t    var metaDoc;\n\t    var docCount;\n\t    var blobSupport;\n\t    var instanceId;\n\n\t    function completeSetup() {\n\t      if (typeof blobSupport === 'undefined' || !storedMetaDoc) {\n\t        return;\n\t      }\n\t      api._meta = {\n\t        name: dbName,\n\t        instanceId: instanceId,\n\t        blobSupport: blobSupport\n\t      };\n\n\t      cachedDBs.set(dbName, {\n\t        idb: idb,\n\t        global: api._meta\n\t      });\n\t      callback(null, api);\n\t    }\n\n\t    function storeMetaDocIfReady() {\n\t      if (typeof docCount === 'undefined' || typeof metaDoc === 'undefined') {\n\t        return;\n\t      }\n\t      var instanceKey = dbName + '_id';\n\t      if (instanceKey in metaDoc) {\n\t        instanceId = metaDoc[instanceKey];\n\t      } else {\n\t        metaDoc[instanceKey] = instanceId = uuid$1();\n\t      }\n\t      metaDoc.docCount = docCount;\n\t      txn.objectStore(META_STORE).put(metaDoc);\n\t    }\n\n\t    //\n\t    // fetch or generate the instanceId\n\t    //\n\t    txn.objectStore(META_STORE).get(META_STORE).onsuccess = function (e) {\n\t      metaDoc = e.target.result || { id: META_STORE };\n\t      storeMetaDocIfReady();\n\t    };\n\n\t    //\n\t    // countDocs\n\t    //\n\t    countDocs(txn, function (count) {\n\t      docCount = count;\n\t      storeMetaDocIfReady();\n\t    });\n\n\t    //\n\t    // check blob support\n\t    //\n\t    if (!blobSupportPromise) {\n\t      // make sure blob support is only checked once\n\t      blobSupportPromise = checkBlobSupport(txn);\n\t    }\n\n\t    blobSupportPromise.then(function (val) {\n\t      blobSupport = val;\n\t      completeSetup();\n\t    });\n\n\t    // only when the metadata put transaction has completed,\n\t    // consider the setup done\n\t    txn.oncomplete = function () {\n\t      storedMetaDoc = true;\n\t      completeSetup();\n\t    };\n\t    txn.onabort = idbError(callback);\n\t  };\n\n\t  req.onerror = function (e) {\n\t    var msg = e.target.error && e.target.error.message;\n\n\t    if (!msg) {\n\t      msg = 'Failed to open indexedDB, are you in private browsing mode?';\n\t    } else if (msg.indexOf(\"stored database is a higher version\") !== -1) {\n\t      msg = new Error('This DB was created with the newer \"indexeddb\" adapter, but you are trying to open it with the older \"idb\" adapter');\n\t    }\n\n\t    guardedConsole('error', msg);\n\t    callback(createError(IDB_ERROR, msg));\n\t  };\n\t}\n\n\tIdbPouch.valid = function () {\n\t  // Following #7085 buggy idb versions (typically Safari < 10.1) are\n\t  // considered valid.\n\n\t  // On Firefox SecurityError is thrown while referencing indexedDB if cookies\n\t  // are not allowed. `typeof indexedDB` also triggers the error.\n\t  try {\n\t    // some outdated implementations of IDB that appear on Samsung\n\t    // and HTC Android devices <4.4 are missing IDBKeyRange\n\t    return typeof indexedDB !== 'undefined' && typeof IDBKeyRange !== 'undefined';\n\t  } catch (e) {\n\t    return false;\n\t  }\n\t};\n\n\tfunction IDBPouch (PouchDB) {\n\t  PouchDB.adapter('idb', IdbPouch, true);\n\t}\n\n\t// dead simple promise pool, inspired by https://github.com/timdp/es6-promise-pool\n\t// but much smaller in code size. limits the number of concurrent promises that are executed\n\n\n\tfunction pool(promiseFactories, limit) {\n\t  return new Promise(function (resolve, reject) {\n\t    var running = 0;\n\t    var current = 0;\n\t    var done = 0;\n\t    var len = promiseFactories.length;\n\t    var err;\n\n\t    function runNext() {\n\t      running++;\n\t      promiseFactories[current++]().then(onSuccess, onError);\n\t    }\n\n\t    function doNext() {\n\t      if (++done === len) {\n\t        /* istanbul ignore if */\n\t        if (err) {\n\t          reject(err);\n\t        } else {\n\t          resolve();\n\t        }\n\t      } else {\n\t        runNextBatch();\n\t      }\n\t    }\n\n\t    function onSuccess() {\n\t      running--;\n\t      doNext();\n\t    }\n\n\t    /* istanbul ignore next */\n\t    function onError(thisErr) {\n\t      running--;\n\t      err = err || thisErr;\n\t      doNext();\n\t    }\n\n\t    function runNextBatch() {\n\t      while (running < limit && current < len) {\n\t        runNext();\n\t      }\n\t    }\n\n\t    runNextBatch();\n\t  });\n\t}\n\n\tvar CHANGES_BATCH_SIZE = 25;\n\tvar MAX_SIMULTANEOUS_REVS = 50;\n\tvar CHANGES_TIMEOUT_BUFFER = 5000;\n\tvar DEFAULT_HEARTBEAT = 10000;\n\n\tvar supportsBulkGetMap = {};\n\n\tfunction readAttachmentsAsBlobOrBuffer(row) {\n\t  var doc = row.doc || row.ok;\n\t  var atts = doc && doc._attachments;\n\t  if (!atts) {\n\t    return;\n\t  }\n\t  Object.keys(atts).forEach(function (filename) {\n\t    var att = atts[filename];\n\t    att.data = b64ToBluffer(att.data, att.content_type);\n\t  });\n\t}\n\n\tfunction encodeDocId(id) {\n\t  if (/^_design/.test(id)) {\n\t    return '_design/' + encodeURIComponent(id.slice(8));\n\t  }\n\t  if (/^_local/.test(id)) {\n\t    return '_local/' + encodeURIComponent(id.slice(7));\n\t  }\n\t  return encodeURIComponent(id);\n\t}\n\n\tfunction preprocessAttachments$1(doc) {\n\t  if (!doc._attachments || !Object.keys(doc._attachments)) {\n\t    return Promise.resolve();\n\t  }\n\n\t  return Promise.all(Object.keys(doc._attachments).map(function (key) {\n\t    var attachment = doc._attachments[key];\n\t    if (attachment.data && typeof attachment.data !== 'string') {\n\t      return new Promise(function (resolve) {\n\t        blobToBase64(attachment.data, resolve);\n\t      }).then(function (b64) {\n\t        attachment.data = b64;\n\t      });\n\t    }\n\t  }));\n\t}\n\n\tfunction hasUrlPrefix(opts) {\n\t  if (!opts.prefix) {\n\t    return false;\n\t  }\n\t  var protocol = parseUri(opts.prefix).protocol;\n\t  return protocol === 'http' || protocol === 'https';\n\t}\n\n\t// Get all the information you possibly can about the URI given by name and\n\t// return it as a suitable object.\n\tfunction getHost(name, opts) {\n\t  // encode db name if opts.prefix is a url (#5574)\n\t  if (hasUrlPrefix(opts)) {\n\t    var dbName = opts.name.substr(opts.prefix.length);\n\t    // Ensure prefix has a trailing slash\n\t    var prefix = opts.prefix.replace(/\\/?$/, '/');\n\t    name = prefix + encodeURIComponent(dbName);\n\t  }\n\n\t  var uri = parseUri(name);\n\t  if (uri.user || uri.password) {\n\t    uri.auth = {username: uri.user, password: uri.password};\n\t  }\n\n\t  // Split the path part of the URI into parts using '/' as the delimiter\n\t  // after removing any leading '/' and any trailing '/'\n\t  var parts = uri.path.replace(/(^\\/|\\/$)/g, '').split('/');\n\n\t  uri.db = parts.pop();\n\t  // Prevent double encoding of URI component\n\t  if (uri.db.indexOf('%') === -1) {\n\t    uri.db = encodeURIComponent(uri.db);\n\t  }\n\n\t  uri.path = parts.join('/');\n\n\t  return uri;\n\t}\n\n\t// Generate a URL with the host data given by opts and the given path\n\tfunction genDBUrl(opts, path) {\n\t  return genUrl(opts, opts.db + '/' + path);\n\t}\n\n\t// Generate a URL with the host data given by opts and the given path\n\tfunction genUrl(opts, path) {\n\t  // If the host already has a path, then we need to have a path delimiter\n\t  // Otherwise, the path delimiter is the empty string\n\t  var pathDel = !opts.path ? '' : '/';\n\n\t  // If the host already has a path, then we need to have a path delimiter\n\t  // Otherwise, the path delimiter is the empty string\n\t  return opts.protocol + '://' + opts.host +\n\t         (opts.port ? (':' + opts.port) : '') +\n\t         '/' + opts.path + pathDel + path;\n\t}\n\n\tfunction paramsToStr(params) {\n\t  return '?' + Object.keys(params).map(function (k) {\n\t    return k + '=' + encodeURIComponent(params[k]);\n\t  }).join('&');\n\t}\n\n\tfunction shouldCacheBust(opts) {\n\t  var ua = (typeof navigator !== 'undefined' && navigator.userAgent) ?\n\t      navigator.userAgent.toLowerCase() : '';\n\t  var isIE = ua.indexOf('msie') !== -1;\n\t  var isTrident = ua.indexOf('trident') !== -1;\n\t  var isEdge = ua.indexOf('edge') !== -1;\n\t  var isGET = !('method' in opts) || opts.method === 'GET';\n\t  return (isIE || isTrident || isEdge) && isGET;\n\t}\n\n\t// Implements the PouchDB API for dealing with CouchDB instances over HTTP\n\tfunction HttpPouch(opts, callback) {\n\n\t  // The functions that will be publicly available for HttpPouch\n\t  var api = this;\n\n\t  var host = getHost(opts.name, opts);\n\t  var dbUrl = genDBUrl(host, '');\n\n\t  opts = clone(opts);\n\n\t  var ourFetch = function (url, options) {\n\n\t    options = options || {};\n\t    options.headers = options.headers || new h();\n\n\t    options.credentials = 'include';\n\n\t    if (opts.auth || host.auth) {\n\t      var nAuth = opts.auth || host.auth;\n\t      var str = nAuth.username + ':' + nAuth.password;\n\t      var token = thisBtoa(unescape(encodeURIComponent(str)));\n\t      options.headers.set('Authorization', 'Basic ' + token);\n\t    }\n\n\t    var headers = opts.headers || {};\n\t    Object.keys(headers).forEach(function (key) {\n\t      options.headers.append(key, headers[key]);\n\t    });\n\n\t    /* istanbul ignore if */\n\t    if (shouldCacheBust(options)) {\n\t      url += (url.indexOf('?') === -1 ? '?' : '&') + '_nonce=' + Date.now();\n\t    }\n\n\t    var fetchFun = opts.fetch || f$1;\n\t    return fetchFun(url, options);\n\t  };\n\n\t  function adapterFun$$1(name, fun) {\n\t    return adapterFun(name, argsarray(function (args) {\n\t      setup().then(function () {\n\t        return fun.apply(this, args);\n\t      }).catch(function (e) {\n\t        var callback = args.pop();\n\t        callback(e);\n\t      });\n\t    })).bind(api);\n\t  }\n\n\t  function fetchJSON(url, options, callback) {\n\n\t    var result = {};\n\n\t    options = options || {};\n\t    options.headers = options.headers || new h();\n\n\t    if (!options.headers.get('Content-Type')) {\n\t      options.headers.set('Content-Type', 'application/json');\n\t    }\n\t    if (!options.headers.get('Accept')) {\n\t      options.headers.set('Accept', 'application/json');\n\t    }\n\n\t    return ourFetch(url, options).then(function (response) {\n\t      result.ok = response.ok;\n\t      result.status = response.status;\n\t      return response.json();\n\t    }).then(function (json) {\n\t      result.data = json;\n\t      if (!result.ok) {\n\t        result.data.status = result.status;\n\t        var err = generateErrorFromResponse(result.data);\n\t        if (callback) {\n\t          return callback(err);\n\t        } else {\n\t          throw err;\n\t        }\n\t      }\n\n\t      if (Array.isArray(result.data)) {\n\t        result.data = result.data.map(function (v) {\n\t          if (v.error || v.missing) {\n\t            return generateErrorFromResponse(v);\n\t          } else {\n\t            return v;\n\t          }\n\t        });\n\t      }\n\n\t      if (callback) {\n\t        callback(null, result.data);\n\t      } else {\n\t        return result;\n\t      }\n\t    });\n\t  }\n\n\t  var setupPromise;\n\n\t  function setup() {\n\t    if (opts.skip_setup) {\n\t      return Promise.resolve();\n\t    }\n\n\t    // If there is a setup in process or previous successful setup\n\t    // done then we will use that\n\t    // If previous setups have been rejected we will try again\n\t    if (setupPromise) {\n\t      return setupPromise;\n\t    }\n\n\t    setupPromise = fetchJSON(dbUrl).catch(function (err) {\n\t      if (err && err.status && err.status === 404) {\n\t        // Doesnt exist, create it\n\t        explainError(404, 'PouchDB is just detecting if the remote exists.');\n\t        return fetchJSON(dbUrl, {method: 'PUT'});\n\t      } else {\n\t        return Promise.reject(err);\n\t      }\n\t    }).catch(function (err) {\n\t      // If we try to create a database that already exists, skipped in\n\t      // istanbul since its catching a race condition.\n\t      /* istanbul ignore if */\n\t      if (err && err.status && err.status === 412) {\n\t        return true;\n\t      }\n\t      return Promise.reject(err);\n\t    });\n\n\t    setupPromise.catch(function () {\n\t      setupPromise = null;\n\t    });\n\n\t    return setupPromise;\n\t  }\n\n\t  lib(function () {\n\t    callback(null, api);\n\t  });\n\n\t  api._remote = true;\n\n\t  /* istanbul ignore next */\n\t  api.type = function () {\n\t    return 'http';\n\t  };\n\n\t  api.id = adapterFun$$1('id', function (callback) {\n\t    ourFetch(genUrl(host, '')).then(function (response) {\n\t      return response.json();\n\t    }).catch(function () {\n\t      return {};\n\t    }).then(function (result) {\n\t      // Bad response or missing `uuid` should not prevent ID generation.\n\t      var uuid$$1 = (result && result.uuid) ?\n\t          (result.uuid + host.db) : genDBUrl(host, '');\n\t      callback(null, uuid$$1);\n\t    });\n\t  });\n\n\t  // Sends a POST request to the host calling the couchdb _compact function\n\t  //    version: The version of CouchDB it is running\n\t  api.compact = adapterFun$$1('compact', function (opts, callback) {\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    opts = clone(opts);\n\n\t    fetchJSON(genDBUrl(host, '_compact'), {method: 'POST'}).then(function () {\n\t      function ping() {\n\t        api.info(function (err, res) {\n\t          // CouchDB may send a \"compact_running:true\" if it's\n\t          // already compacting. PouchDB Server doesn't.\n\t          /* istanbul ignore else */\n\t          if (res && !res.compact_running) {\n\t            callback(null, {ok: true});\n\t          } else {\n\t            setTimeout(ping, opts.interval || 200);\n\t          }\n\t        });\n\t      }\n\t      // Ping the http if it's finished compaction\n\t      ping();\n\t    });\n\t  });\n\n\t  api.bulkGet = adapterFun('bulkGet', function (opts, callback) {\n\t    var self = this;\n\n\t    function doBulkGet(cb) {\n\t      var params = {};\n\t      if (opts.revs) {\n\t        params.revs = true;\n\t      }\n\t      if (opts.attachments) {\n\t        /* istanbul ignore next */\n\t        params.attachments = true;\n\t      }\n\t      if (opts.latest) {\n\t        params.latest = true;\n\t      }\n\t      fetchJSON(genDBUrl(host, '_bulk_get' + paramsToStr(params)), {\n\t        method: 'POST',\n\t        body: JSON.stringify({ docs: opts.docs})\n\t      }).then(function (result) {\n\t        if (opts.attachments && opts.binary) {\n\t          result.data.results.forEach(function (res) {\n\t            res.docs.forEach(readAttachmentsAsBlobOrBuffer);\n\t          });\n\t        }\n\t        cb(null, result.data);\n\t      }).catch(cb);\n\t    }\n\n\t    /* istanbul ignore next */\n\t    function doBulkGetShim() {\n\t      // avoid \"url too long error\" by splitting up into multiple requests\n\t      var batchSize = MAX_SIMULTANEOUS_REVS;\n\t      var numBatches = Math.ceil(opts.docs.length / batchSize);\n\t      var numDone = 0;\n\t      var results = new Array(numBatches);\n\n\t      function onResult(batchNum) {\n\t        return function (err, res) {\n\t          // err is impossible because shim returns a list of errs in that case\n\t          results[batchNum] = res.results;\n\t          if (++numDone === numBatches) {\n\t            callback(null, {results: flatten(results)});\n\t          }\n\t        };\n\t      }\n\n\t      for (var i = 0; i < numBatches; i++) {\n\t        var subOpts = pick(opts, ['revs', 'attachments', 'binary', 'latest']);\n\t        subOpts.docs = opts.docs.slice(i * batchSize,\n\t          Math.min(opts.docs.length, (i + 1) * batchSize));\n\t        bulkGet(self, subOpts, onResult(i));\n\t      }\n\t    }\n\n\t    // mark the whole database as either supporting or not supporting _bulk_get\n\t    var dbUrl = genUrl(host, '');\n\t    var supportsBulkGet = supportsBulkGetMap[dbUrl];\n\n\t    /* istanbul ignore next */\n\t    if (typeof supportsBulkGet !== 'boolean') {\n\t      // check if this database supports _bulk_get\n\t      doBulkGet(function (err, res) {\n\t        if (err) {\n\t          supportsBulkGetMap[dbUrl] = false;\n\t          explainError(\n\t            err.status,\n\t            'PouchDB is just detecting if the remote ' +\n\t            'supports the _bulk_get API.'\n\t          );\n\t          doBulkGetShim();\n\t        } else {\n\t          supportsBulkGetMap[dbUrl] = true;\n\t          callback(null, res);\n\t        }\n\t      });\n\t    } else if (supportsBulkGet) {\n\t      doBulkGet(callback);\n\t    } else {\n\t      doBulkGetShim();\n\t    }\n\t  });\n\n\t  // Calls GET on the host, which gets back a JSON string containing\n\t  //    couchdb: A welcome string\n\t  //    version: The version of CouchDB it is running\n\t  api._info = function (callback) {\n\t    setup().then(function () {\n\t      return ourFetch(genDBUrl(host, ''));\n\t    }).then(function (response) {\n\t      return response.json();\n\t    }).then(function (info) {\n\t      info.host = genDBUrl(host, '');\n\t      callback(null, info);\n\t    }).catch(callback);\n\t  };\n\n\t  api.fetch = function (path, options) {\n\t    return setup().then(function () {\n\t      var url = path.substring(0, 1) === '/' ?\n\t        genUrl(host, path.substring(1)) :\n\t        genDBUrl(host, path);\n\t      return ourFetch(url, options);\n\t    });\n\t  };\n\n\t  // Get the document with the given id from the database given by host.\n\t  // The id could be solely the _id in the database, or it may be a\n\t  // _design/ID or _local/ID path\n\t  api.get = adapterFun$$1('get', function (id, opts, callback) {\n\t    // If no options were given, set the callback to the second parameter\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    opts = clone(opts);\n\n\t    // List of parameters to add to the GET request\n\t    var params = {};\n\n\t    if (opts.revs) {\n\t      params.revs = true;\n\t    }\n\n\t    if (opts.revs_info) {\n\t      params.revs_info = true;\n\t    }\n\n\t    if (opts.latest) {\n\t      params.latest = true;\n\t    }\n\n\t    if (opts.open_revs) {\n\t      if (opts.open_revs !== \"all\") {\n\t        opts.open_revs = JSON.stringify(opts.open_revs);\n\t      }\n\t      params.open_revs = opts.open_revs;\n\t    }\n\n\t    if (opts.rev) {\n\t      params.rev = opts.rev;\n\t    }\n\n\t    if (opts.conflicts) {\n\t      params.conflicts = opts.conflicts;\n\t    }\n\n\t    /* istanbul ignore if */\n\t    if (opts.update_seq) {\n\t      params.update_seq = opts.update_seq;\n\t    }\n\n\t    id = encodeDocId(id);\n\n\t    function fetchAttachments(doc) {\n\t      var atts = doc._attachments;\n\t      var filenames = atts && Object.keys(atts);\n\t      if (!atts || !filenames.length) {\n\t        return;\n\t      }\n\t      // we fetch these manually in separate XHRs, because\n\t      // Sync Gateway would normally send it back as multipart/mixed,\n\t      // which we cannot parse. Also, this is more efficient than\n\t      // receiving attachments as base64-encoded strings.\n\t      function fetchData(filename) {\n\t        var att = atts[filename];\n\t        var path = encodeDocId(doc._id) + '/' + encodeAttachmentId(filename) +\n\t            '?rev=' + doc._rev;\n\t        return ourFetch(genDBUrl(host, path)).then(function (response) {\n\t          if ('buffer' in response) {\n\t            return response.buffer();\n\t          } else {\n\t            /* istanbul ignore next */\n\t            return response.blob();\n\t          }\n\t        }).then(function (blob) {\n\t          if (opts.binary) {\n\t            var typeFieldDescriptor = Object.getOwnPropertyDescriptor(blob.__proto__, 'type');\n\t            if (!typeFieldDescriptor || typeFieldDescriptor.set) {\n\t              blob.type = att.content_type;\n\t            }\n\t            return blob;\n\t          }\n\t          return new Promise(function (resolve) {\n\t            blobToBase64(blob, resolve);\n\t          });\n\t        }).then(function (data) {\n\t          delete att.stub;\n\t          delete att.length;\n\t          att.data = data;\n\t        });\n\t      }\n\n\t      var promiseFactories = filenames.map(function (filename) {\n\t        return function () {\n\t          return fetchData(filename);\n\t        };\n\t      });\n\n\t      // This limits the number of parallel xhr requests to 5 any time\n\t      // to avoid issues with maximum browser request limits\n\t      return pool(promiseFactories, 5);\n\t    }\n\n\t    function fetchAllAttachments(docOrDocs) {\n\t      if (Array.isArray(docOrDocs)) {\n\t        return Promise.all(docOrDocs.map(function (doc) {\n\t          if (doc.ok) {\n\t            return fetchAttachments(doc.ok);\n\t          }\n\t        }));\n\t      }\n\t      return fetchAttachments(docOrDocs);\n\t    }\n\n\t    var url = genDBUrl(host, id + paramsToStr(params));\n\t    fetchJSON(url).then(function (res) {\n\t      return Promise.resolve().then(function () {\n\t        if (opts.attachments) {\n\t          return fetchAllAttachments(res.data);\n\t        }\n\t      }).then(function () {\n\t        callback(null, res.data);\n\t      });\n\t    }).catch(function (e) {\n\t      e.docId = id;\n\t      callback(e);\n\t    });\n\t  });\n\n\n\t  // Delete the document given by doc from the database given by host.\n\t  api.remove = adapterFun$$1('remove', function (docOrId, optsOrRev, opts, cb) {\n\t    var doc;\n\t    if (typeof optsOrRev === 'string') {\n\t      // id, rev, opts, callback style\n\t      doc = {\n\t        _id: docOrId,\n\t        _rev: optsOrRev\n\t      };\n\t      if (typeof opts === 'function') {\n\t        cb = opts;\n\t        opts = {};\n\t      }\n\t    } else {\n\t      // doc, opts, callback style\n\t      doc = docOrId;\n\t      if (typeof optsOrRev === 'function') {\n\t        cb = optsOrRev;\n\t        opts = {};\n\t      } else {\n\t        cb = opts;\n\t        opts = optsOrRev;\n\t      }\n\t    }\n\n\t    var rev$$1 = (doc._rev || opts.rev);\n\t    var url = genDBUrl(host, encodeDocId(doc._id)) + '?rev=' + rev$$1;\n\n\t    fetchJSON(url, {method: 'DELETE'}, cb).catch(cb);\n\t  });\n\n\t  function encodeAttachmentId(attachmentId) {\n\t    return attachmentId.split(\"/\").map(encodeURIComponent).join(\"/\");\n\t  }\n\n\t  // Get the attachment\n\t  api.getAttachment = adapterFun$$1('getAttachment', function (docId, attachmentId,\n\t                                                            opts, callback) {\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    var params = opts.rev ? ('?rev=' + opts.rev) : '';\n\t    var url = genDBUrl(host, encodeDocId(docId)) + '/' +\n\t        encodeAttachmentId(attachmentId) + params;\n\t    var contentType;\n\t    ourFetch(url, {method: 'GET'}).then(function (response) {\n\t      contentType = response.headers.get('content-type');\n\t      if (!response.ok) {\n\t        throw response;\n\t      } else {\n\t        if (typeof process !== 'undefined' && !true && typeof response.buffer === 'function') {\n\t          return response.buffer();\n\t        } else {\n\t          /* istanbul ignore next */\n\t          return response.blob();\n\t        }\n\t      }\n\t    }).then(function (blob) {\n\t      // TODO: also remove\n\t      if (typeof process !== 'undefined' && !true) {\n\t        blob.type = contentType;\n\t      }\n\t      callback(null, blob);\n\t    }).catch(function (err) {\n\t      callback(err);\n\t    });\n\t  });\n\n\t  // Remove the attachment given by the id and rev\n\t  api.removeAttachment =  adapterFun$$1('removeAttachment', function (docId,\n\t                                                                   attachmentId,\n\t                                                                   rev$$1,\n\t                                                                   callback) {\n\t    var url = genDBUrl(host, encodeDocId(docId) + '/' +\n\t                       encodeAttachmentId(attachmentId)) + '?rev=' + rev$$1;\n\t    fetchJSON(url, {method: 'DELETE'}, callback).catch(callback);\n\t  });\n\n\t  // Add the attachment given by blob and its contentType property\n\t  // to the document with the given id, the revision given by rev, and\n\t  // add it to the database given by host.\n\t  api.putAttachment = adapterFun$$1('putAttachment', function (docId, attachmentId,\n\t                                                            rev$$1, blob,\n\t                                                            type, callback) {\n\t    if (typeof type === 'function') {\n\t      callback = type;\n\t      type = blob;\n\t      blob = rev$$1;\n\t      rev$$1 = null;\n\t    }\n\t    var id = encodeDocId(docId) + '/' + encodeAttachmentId(attachmentId);\n\t    var url = genDBUrl(host, id);\n\t    if (rev$$1) {\n\t      url += '?rev=' + rev$$1;\n\t    }\n\n\t    if (typeof blob === 'string') {\n\t      // input is assumed to be a base64 string\n\t      var binary;\n\t      try {\n\t        binary = thisAtob(blob);\n\t      } catch (err) {\n\t        return callback(createError(BAD_ARG,\n\t                        'Attachment is not a valid base64 string'));\n\t      }\n\t      blob = binary ? binStringToBluffer(binary, type) : '';\n\t    }\n\n\t    // Add the attachment\n\t    fetchJSON(url, {\n\t      headers: new h({'Content-Type': type}),\n\t      method: 'PUT',\n\t      body: blob\n\t    }, callback).catch(callback);\n\t  });\n\n\t  // Update/create multiple documents given by req in the database\n\t  // given by host.\n\t  api._bulkDocs = function (req, opts, callback) {\n\t    // If new_edits=false then it prevents the database from creating\n\t    // new revision numbers for the documents. Instead it just uses\n\t    // the old ones. This is used in database replication.\n\t    req.new_edits = opts.new_edits;\n\n\t    setup().then(function () {\n\t      return Promise.all(req.docs.map(preprocessAttachments$1));\n\t    }).then(function () {\n\t      // Update/create the documents\n\t      return fetchJSON(genDBUrl(host, '_bulk_docs'), {\n\t        method: 'POST',\n\t        body: JSON.stringify(req)\n\t      }, callback);\n\t    }).catch(callback);\n\t  };\n\n\n\t  // Update/create document\n\t  api._put = function (doc, opts, callback) {\n\t    setup().then(function () {\n\t      return preprocessAttachments$1(doc);\n\t    }).then(function () {\n\t      return fetchJSON(genDBUrl(host, encodeDocId(doc._id)), {\n\t        method: 'PUT',\n\t        body: JSON.stringify(doc)\n\t      });\n\t    }).then(function (result) {\n\t      callback(null, result.data);\n\t    }).catch(function (err) {\n\t      err.docId = doc && doc._id;\n\t      callback(err);\n\t    });\n\t  };\n\n\n\t  // Get a listing of the documents in the database given\n\t  // by host and ordered by increasing id.\n\t  api.allDocs = adapterFun$$1('allDocs', function (opts, callback) {\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    opts = clone(opts);\n\n\t    // List of parameters to add to the GET request\n\t    var params = {};\n\t    var body;\n\t    var method = 'GET';\n\n\t    if (opts.conflicts) {\n\t      params.conflicts = true;\n\t    }\n\n\t    /* istanbul ignore if */\n\t    if (opts.update_seq) {\n\t      params.update_seq = true;\n\t    }\n\n\t    if (opts.descending) {\n\t      params.descending = true;\n\t    }\n\n\t    if (opts.include_docs) {\n\t      params.include_docs = true;\n\t    }\n\n\t    // added in CouchDB 1.6.0\n\t    if (opts.attachments) {\n\t      params.attachments = true;\n\t    }\n\n\t    if (opts.key) {\n\t      params.key = JSON.stringify(opts.key);\n\t    }\n\n\t    if (opts.start_key) {\n\t      opts.startkey = opts.start_key;\n\t    }\n\n\t    if (opts.startkey) {\n\t      params.startkey = JSON.stringify(opts.startkey);\n\t    }\n\n\t    if (opts.end_key) {\n\t      opts.endkey = opts.end_key;\n\t    }\n\n\t    if (opts.endkey) {\n\t      params.endkey = JSON.stringify(opts.endkey);\n\t    }\n\n\t    if (typeof opts.inclusive_end !== 'undefined') {\n\t      params.inclusive_end = !!opts.inclusive_end;\n\t    }\n\n\t    if (typeof opts.limit !== 'undefined') {\n\t      params.limit = opts.limit;\n\t    }\n\n\t    if (typeof opts.skip !== 'undefined') {\n\t      params.skip = opts.skip;\n\t    }\n\n\t    var paramStr = paramsToStr(params);\n\n\t    if (typeof opts.keys !== 'undefined') {\n\t      method = 'POST';\n\t      body = {keys: opts.keys};\n\t    }\n\n\t    fetchJSON(genDBUrl(host, '_all_docs' + paramStr), {\n\t       method: method,\n\t      body: JSON.stringify(body)\n\t    }).then(function (result) {\n\t      if (opts.include_docs && opts.attachments && opts.binary) {\n\t        result.data.rows.forEach(readAttachmentsAsBlobOrBuffer);\n\t      }\n\t      callback(null, result.data);\n\t    }).catch(callback);\n\t  });\n\n\t  // Get a list of changes made to documents in the database given by host.\n\t  // TODO According to the README, there should be two other methods here,\n\t  // api.changes.addListener and api.changes.removeListener.\n\t  api._changes = function (opts) {\n\n\t    // We internally page the results of a changes request, this means\n\t    // if there is a large set of changes to be returned we can start\n\t    // processing them quicker instead of waiting on the entire\n\t    // set of changes to return and attempting to process them at once\n\t    var batchSize = 'batch_size' in opts ? opts.batch_size : CHANGES_BATCH_SIZE;\n\n\t    opts = clone(opts);\n\n\t    if (opts.continuous && !('heartbeat' in opts)) {\n\t      opts.heartbeat = DEFAULT_HEARTBEAT;\n\t    }\n\n\t    var requestTimeout = ('timeout' in opts) ? opts.timeout : 30 * 1000;\n\n\t    // ensure CHANGES_TIMEOUT_BUFFER applies\n\t    if ('timeout' in opts && opts.timeout &&\n\t      (requestTimeout - opts.timeout) < CHANGES_TIMEOUT_BUFFER) {\n\t        requestTimeout = opts.timeout + CHANGES_TIMEOUT_BUFFER;\n\t    }\n\n\t    /* istanbul ignore if */\n\t    if ('heartbeat' in opts && opts.heartbeat &&\n\t       (requestTimeout - opts.heartbeat) < CHANGES_TIMEOUT_BUFFER) {\n\t        requestTimeout = opts.heartbeat + CHANGES_TIMEOUT_BUFFER;\n\t    }\n\n\t    var params = {};\n\t    if ('timeout' in opts && opts.timeout) {\n\t      params.timeout = opts.timeout;\n\t    }\n\n\t    var limit = (typeof opts.limit !== 'undefined') ? opts.limit : false;\n\t    var leftToFetch = limit;\n\n\t    if (opts.style) {\n\t      params.style = opts.style;\n\t    }\n\n\t    if (opts.include_docs || opts.filter && typeof opts.filter === 'function') {\n\t      params.include_docs = true;\n\t    }\n\n\t    if (opts.attachments) {\n\t      params.attachments = true;\n\t    }\n\n\t    if (opts.continuous) {\n\t      params.feed = 'longpoll';\n\t    }\n\n\t    if (opts.seq_interval) {\n\t      params.seq_interval = opts.seq_interval;\n\t    }\n\n\t    if (opts.conflicts) {\n\t      params.conflicts = true;\n\t    }\n\n\t    if (opts.descending) {\n\t      params.descending = true;\n\t    }\n\n\t    /* istanbul ignore if */\n\t    if (opts.update_seq) {\n\t      params.update_seq = true;\n\t    }\n\n\t    if ('heartbeat' in opts) {\n\t      // If the heartbeat value is false, it disables the default heartbeat\n\t      if (opts.heartbeat) {\n\t        params.heartbeat = opts.heartbeat;\n\t      }\n\t    }\n\n\t    if (opts.filter && typeof opts.filter === 'string') {\n\t      params.filter = opts.filter;\n\t    }\n\n\t    if (opts.view && typeof opts.view === 'string') {\n\t      params.filter = '_view';\n\t      params.view = opts.view;\n\t    }\n\n\t    // If opts.query_params exists, pass it through to the changes request.\n\t    // These parameters may be used by the filter on the source database.\n\t    if (opts.query_params && typeof opts.query_params === 'object') {\n\t      for (var param_name in opts.query_params) {\n\t        /* istanbul ignore else */\n\t        if (opts.query_params.hasOwnProperty(param_name)) {\n\t          params[param_name] = opts.query_params[param_name];\n\t        }\n\t      }\n\t    }\n\n\t    var method = 'GET';\n\t    var body;\n\n\t    if (opts.doc_ids) {\n\t      // set this automagically for the user; it's annoying that couchdb\n\t      // requires both a \"filter\" and a \"doc_ids\" param.\n\t      params.filter = '_doc_ids';\n\t      method = 'POST';\n\t      body = {doc_ids: opts.doc_ids };\n\t    }\n\t    /* istanbul ignore next */\n\t    else if (opts.selector) {\n\t      // set this automagically for the user, similar to above\n\t      params.filter = '_selector';\n\t      method = 'POST';\n\t      body = {selector: opts.selector };\n\t    }\n\n\t    var controller = new a();\n\t    var lastFetchedSeq;\n\n\t    // Get all the changes starting wtih the one immediately after the\n\t    // sequence number given by since.\n\t    var fetchData = function (since, callback) {\n\t      if (opts.aborted) {\n\t        return;\n\t      }\n\t      params.since = since;\n\t      // \"since\" can be any kind of json object in Cloudant/CouchDB 2.x\n\t      /* istanbul ignore next */\n\t      if (typeof params.since === \"object\") {\n\t        params.since = JSON.stringify(params.since);\n\t      }\n\n\t      if (opts.descending) {\n\t        if (limit) {\n\t          params.limit = leftToFetch;\n\t        }\n\t      } else {\n\t        params.limit = (!limit || leftToFetch > batchSize) ?\n\t          batchSize : leftToFetch;\n\t      }\n\n\t      // Set the options for the ajax call\n\t      var url = genDBUrl(host, '_changes' + paramsToStr(params));\n\t      var fetchOpts = {\n\t        signal: controller.signal,\n\t        method: method,\n\t        body: JSON.stringify(body)\n\t      };\n\t      lastFetchedSeq = since;\n\n\t      /* istanbul ignore if */\n\t      if (opts.aborted) {\n\t        return;\n\t      }\n\n\t      // Get the changes\n\t      setup().then(function () {\n\t        return fetchJSON(url, fetchOpts, callback);\n\t      }).catch(callback);\n\t    };\n\n\t    // If opts.since exists, get all the changes from the sequence\n\t    // number given by opts.since. Otherwise, get all the changes\n\t    // from the sequence number 0.\n\t    var results = {results: []};\n\n\t    var fetched = function (err, res) {\n\t      if (opts.aborted) {\n\t        return;\n\t      }\n\t      var raw_results_length = 0;\n\t      // If the result of the ajax call (res) contains changes (res.results)\n\t      if (res && res.results) {\n\t        raw_results_length = res.results.length;\n\t        results.last_seq = res.last_seq;\n\t        var pending = null;\n\t        var lastSeq = null;\n\t        // Attach 'pending' property if server supports it (CouchDB 2.0+)\n\t        /* istanbul ignore if */\n\t        if (typeof res.pending === 'number') {\n\t          pending = res.pending;\n\t        }\n\t        if (typeof results.last_seq === 'string' || typeof results.last_seq === 'number') {\n\t          lastSeq = results.last_seq;\n\t        }\n\t        // For each change\n\t        var req = {};\n\t        req.query = opts.query_params;\n\t        res.results = res.results.filter(function (c) {\n\t          leftToFetch--;\n\t          var ret = filterChange(opts)(c);\n\t          if (ret) {\n\t            if (opts.include_docs && opts.attachments && opts.binary) {\n\t              readAttachmentsAsBlobOrBuffer(c);\n\t            }\n\t            if (opts.return_docs) {\n\t              results.results.push(c);\n\t            }\n\t            opts.onChange(c, pending, lastSeq);\n\t          }\n\t          return ret;\n\t        });\n\t      } else if (err) {\n\t        // In case of an error, stop listening for changes and call\n\t        // opts.complete\n\t        opts.aborted = true;\n\t        opts.complete(err);\n\t        return;\n\t      }\n\n\t      // The changes feed may have timed out with no results\n\t      // if so reuse last update sequence\n\t      if (res && res.last_seq) {\n\t        lastFetchedSeq = res.last_seq;\n\t      }\n\n\t      var finished = (limit && leftToFetch <= 0) ||\n\t        (res && raw_results_length < batchSize) ||\n\t        (opts.descending);\n\n\t      if ((opts.continuous && !(limit && leftToFetch <= 0)) || !finished) {\n\t        // Queue a call to fetch again with the newest sequence number\n\t        lib(function () { fetchData(lastFetchedSeq, fetched); });\n\t      } else {\n\t        // We're done, call the callback\n\t        opts.complete(null, results);\n\t      }\n\t    };\n\n\t    fetchData(opts.since || 0, fetched);\n\n\t    // Return a method to cancel this method from processing any more\n\t    return {\n\t      cancel: function () {\n\t        opts.aborted = true;\n\t        controller.abort();\n\t      }\n\t    };\n\t  };\n\n\t  // Given a set of document/revision IDs (given by req), tets the subset of\n\t  // those that do NOT correspond to revisions stored in the database.\n\t  // See http://wiki.apache.org/couchdb/HttpPostRevsDiff\n\t  api.revsDiff = adapterFun$$1('revsDiff', function (req, opts, callback) {\n\t    // If no options were given, set the callback to be the second parameter\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\n\t    // Get the missing document/revision IDs\n\t    fetchJSON(genDBUrl(host, '_revs_diff'), {\n\t      method: 'POST',\n\t      body: JSON.stringify(req)\n\t    }, callback).catch(callback);\n\t  });\n\n\t  api._close = function (callback) {\n\t    callback();\n\t  };\n\n\t  api._destroy = function (options, callback) {\n\t    fetchJSON(genDBUrl(host, ''), {method: 'DELETE'}).then(function (json) {\n\t      callback(null, json);\n\t    }).catch(function (err) {\n\t      /* istanbul ignore if */\n\t      if (err.status === 404) {\n\t        callback(null, {ok: true});\n\t      } else {\n\t        callback(err);\n\t      }\n\t    });\n\t  };\n\t}\n\n\t// HttpPouch is a valid adapter.\n\tHttpPouch.valid = function () {\n\t  return true;\n\t};\n\n\tfunction HttpPouch$1 (PouchDB) {\n\t  PouchDB.adapter('http', HttpPouch, false);\n\t  PouchDB.adapter('https', HttpPouch, false);\n\t}\n\n\tfunction QueryParseError(message) {\n\t  this.status = 400;\n\t  this.name = 'query_parse_error';\n\t  this.message = message;\n\t  this.error = true;\n\t  try {\n\t    Error.captureStackTrace(this, QueryParseError);\n\t  } catch (e) {}\n\t}\n\n\tinherits$2(QueryParseError, Error);\n\n\tfunction NotFoundError(message) {\n\t  this.status = 404;\n\t  this.name = 'not_found';\n\t  this.message = message;\n\t  this.error = true;\n\t  try {\n\t    Error.captureStackTrace(this, NotFoundError);\n\t  } catch (e) {}\n\t}\n\n\tinherits$2(NotFoundError, Error);\n\n\tfunction BuiltInError(message) {\n\t  this.status = 500;\n\t  this.name = 'invalid_value';\n\t  this.message = message;\n\t  this.error = true;\n\t  try {\n\t    Error.captureStackTrace(this, BuiltInError);\n\t  } catch (e) {}\n\t}\n\n\tinherits$2(BuiltInError, Error);\n\n\tfunction promisedCallback(promise, callback) {\n\t  if (callback) {\n\t    promise.then(function (res) {\n\t      lib(function () {\n\t        callback(null, res);\n\t      });\n\t    }, function (reason) {\n\t      lib(function () {\n\t        callback(reason);\n\t      });\n\t    });\n\t  }\n\t  return promise;\n\t}\n\n\tfunction callbackify(fun) {\n\t  return argsarray(function (args) {\n\t    var cb = args.pop();\n\t    var promise = fun.apply(this, args);\n\t    if (typeof cb === 'function') {\n\t      promisedCallback(promise, cb);\n\t    }\n\t    return promise;\n\t  });\n\t}\n\n\t// Promise finally util similar to Q.finally\n\tfunction fin(promise, finalPromiseFactory) {\n\t  return promise.then(function (res) {\n\t    return finalPromiseFactory().then(function () {\n\t      return res;\n\t    });\n\t  }, function (reason) {\n\t    return finalPromiseFactory().then(function () {\n\t      throw reason;\n\t    });\n\t  });\n\t}\n\n\tfunction sequentialize(queue, promiseFactory) {\n\t  return function () {\n\t    var args = arguments;\n\t    var that = this;\n\t    return queue.add(function () {\n\t      return promiseFactory.apply(that, args);\n\t    });\n\t  };\n\t}\n\n\t// uniq an array of strings, order not guaranteed\n\t// similar to underscore/lodash _.uniq\n\tfunction uniq(arr) {\n\t  var theSet = new ExportedSet(arr);\n\t  var result = new Array(theSet.size);\n\t  var index = -1;\n\t  theSet.forEach(function (value) {\n\t    result[++index] = value;\n\t  });\n\t  return result;\n\t}\n\n\tfunction mapToKeysArray(map) {\n\t  var result = new Array(map.size);\n\t  var index = -1;\n\t  map.forEach(function (value, key) {\n\t    result[++index] = key;\n\t  });\n\t  return result;\n\t}\n\n\tfunction createBuiltInError(name) {\n\t  var message = 'builtin ' + name +\n\t    ' function requires map values to be numbers' +\n\t    ' or number arrays';\n\t  return new BuiltInError(message);\n\t}\n\n\tfunction sum(values) {\n\t  var result = 0;\n\t  for (var i = 0, len = values.length; i < len; i++) {\n\t    var num = values[i];\n\t    if (typeof num !== 'number') {\n\t      if (Array.isArray(num)) {\n\t        // lists of numbers are also allowed, sum them separately\n\t        result = typeof result === 'number' ? [result] : result;\n\t        for (var j = 0, jLen = num.length; j < jLen; j++) {\n\t          var jNum = num[j];\n\t          if (typeof jNum !== 'number') {\n\t            throw createBuiltInError('_sum');\n\t          } else if (typeof result[j] === 'undefined') {\n\t            result.push(jNum);\n\t          } else {\n\t            result[j] += jNum;\n\t          }\n\t        }\n\t      } else { // not array/number\n\t        throw createBuiltInError('_sum');\n\t      }\n\t    } else if (typeof result === 'number') {\n\t      result += num;\n\t    } else { // add number to array\n\t      result[0] += num;\n\t    }\n\t  }\n\t  return result;\n\t}\n\n\tvar log$1 = guardedConsole.bind(null, 'log');\n\tvar isArray$1 = Array.isArray;\n\tvar toJSON = JSON.parse;\n\n\tfunction evalFunctionWithEval(func, emit) {\n\t  return scopeEval(\n\t    \"return (\" + func.replace(/;\\s*$/, \"\") + \");\",\n\t    {\n\t      emit: emit,\n\t      sum: sum,\n\t      log: log$1,\n\t      isArray: isArray$1,\n\t      toJSON: toJSON\n\t    }\n\t  );\n\t}\n\n\t/*\n\t * Simple task queue to sequentialize actions. Assumes\n\t * callbacks will eventually fire (once).\n\t */\n\n\n\tfunction TaskQueue$1() {\n\t  this.promise = new Promise(function (fulfill) {fulfill(); });\n\t}\n\tTaskQueue$1.prototype.add = function (promiseFactory) {\n\t  this.promise = this.promise.catch(function () {\n\t    // just recover\n\t  }).then(function () {\n\t    return promiseFactory();\n\t  });\n\t  return this.promise;\n\t};\n\tTaskQueue$1.prototype.finish = function () {\n\t  return this.promise;\n\t};\n\n\tfunction stringify$1(input) {\n\t  if (!input) {\n\t    return 'undefined'; // backwards compat for empty reduce\n\t  }\n\t  // for backwards compat with mapreduce, functions/strings are stringified\n\t  // as-is. everything else is JSON-stringified.\n\t  switch (typeof input) {\n\t    case 'function':\n\t      // e.g. a mapreduce map\n\t      return input.toString();\n\t    case 'string':\n\t      // e.g. a mapreduce built-in _reduce function\n\t      return input.toString();\n\t    default:\n\t      // e.g. a JSON object in the case of mango queries\n\t      return JSON.stringify(input);\n\t  }\n\t}\n\n\t/* create a string signature for a view so we can cache it and uniq it */\n\tfunction createViewSignature(mapFun, reduceFun) {\n\t  // the \"undefined\" part is for backwards compatibility\n\t  return stringify$1(mapFun) + stringify$1(reduceFun) + 'undefined';\n\t}\n\n\tfunction createView(sourceDB, viewName, mapFun, reduceFun, temporary, localDocName) {\n\t  var viewSignature = createViewSignature(mapFun, reduceFun);\n\n\t  var cachedViews;\n\t  if (!temporary) {\n\t    // cache this to ensure we don't try to update the same view twice\n\t    cachedViews = sourceDB._cachedViews = sourceDB._cachedViews || {};\n\t    if (cachedViews[viewSignature]) {\n\t      return cachedViews[viewSignature];\n\t    }\n\t  }\n\n\t  var promiseForView = sourceDB.info().then(function (info) {\n\n\t    var depDbName = info.db_name + '-mrview-' +\n\t      (temporary ? 'temp' : stringMd5(viewSignature));\n\n\t    // save the view name in the source db so it can be cleaned up if necessary\n\t    // (e.g. when the _design doc is deleted, remove all associated view data)\n\t    function diffFunction(doc) {\n\t      doc.views = doc.views || {};\n\t      var fullViewName = viewName;\n\t      if (fullViewName.indexOf('/') === -1) {\n\t        fullViewName = viewName + '/' + viewName;\n\t      }\n\t      var depDbs = doc.views[fullViewName] = doc.views[fullViewName] || {};\n\t      /* istanbul ignore if */\n\t      if (depDbs[depDbName]) {\n\t        return; // no update necessary\n\t      }\n\t      depDbs[depDbName] = true;\n\t      return doc;\n\t    }\n\t    return upsert(sourceDB, '_local/' + localDocName, diffFunction).then(function () {\n\t      return sourceDB.registerDependentDatabase(depDbName).then(function (res) {\n\t        var db = res.db;\n\t        db.auto_compaction = true;\n\t        var view = {\n\t          name: depDbName,\n\t          db: db,\n\t          sourceDB: sourceDB,\n\t          adapter: sourceDB.adapter,\n\t          mapFun: mapFun,\n\t          reduceFun: reduceFun\n\t        };\n\t        return view.db.get('_local/lastSeq').catch(function (err) {\n\t          /* istanbul ignore if */\n\t          if (err.status !== 404) {\n\t            throw err;\n\t          }\n\t        }).then(function (lastSeqDoc) {\n\t          view.seq = lastSeqDoc ? lastSeqDoc.seq : 0;\n\t          if (cachedViews) {\n\t            view.db.once('destroyed', function () {\n\t              delete cachedViews[viewSignature];\n\t            });\n\t          }\n\t          return view;\n\t        });\n\t      });\n\t    });\n\t  });\n\n\t  if (cachedViews) {\n\t    cachedViews[viewSignature] = promiseForView;\n\t  }\n\t  return promiseForView;\n\t}\n\n\tvar persistentQueues = {};\n\tvar tempViewQueue = new TaskQueue$1();\n\tvar CHANGES_BATCH_SIZE$1 = 50;\n\n\tfunction parseViewName(name) {\n\t  // can be either 'ddocname/viewname' or just 'viewname'\n\t  // (where the ddoc name is the same)\n\t  return name.indexOf('/') === -1 ? [name, name] : name.split('/');\n\t}\n\n\tfunction isGenOne(changes) {\n\t  // only return true if the current change is 1-\n\t  // and there are no other leafs\n\t  return changes.length === 1 && /^1-/.test(changes[0].rev);\n\t}\n\n\tfunction emitError(db, e) {\n\t  try {\n\t    db.emit('error', e);\n\t  } catch (err) {\n\t    guardedConsole('error',\n\t      'The user\\'s map/reduce function threw an uncaught error.\\n' +\n\t      'You can debug this error by doing:\\n' +\n\t      'myDatabase.on(\\'error\\', function (err) { debugger; });\\n' +\n\t      'Please double-check your map/reduce function.');\n\t    guardedConsole('error', e);\n\t  }\n\t}\n\n\t/**\n\t * Returns an \"abstract\" mapreduce object of the form:\n\t *\n\t *   {\n\t *     query: queryFun,\n\t *     viewCleanup: viewCleanupFun\n\t *   }\n\t *\n\t * Arguments are:\n\t *\n\t * localDoc: string\n\t *   This is for the local doc that gets saved in order to track the\n\t *   \"dependent\" DBs and clean them up for viewCleanup. It should be\n\t *   unique, so that indexer plugins don't collide with each other.\n\t * mapper: function (mapFunDef, emit)\n\t *   Returns a map function based on the mapFunDef, which in the case of\n\t *   normal map/reduce is just the de-stringified function, but may be\n\t *   something else, such as an object in the case of pouchdb-find.\n\t * reducer: function (reduceFunDef)\n\t *   Ditto, but for reducing. Modules don't have to support reducing\n\t *   (e.g. pouchdb-find).\n\t * ddocValidator: function (ddoc, viewName)\n\t *   Throws an error if the ddoc or viewName is not valid.\n\t *   This could be a way to communicate to the user that the configuration for the\n\t *   indexer is invalid.\n\t */\n\tfunction createAbstractMapReduce(localDocName, mapper, reducer, ddocValidator) {\n\n\t  function tryMap(db, fun, doc) {\n\t    // emit an event if there was an error thrown by a map function.\n\t    // putting try/catches in a single function also avoids deoptimizations.\n\t    try {\n\t      fun(doc);\n\t    } catch (e) {\n\t      emitError(db, e);\n\t    }\n\t  }\n\n\t  function tryReduce(db, fun, keys, values, rereduce) {\n\t    // same as above, but returning the result or an error. there are two separate\n\t    // functions to avoid extra memory allocations since the tryCode() case is used\n\t    // for custom map functions (common) vs this function, which is only used for\n\t    // custom reduce functions (rare)\n\t    try {\n\t      return {output : fun(keys, values, rereduce)};\n\t    } catch (e) {\n\t      emitError(db, e);\n\t      return {error: e};\n\t    }\n\t  }\n\n\t  function sortByKeyThenValue(x, y) {\n\t    var keyCompare = collate(x.key, y.key);\n\t    return keyCompare !== 0 ? keyCompare : collate(x.value, y.value);\n\t  }\n\n\t  function sliceResults(results, limit, skip) {\n\t    skip = skip || 0;\n\t    if (typeof limit === 'number') {\n\t      return results.slice(skip, limit + skip);\n\t    } else if (skip > 0) {\n\t      return results.slice(skip);\n\t    }\n\t    return results;\n\t  }\n\n\t  function rowToDocId(row) {\n\t    var val = row.value;\n\t    // Users can explicitly specify a joined doc _id, or it\n\t    // defaults to the doc _id that emitted the key/value.\n\t    var docId = (val && typeof val === 'object' && val._id) || row.id;\n\t    return docId;\n\t  }\n\n\t  function readAttachmentsAsBlobOrBuffer(res) {\n\t    res.rows.forEach(function (row) {\n\t      var atts = row.doc && row.doc._attachments;\n\t      if (!atts) {\n\t        return;\n\t      }\n\t      Object.keys(atts).forEach(function (filename) {\n\t        var att = atts[filename];\n\t        atts[filename].data = b64ToBluffer(att.data, att.content_type);\n\t      });\n\t    });\n\t  }\n\n\t  function postprocessAttachments(opts) {\n\t    return function (res) {\n\t      if (opts.include_docs && opts.attachments && opts.binary) {\n\t        readAttachmentsAsBlobOrBuffer(res);\n\t      }\n\t      return res;\n\t    };\n\t  }\n\n\t  function addHttpParam(paramName, opts, params, asJson) {\n\t    // add an http param from opts to params, optionally json-encoded\n\t    var val = opts[paramName];\n\t    if (typeof val !== 'undefined') {\n\t      if (asJson) {\n\t        val = encodeURIComponent(JSON.stringify(val));\n\t      }\n\t      params.push(paramName + '=' + val);\n\t    }\n\t  }\n\n\t  function coerceInteger(integerCandidate) {\n\t    if (typeof integerCandidate !== 'undefined') {\n\t      var asNumber = Number(integerCandidate);\n\t      // prevents e.g. '1foo' or '1.1' being coerced to 1\n\t      if (!isNaN(asNumber) && asNumber === parseInt(integerCandidate, 10)) {\n\t        return asNumber;\n\t      } else {\n\t        return integerCandidate;\n\t      }\n\t    }\n\t  }\n\n\t  function coerceOptions(opts) {\n\t    opts.group_level = coerceInteger(opts.group_level);\n\t    opts.limit = coerceInteger(opts.limit);\n\t    opts.skip = coerceInteger(opts.skip);\n\t    return opts;\n\t  }\n\n\t  function checkPositiveInteger(number) {\n\t    if (number) {\n\t      if (typeof number !== 'number') {\n\t        return  new QueryParseError('Invalid value for integer: \"' +\n\t          number + '\"');\n\t      }\n\t      if (number < 0) {\n\t        return new QueryParseError('Invalid value for positive integer: ' +\n\t          '\"' + number + '\"');\n\t      }\n\t    }\n\t  }\n\n\t  function checkQueryParseError(options, fun) {\n\t    var startkeyName = options.descending ? 'endkey' : 'startkey';\n\t    var endkeyName = options.descending ? 'startkey' : 'endkey';\n\n\t    if (typeof options[startkeyName] !== 'undefined' &&\n\t      typeof options[endkeyName] !== 'undefined' &&\n\t      collate(options[startkeyName], options[endkeyName]) > 0) {\n\t      throw new QueryParseError('No rows can match your key range, ' +\n\t        'reverse your start_key and end_key or set {descending : true}');\n\t    } else if (fun.reduce && options.reduce !== false) {\n\t      if (options.include_docs) {\n\t        throw new QueryParseError('{include_docs:true} is invalid for reduce');\n\t      } else if (options.keys && options.keys.length > 1 &&\n\t        !options.group && !options.group_level) {\n\t        throw new QueryParseError('Multi-key fetches for reduce views must use ' +\n\t          '{group: true}');\n\t      }\n\t    }\n\t    ['group_level', 'limit', 'skip'].forEach(function (optionName) {\n\t      var error = checkPositiveInteger(options[optionName]);\n\t      if (error) {\n\t        throw error;\n\t      }\n\t    });\n\t  }\n\n\t  function httpQuery(db, fun, opts) {\n\t    // List of parameters to add to the PUT request\n\t    var params = [];\n\t    var body;\n\t    var method = 'GET';\n\t    var ok, status;\n\n\t    // If opts.reduce exists and is defined, then add it to the list\n\t    // of parameters.\n\t    // If reduce=false then the results are that of only the map function\n\t    // not the final result of map and reduce.\n\t    addHttpParam('reduce', opts, params);\n\t    addHttpParam('include_docs', opts, params);\n\t    addHttpParam('attachments', opts, params);\n\t    addHttpParam('limit', opts, params);\n\t    addHttpParam('descending', opts, params);\n\t    addHttpParam('group', opts, params);\n\t    addHttpParam('group_level', opts, params);\n\t    addHttpParam('skip', opts, params);\n\t    addHttpParam('stale', opts, params);\n\t    addHttpParam('conflicts', opts, params);\n\t    addHttpParam('startkey', opts, params, true);\n\t    addHttpParam('start_key', opts, params, true);\n\t    addHttpParam('endkey', opts, params, true);\n\t    addHttpParam('end_key', opts, params, true);\n\t    addHttpParam('inclusive_end', opts, params);\n\t    addHttpParam('key', opts, params, true);\n\t    addHttpParam('update_seq', opts, params);\n\n\t    // Format the list of parameters into a valid URI query string\n\t    params = params.join('&');\n\t    params = params === '' ? '' : '?' + params;\n\n\t    // If keys are supplied, issue a POST to circumvent GET query string limits\n\t    // see http://wiki.apache.org/couchdb/HTTP_view_API#Querying_Options\n\t    if (typeof opts.keys !== 'undefined') {\n\t      var MAX_URL_LENGTH = 2000;\n\t      // according to http://stackoverflow.com/a/417184/680742,\n\t      // the de facto URL length limit is 2000 characters\n\n\t      var keysAsString =\n\t        'keys=' + encodeURIComponent(JSON.stringify(opts.keys));\n\t      if (keysAsString.length + params.length + 1 <= MAX_URL_LENGTH) {\n\t        // If the keys are short enough, do a GET. we do this to work around\n\t        // Safari not understanding 304s on POSTs (see pouchdb/pouchdb#1239)\n\t        params += (params[0] === '?' ? '&' : '?') + keysAsString;\n\t      } else {\n\t        method = 'POST';\n\t        if (typeof fun === 'string') {\n\t          body = {keys: opts.keys};\n\t        } else { // fun is {map : mapfun}, so append to this\n\t          fun.keys = opts.keys;\n\t        }\n\t      }\n\t    }\n\n\t    // We are referencing a query defined in the design doc\n\t    if (typeof fun === 'string') {\n\t      var parts = parseViewName(fun);\n\t      return db.fetch('_design/' + parts[0] + '/_view/' + parts[1] + params, {\n\t        headers: new h({'Content-Type': 'application/json'}),\n\t        method: method,\n\t        body: JSON.stringify(body)\n\t      }).then(function (response) {\n\t        ok = response.ok;\n\t        status = response.status;\n\t        return response.json();\n\t      }).then(function (result) {\n\t        if (!ok) {\n\t          result.status = status;\n\t          throw generateErrorFromResponse(result);\n\t        }\n\t        // fail the entire request if the result contains an error\n\t        result.rows.forEach(function (row) {\n\t          /* istanbul ignore if */\n\t          if (row.value && row.value.error && row.value.error === \"builtin_reduce_error\") {\n\t            throw new Error(row.reason);\n\t          }\n\t        });\n\t        return result;\n\t      }).then(postprocessAttachments(opts));\n\t    }\n\n\t    // We are using a temporary view, terrible for performance, good for testing\n\t    body = body || {};\n\t    Object.keys(fun).forEach(function (key) {\n\t      if (Array.isArray(fun[key])) {\n\t        body[key] = fun[key];\n\t      } else {\n\t        body[key] = fun[key].toString();\n\t      }\n\t    });\n\n\t    return db.fetch('_temp_view' + params, {\n\t      headers: new h({'Content-Type': 'application/json'}),\n\t      method: 'POST',\n\t      body: JSON.stringify(body)\n\t    }).then(function (response) {\n\t        ok = response.ok;\n\t        status = response.status;\n\t      return response.json();\n\t    }).then(function (result) {\n\t      if (!ok) {\n\t        result.status = status;\n\t        throw generateErrorFromResponse(result);\n\t      }\n\t      return result;\n\t    }).then(postprocessAttachments(opts));\n\t  }\n\n\t  // custom adapters can define their own api._query\n\t  // and override the default behavior\n\t  /* istanbul ignore next */\n\t  function customQuery(db, fun, opts) {\n\t    return new Promise(function (resolve, reject) {\n\t      db._query(fun, opts, function (err, res) {\n\t        if (err) {\n\t          return reject(err);\n\t        }\n\t        resolve(res);\n\t      });\n\t    });\n\t  }\n\n\t  // custom adapters can define their own api._viewCleanup\n\t  // and override the default behavior\n\t  /* istanbul ignore next */\n\t  function customViewCleanup(db) {\n\t    return new Promise(function (resolve, reject) {\n\t      db._viewCleanup(function (err, res) {\n\t        if (err) {\n\t          return reject(err);\n\t        }\n\t        resolve(res);\n\t      });\n\t    });\n\t  }\n\n\t  function defaultsTo(value) {\n\t    return function (reason) {\n\t      /* istanbul ignore else */\n\t      if (reason.status === 404) {\n\t        return value;\n\t      } else {\n\t        throw reason;\n\t      }\n\t    };\n\t  }\n\n\t  // returns a promise for a list of docs to update, based on the input docId.\n\t  // the order doesn't matter, because post-3.2.0, bulkDocs\n\t  // is an atomic operation in all three adapters.\n\t  function getDocsToPersist(docId, view, docIdsToChangesAndEmits) {\n\t    var metaDocId = '_local/doc_' + docId;\n\t    var defaultMetaDoc = {_id: metaDocId, keys: []};\n\t    var docData = docIdsToChangesAndEmits.get(docId);\n\t    var indexableKeysToKeyValues = docData[0];\n\t    var changes = docData[1];\n\n\t    function getMetaDoc() {\n\t      if (isGenOne(changes)) {\n\t        // generation 1, so we can safely assume initial state\n\t        // for performance reasons (avoids unnecessary GETs)\n\t        return Promise.resolve(defaultMetaDoc);\n\t      }\n\t      return view.db.get(metaDocId).catch(defaultsTo(defaultMetaDoc));\n\t    }\n\n\t    function getKeyValueDocs(metaDoc) {\n\t      if (!metaDoc.keys.length) {\n\t        // no keys, no need for a lookup\n\t        return Promise.resolve({rows: []});\n\t      }\n\t      return view.db.allDocs({\n\t        keys: metaDoc.keys,\n\t        include_docs: true\n\t      });\n\t    }\n\n\t    function processKeyValueDocs(metaDoc, kvDocsRes) {\n\t      var kvDocs = [];\n\t      var oldKeys = new ExportedSet();\n\n\t      for (var i = 0, len = kvDocsRes.rows.length; i < len; i++) {\n\t        var row = kvDocsRes.rows[i];\n\t        var doc = row.doc;\n\t        if (!doc) { // deleted\n\t          continue;\n\t        }\n\t        kvDocs.push(doc);\n\t        oldKeys.add(doc._id);\n\t        doc._deleted = !indexableKeysToKeyValues.has(doc._id);\n\t        if (!doc._deleted) {\n\t          var keyValue = indexableKeysToKeyValues.get(doc._id);\n\t          if ('value' in keyValue) {\n\t            doc.value = keyValue.value;\n\t          }\n\t        }\n\t      }\n\t      var newKeys = mapToKeysArray(indexableKeysToKeyValues);\n\t      newKeys.forEach(function (key) {\n\t        if (!oldKeys.has(key)) {\n\t          // new doc\n\t          var kvDoc = {\n\t            _id: key\n\t          };\n\t          var keyValue = indexableKeysToKeyValues.get(key);\n\t          if ('value' in keyValue) {\n\t            kvDoc.value = keyValue.value;\n\t          }\n\t          kvDocs.push(kvDoc);\n\t        }\n\t      });\n\t      metaDoc.keys = uniq(newKeys.concat(metaDoc.keys));\n\t      kvDocs.push(metaDoc);\n\n\t      return kvDocs;\n\t    }\n\n\t    return getMetaDoc().then(function (metaDoc) {\n\t      return getKeyValueDocs(metaDoc).then(function (kvDocsRes) {\n\t        return processKeyValueDocs(metaDoc, kvDocsRes);\n\t      });\n\t    });\n\t  }\n\n\t  // updates all emitted key/value docs and metaDocs in the mrview database\n\t  // for the given batch of documents from the source database\n\t  function saveKeyValues(view, docIdsToChangesAndEmits, seq) {\n\t    var seqDocId = '_local/lastSeq';\n\t    return view.db.get(seqDocId)\n\t      .catch(defaultsTo({_id: seqDocId, seq: 0}))\n\t      .then(function (lastSeqDoc) {\n\t        var docIds = mapToKeysArray(docIdsToChangesAndEmits);\n\t        return Promise.all(docIds.map(function (docId) {\n\t          return getDocsToPersist(docId, view, docIdsToChangesAndEmits);\n\t        })).then(function (listOfDocsToPersist) {\n\t          var docsToPersist = flatten(listOfDocsToPersist);\n\t          lastSeqDoc.seq = seq;\n\t          docsToPersist.push(lastSeqDoc);\n\t          // write all docs in a single operation, update the seq once\n\t          return view.db.bulkDocs({docs : docsToPersist});\n\t        });\n\t      });\n\t  }\n\n\t  function getQueue(view) {\n\t    var viewName = typeof view === 'string' ? view : view.name;\n\t    var queue = persistentQueues[viewName];\n\t    if (!queue) {\n\t      queue = persistentQueues[viewName] = new TaskQueue$1();\n\t    }\n\t    return queue;\n\t  }\n\n\t  function updateView(view) {\n\t    return sequentialize(getQueue(view), function () {\n\t      return updateViewInQueue(view);\n\t    })();\n\t  }\n\n\t  function updateViewInQueue(view) {\n\t    // bind the emit function once\n\t    var mapResults;\n\t    var doc;\n\n\t    function emit(key, value) {\n\t      var output = {id: doc._id, key: normalizeKey(key)};\n\t      // Don't explicitly store the value unless it's defined and non-null.\n\t      // This saves on storage space, because often people don't use it.\n\t      if (typeof value !== 'undefined' && value !== null) {\n\t        output.value = normalizeKey(value);\n\t      }\n\t      mapResults.push(output);\n\t    }\n\n\t    var mapFun = mapper(view.mapFun, emit);\n\n\t    var currentSeq = view.seq || 0;\n\n\t    function processChange(docIdsToChangesAndEmits, seq) {\n\t      return function () {\n\t        return saveKeyValues(view, docIdsToChangesAndEmits, seq);\n\t      };\n\t    }\n\n\t    var queue = new TaskQueue$1();\n\n\t    function processNextBatch() {\n\t      return view.sourceDB.changes({\n\t        return_docs: true,\n\t        conflicts: true,\n\t        include_docs: true,\n\t        style: 'all_docs',\n\t        since: currentSeq,\n\t        limit: CHANGES_BATCH_SIZE$1\n\t      }).then(processBatch);\n\t    }\n\n\t    function processBatch(response) {\n\t      var results = response.results;\n\t      if (!results.length) {\n\t        return;\n\t      }\n\t      var docIdsToChangesAndEmits = createDocIdsToChangesAndEmits(results);\n\t      queue.add(processChange(docIdsToChangesAndEmits, currentSeq));\n\t      if (results.length < CHANGES_BATCH_SIZE$1) {\n\t        return;\n\t      }\n\t      return processNextBatch();\n\t    }\n\n\t    function createDocIdsToChangesAndEmits(results) {\n\t      var docIdsToChangesAndEmits = new ExportedMap();\n\t      for (var i = 0, len = results.length; i < len; i++) {\n\t        var change = results[i];\n\t        if (change.doc._id[0] !== '_') {\n\t          mapResults = [];\n\t          doc = change.doc;\n\n\t          if (!doc._deleted) {\n\t            tryMap(view.sourceDB, mapFun, doc);\n\t          }\n\t          mapResults.sort(sortByKeyThenValue);\n\n\t          var indexableKeysToKeyValues = createIndexableKeysToKeyValues(mapResults);\n\t          docIdsToChangesAndEmits.set(change.doc._id, [\n\t            indexableKeysToKeyValues,\n\t            change.changes\n\t          ]);\n\t        }\n\t        currentSeq = change.seq;\n\t      }\n\t      return docIdsToChangesAndEmits;\n\t    }\n\n\t    function createIndexableKeysToKeyValues(mapResults) {\n\t      var indexableKeysToKeyValues = new ExportedMap();\n\t      var lastKey;\n\t      for (var i = 0, len = mapResults.length; i < len; i++) {\n\t        var emittedKeyValue = mapResults[i];\n\t        var complexKey = [emittedKeyValue.key, emittedKeyValue.id];\n\t        if (i > 0 && collate(emittedKeyValue.key, lastKey) === 0) {\n\t          complexKey.push(i); // dup key+id, so make it unique\n\t        }\n\t        indexableKeysToKeyValues.set(toIndexableString(complexKey), emittedKeyValue);\n\t        lastKey = emittedKeyValue.key;\n\t      }\n\t      return indexableKeysToKeyValues;\n\t    }\n\n\t    return processNextBatch().then(function () {\n\t      return queue.finish();\n\t    }).then(function () {\n\t      view.seq = currentSeq;\n\t    });\n\t  }\n\n\t  function reduceView(view, results, options) {\n\t    if (options.group_level === 0) {\n\t      delete options.group_level;\n\t    }\n\n\t    var shouldGroup = options.group || options.group_level;\n\n\t    var reduceFun = reducer(view.reduceFun);\n\n\t    var groups = [];\n\t    var lvl = isNaN(options.group_level) ? Number.POSITIVE_INFINITY :\n\t      options.group_level;\n\t    results.forEach(function (e) {\n\t      var last = groups[groups.length - 1];\n\t      var groupKey = shouldGroup ? e.key : null;\n\n\t      // only set group_level for array keys\n\t      if (shouldGroup && Array.isArray(groupKey)) {\n\t        groupKey = groupKey.slice(0, lvl);\n\t      }\n\n\t      if (last && collate(last.groupKey, groupKey) === 0) {\n\t        last.keys.push([e.key, e.id]);\n\t        last.values.push(e.value);\n\t        return;\n\t      }\n\t      groups.push({\n\t        keys: [[e.key, e.id]],\n\t        values: [e.value],\n\t        groupKey: groupKey\n\t      });\n\t    });\n\t    results = [];\n\t    for (var i = 0, len = groups.length; i < len; i++) {\n\t      var e = groups[i];\n\t      var reduceTry = tryReduce(view.sourceDB, reduceFun, e.keys, e.values, false);\n\t      if (reduceTry.error && reduceTry.error instanceof BuiltInError) {\n\t        // CouchDB returns an error if a built-in errors out\n\t        throw reduceTry.error;\n\t      }\n\t      results.push({\n\t        // CouchDB just sets the value to null if a non-built-in errors out\n\t        value: reduceTry.error ? null : reduceTry.output,\n\t        key: e.groupKey\n\t      });\n\t    }\n\t    // no total_rows/offset when reducing\n\t    return {rows: sliceResults(results, options.limit, options.skip)};\n\t  }\n\n\t  function queryView(view, opts) {\n\t    return sequentialize(getQueue(view), function () {\n\t      return queryViewInQueue(view, opts);\n\t    })();\n\t  }\n\n\t  function queryViewInQueue(view, opts) {\n\t    var totalRows;\n\t    var shouldReduce = view.reduceFun && opts.reduce !== false;\n\t    var skip = opts.skip || 0;\n\t    if (typeof opts.keys !== 'undefined' && !opts.keys.length) {\n\t      // equivalent query\n\t      opts.limit = 0;\n\t      delete opts.keys;\n\t    }\n\n\t    function fetchFromView(viewOpts) {\n\t      viewOpts.include_docs = true;\n\t      return view.db.allDocs(viewOpts).then(function (res) {\n\t        totalRows = res.total_rows;\n\t        return res.rows.map(function (result) {\n\n\t          // implicit migration - in older versions of PouchDB,\n\t          // we explicitly stored the doc as {id: ..., key: ..., value: ...}\n\t          // this is tested in a migration test\n\t          /* istanbul ignore next */\n\t          if ('value' in result.doc && typeof result.doc.value === 'object' &&\n\t            result.doc.value !== null) {\n\t            var keys = Object.keys(result.doc.value).sort();\n\t            // this detection method is not perfect, but it's unlikely the user\n\t            // emitted a value which was an object with these 3 exact keys\n\t            var expectedKeys = ['id', 'key', 'value'];\n\t            if (!(keys < expectedKeys || keys > expectedKeys)) {\n\t              return result.doc.value;\n\t            }\n\t          }\n\n\t          var parsedKeyAndDocId = parseIndexableString(result.doc._id);\n\t          return {\n\t            key: parsedKeyAndDocId[0],\n\t            id: parsedKeyAndDocId[1],\n\t            value: ('value' in result.doc ? result.doc.value : null)\n\t          };\n\t        });\n\t      });\n\t    }\n\n\t    function onMapResultsReady(rows) {\n\t      var finalResults;\n\t      if (shouldReduce) {\n\t        finalResults = reduceView(view, rows, opts);\n\t      } else {\n\t        finalResults = {\n\t          total_rows: totalRows,\n\t          offset: skip,\n\t          rows: rows\n\t        };\n\t      }\n\t      /* istanbul ignore if */\n\t      if (opts.update_seq) {\n\t        finalResults.update_seq = view.seq;\n\t      }\n\t      if (opts.include_docs) {\n\t        var docIds = uniq(rows.map(rowToDocId));\n\n\t        return view.sourceDB.allDocs({\n\t          keys: docIds,\n\t          include_docs: true,\n\t          conflicts: opts.conflicts,\n\t          attachments: opts.attachments,\n\t          binary: opts.binary\n\t        }).then(function (allDocsRes) {\n\t          var docIdsToDocs = new ExportedMap();\n\t          allDocsRes.rows.forEach(function (row) {\n\t            docIdsToDocs.set(row.id, row.doc);\n\t          });\n\t          rows.forEach(function (row) {\n\t            var docId = rowToDocId(row);\n\t            var doc = docIdsToDocs.get(docId);\n\t            if (doc) {\n\t              row.doc = doc;\n\t            }\n\t          });\n\t          return finalResults;\n\t        });\n\t      } else {\n\t        return finalResults;\n\t      }\n\t    }\n\n\t    if (typeof opts.keys !== 'undefined') {\n\t      var keys = opts.keys;\n\t      var fetchPromises = keys.map(function (key) {\n\t        var viewOpts = {\n\t          startkey : toIndexableString([key]),\n\t          endkey   : toIndexableString([key, {}])\n\t        };\n\t        /* istanbul ignore if */\n\t        if (opts.update_seq) {\n\t          viewOpts.update_seq = true;\n\t        }\n\t        return fetchFromView(viewOpts);\n\t      });\n\t      return Promise.all(fetchPromises).then(flatten).then(onMapResultsReady);\n\t    } else { // normal query, no 'keys'\n\t      var viewOpts = {\n\t        descending : opts.descending\n\t      };\n\t      /* istanbul ignore if */\n\t      if (opts.update_seq) {\n\t        viewOpts.update_seq = true;\n\t      }\n\t      var startkey;\n\t      var endkey;\n\t      if ('start_key' in opts) {\n\t        startkey = opts.start_key;\n\t      }\n\t      if ('startkey' in opts) {\n\t        startkey = opts.startkey;\n\t      }\n\t      if ('end_key' in opts) {\n\t        endkey = opts.end_key;\n\t      }\n\t      if ('endkey' in opts) {\n\t        endkey = opts.endkey;\n\t      }\n\t      if (typeof startkey !== 'undefined') {\n\t        viewOpts.startkey = opts.descending ?\n\t          toIndexableString([startkey, {}]) :\n\t          toIndexableString([startkey]);\n\t      }\n\t      if (typeof endkey !== 'undefined') {\n\t        var inclusiveEnd = opts.inclusive_end !== false;\n\t        if (opts.descending) {\n\t          inclusiveEnd = !inclusiveEnd;\n\t        }\n\n\t        viewOpts.endkey = toIndexableString(\n\t          inclusiveEnd ? [endkey, {}] : [endkey]);\n\t      }\n\t      if (typeof opts.key !== 'undefined') {\n\t        var keyStart = toIndexableString([opts.key]);\n\t        var keyEnd = toIndexableString([opts.key, {}]);\n\t        if (viewOpts.descending) {\n\t          viewOpts.endkey = keyStart;\n\t          viewOpts.startkey = keyEnd;\n\t        } else {\n\t          viewOpts.startkey = keyStart;\n\t          viewOpts.endkey = keyEnd;\n\t        }\n\t      }\n\t      if (!shouldReduce) {\n\t        if (typeof opts.limit === 'number') {\n\t          viewOpts.limit = opts.limit;\n\t        }\n\t        viewOpts.skip = skip;\n\t      }\n\t      return fetchFromView(viewOpts).then(onMapResultsReady);\n\t    }\n\t  }\n\n\t  function httpViewCleanup(db) {\n\t    return db.fetch('_view_cleanup', {\n\t      headers: new h({'Content-Type': 'application/json'}),\n\t      method: 'POST'\n\t    }).then(function (response) {\n\t      return response.json();\n\t    });\n\t  }\n\n\t  function localViewCleanup(db) {\n\t    return db.get('_local/' + localDocName).then(function (metaDoc) {\n\t      var docsToViews = new ExportedMap();\n\t      Object.keys(metaDoc.views).forEach(function (fullViewName) {\n\t        var parts = parseViewName(fullViewName);\n\t        var designDocName = '_design/' + parts[0];\n\t        var viewName = parts[1];\n\t        var views = docsToViews.get(designDocName);\n\t        if (!views) {\n\t          views = new ExportedSet();\n\t          docsToViews.set(designDocName, views);\n\t        }\n\t        views.add(viewName);\n\t      });\n\t      var opts = {\n\t        keys : mapToKeysArray(docsToViews),\n\t        include_docs : true\n\t      };\n\t      return db.allDocs(opts).then(function (res) {\n\t        var viewsToStatus = {};\n\t        res.rows.forEach(function (row) {\n\t          var ddocName = row.key.substring(8); // cuts off '_design/'\n\t          docsToViews.get(row.key).forEach(function (viewName) {\n\t            var fullViewName = ddocName + '/' + viewName;\n\t            /* istanbul ignore if */\n\t            if (!metaDoc.views[fullViewName]) {\n\t              // new format, without slashes, to support PouchDB 2.2.0\n\t              // migration test in pouchdb's browser.migration.js verifies this\n\t              fullViewName = viewName;\n\t            }\n\t            var viewDBNames = Object.keys(metaDoc.views[fullViewName]);\n\t            // design doc deleted, or view function nonexistent\n\t            var statusIsGood = row.doc && row.doc.views &&\n\t              row.doc.views[viewName];\n\t            viewDBNames.forEach(function (viewDBName) {\n\t              viewsToStatus[viewDBName] =\n\t                viewsToStatus[viewDBName] || statusIsGood;\n\t            });\n\t          });\n\t        });\n\t        var dbsToDelete = Object.keys(viewsToStatus).filter(\n\t          function (viewDBName) { return !viewsToStatus[viewDBName]; });\n\t        var destroyPromises = dbsToDelete.map(function (viewDBName) {\n\t          return sequentialize(getQueue(viewDBName), function () {\n\t            return new db.constructor(viewDBName, db.__opts).destroy();\n\t          })();\n\t        });\n\t        return Promise.all(destroyPromises).then(function () {\n\t          return {ok: true};\n\t        });\n\t      });\n\t    }, defaultsTo({ok: true}));\n\t  }\n\n\t  function queryPromised(db, fun, opts) {\n\t    /* istanbul ignore next */\n\t    if (typeof db._query === 'function') {\n\t      return customQuery(db, fun, opts);\n\t    }\n\t    if (isRemote(db)) {\n\t      return httpQuery(db, fun, opts);\n\t    }\n\n\t    if (typeof fun !== 'string') {\n\t      // temp_view\n\t      checkQueryParseError(opts, fun);\n\n\t      tempViewQueue.add(function () {\n\t        var createViewPromise = createView(\n\t          /* sourceDB */ db,\n\t          /* viewName */ 'temp_view/temp_view',\n\t          /* mapFun */ fun.map,\n\t          /* reduceFun */ fun.reduce,\n\t          /* temporary */ true,\n\t          /* localDocName */ localDocName);\n\t        return createViewPromise.then(function (view) {\n\t          return fin(updateView(view).then(function () {\n\t            return queryView(view, opts);\n\t          }), function () {\n\t            return view.db.destroy();\n\t          });\n\t        });\n\t      });\n\t      return tempViewQueue.finish();\n\t    } else {\n\t      // persistent view\n\t      var fullViewName = fun;\n\t      var parts = parseViewName(fullViewName);\n\t      var designDocName = parts[0];\n\t      var viewName = parts[1];\n\t      return db.get('_design/' + designDocName).then(function (doc) {\n\t        var fun = doc.views && doc.views[viewName];\n\n\t        if (!fun) {\n\t          // basic validator; it's assumed that every subclass would want this\n\t          throw new NotFoundError('ddoc ' + doc._id + ' has no view named ' +\n\t            viewName);\n\t        }\n\n\t        ddocValidator(doc, viewName);\n\t        checkQueryParseError(opts, fun);\n\n\t        var createViewPromise = createView(\n\t          /* sourceDB */ db,\n\t          /* viewName */ fullViewName,\n\t          /* mapFun */ fun.map,\n\t          /* reduceFun */ fun.reduce,\n\t          /* temporary */ false,\n\t          /* localDocName */ localDocName);\n\t        return createViewPromise.then(function (view) {\n\t          if (opts.stale === 'ok' || opts.stale === 'update_after') {\n\t            if (opts.stale === 'update_after') {\n\t              lib(function () {\n\t                updateView(view);\n\t              });\n\t            }\n\t            return queryView(view, opts);\n\t          } else { // stale not ok\n\t            return updateView(view).then(function () {\n\t              return queryView(view, opts);\n\t            });\n\t          }\n\t        });\n\t      });\n\t    }\n\t  }\n\n\t  function abstractQuery(fun, opts, callback) {\n\t    var db = this;\n\t    if (typeof opts === 'function') {\n\t      callback = opts;\n\t      opts = {};\n\t    }\n\t    opts = opts ? coerceOptions(opts) : {};\n\n\t    if (typeof fun === 'function') {\n\t      fun = {map : fun};\n\t    }\n\n\t    var promise = Promise.resolve().then(function () {\n\t      return queryPromised(db, fun, opts);\n\t    });\n\t    promisedCallback(promise, callback);\n\t    return promise;\n\t  }\n\n\t  var abstractViewCleanup = callbackify(function () {\n\t    var db = this;\n\t    /* istanbul ignore next */\n\t    if (typeof db._viewCleanup === 'function') {\n\t      return customViewCleanup(db);\n\t    }\n\t    if (isRemote(db)) {\n\t      return httpViewCleanup(db);\n\t    }\n\t    return localViewCleanup(db);\n\t  });\n\n\t  return {\n\t    query: abstractQuery,\n\t    viewCleanup: abstractViewCleanup\n\t  };\n\t}\n\n\tvar builtInReduce = {\n\t  _sum: function (keys, values) {\n\t    return sum(values);\n\t  },\n\n\t  _count: function (keys, values) {\n\t    return values.length;\n\t  },\n\n\t  _stats: function (keys, values) {\n\t    // no need to implement rereduce=true, because Pouch\n\t    // will never call it\n\t    function sumsqr(values) {\n\t      var _sumsqr = 0;\n\t      for (var i = 0, len = values.length; i < len; i++) {\n\t        var num = values[i];\n\t        _sumsqr += (num * num);\n\t      }\n\t      return _sumsqr;\n\t    }\n\t    return {\n\t      sum     : sum(values),\n\t      min     : Math.min.apply(null, values),\n\t      max     : Math.max.apply(null, values),\n\t      count   : values.length,\n\t      sumsqr : sumsqr(values)\n\t    };\n\t  }\n\t};\n\n\tfunction getBuiltIn(reduceFunString) {\n\t  if (/^_sum/.test(reduceFunString)) {\n\t    return builtInReduce._sum;\n\t  } else if (/^_count/.test(reduceFunString)) {\n\t    return builtInReduce._count;\n\t  } else if (/^_stats/.test(reduceFunString)) {\n\t    return builtInReduce._stats;\n\t  } else if (/^_/.test(reduceFunString)) {\n\t    throw new Error(reduceFunString + ' is not a supported reduce function.');\n\t  }\n\t}\n\n\tfunction mapper(mapFun, emit) {\n\t  // for temp_views one can use emit(doc, emit), see #38\n\t  if (typeof mapFun === \"function\" && mapFun.length === 2) {\n\t    var origMap = mapFun;\n\t    return function (doc) {\n\t      return origMap(doc, emit);\n\t    };\n\t  } else {\n\t    return evalFunctionWithEval(mapFun.toString(), emit);\n\t  }\n\t}\n\n\tfunction reducer(reduceFun) {\n\t  var reduceFunString = reduceFun.toString();\n\t  var builtIn = getBuiltIn(reduceFunString);\n\t  if (builtIn) {\n\t    return builtIn;\n\t  } else {\n\t    return evalFunctionWithEval(reduceFunString);\n\t  }\n\t}\n\n\tfunction ddocValidator(ddoc, viewName) {\n\t  var fun = ddoc.views && ddoc.views[viewName];\n\t  if (typeof fun.map !== 'string') {\n\t    throw new NotFoundError('ddoc ' + ddoc._id + ' has no string view named ' +\n\t      viewName + ', instead found object of type: ' + typeof fun.map);\n\t  }\n\t}\n\n\tvar localDocName = 'mrviews';\n\tvar abstract = createAbstractMapReduce(localDocName, mapper, reducer, ddocValidator);\n\n\tfunction query(fun, opts, callback) {\n\t  return abstract.query.call(this, fun, opts, callback);\n\t}\n\n\tfunction viewCleanup(callback) {\n\t  return abstract.viewCleanup.call(this, callback);\n\t}\n\n\tvar mapreduce = {\n\t  query: query,\n\t  viewCleanup: viewCleanup\n\t};\n\n\tfunction isGenOne$1(rev$$1) {\n\t  return /^1-/.test(rev$$1);\n\t}\n\n\tfunction fileHasChanged(localDoc, remoteDoc, filename) {\n\t  return !localDoc._attachments ||\n\t         !localDoc._attachments[filename] ||\n\t         localDoc._attachments[filename].digest !== remoteDoc._attachments[filename].digest;\n\t}\n\n\tfunction getDocAttachments(db, doc) {\n\t  var filenames = Object.keys(doc._attachments);\n\t  return Promise.all(filenames.map(function (filename) {\n\t    return db.getAttachment(doc._id, filename, {rev: doc._rev});\n\t  }));\n\t}\n\n\tfunction getDocAttachmentsFromTargetOrSource(target, src, doc) {\n\t  var doCheckForLocalAttachments = isRemote(src) && !isRemote(target);\n\t  var filenames = Object.keys(doc._attachments);\n\n\t  if (!doCheckForLocalAttachments) {\n\t    return getDocAttachments(src, doc);\n\t  }\n\n\t  return target.get(doc._id).then(function (localDoc) {\n\t    return Promise.all(filenames.map(function (filename) {\n\t      if (fileHasChanged(localDoc, doc, filename)) {\n\t        return src.getAttachment(doc._id, filename);\n\t      }\n\n\t      return target.getAttachment(localDoc._id, filename);\n\t    }));\n\t  }).catch(function (error) {\n\t    /* istanbul ignore if */\n\t    if (error.status !== 404) {\n\t      throw error;\n\t    }\n\n\t    return getDocAttachments(src, doc);\n\t  });\n\t}\n\n\tfunction createBulkGetOpts(diffs) {\n\t  var requests = [];\n\t  Object.keys(diffs).forEach(function (id) {\n\t    var missingRevs = diffs[id].missing;\n\t    missingRevs.forEach(function (missingRev) {\n\t      requests.push({\n\t        id: id,\n\t        rev: missingRev\n\t      });\n\t    });\n\t  });\n\n\t  return {\n\t    docs: requests,\n\t    revs: true,\n\t    latest: true\n\t  };\n\t}\n\n\t//\n\t// Fetch all the documents from the src as described in the \"diffs\",\n\t// which is a mapping of docs IDs to revisions. If the state ever\n\t// changes to \"cancelled\", then the returned promise will be rejected.\n\t// Else it will be resolved with a list of fetched documents.\n\t//\n\tfunction getDocs(src, target, diffs, state) {\n\t  diffs = clone(diffs); // we do not need to modify this\n\n\t  var resultDocs = [],\n\t      ok = true;\n\n\t  function getAllDocs() {\n\n\t    var bulkGetOpts = createBulkGetOpts(diffs);\n\n\t    if (!bulkGetOpts.docs.length) { // optimization: skip empty requests\n\t      return;\n\t    }\n\n\t    return src.bulkGet(bulkGetOpts).then(function (bulkGetResponse) {\n\t      /* istanbul ignore if */\n\t      if (state.cancelled) {\n\t        throw new Error('cancelled');\n\t      }\n\t      return Promise.all(bulkGetResponse.results.map(function (bulkGetInfo) {\n\t        return Promise.all(bulkGetInfo.docs.map(function (doc) {\n\t          var remoteDoc = doc.ok;\n\n\t          if (doc.error) {\n\t            // when AUTO_COMPACTION is set, docs can be returned which look\n\t            // like this: {\"missing\":\"1-7c3ac256b693c462af8442f992b83696\"}\n\t            ok = false;\n\t          }\n\n\t          if (!remoteDoc || !remoteDoc._attachments) {\n\t            return remoteDoc;\n\t          }\n\n\t          return getDocAttachmentsFromTargetOrSource(target, src, remoteDoc)\n\t                   .then(function (attachments) {\n\t                           var filenames = Object.keys(remoteDoc._attachments);\n\t                           attachments\n\t                             .forEach(function (attachment, i) {\n\t                                        var att = remoteDoc._attachments[filenames[i]];\n\t                                        delete att.stub;\n\t                                        delete att.length;\n\t                                        att.data = attachment;\n\t                                      });\n\n\t                                      return remoteDoc;\n\t                                    });\n\t        }));\n\t      }))\n\n\t      .then(function (results) {\n\t        resultDocs = resultDocs.concat(flatten(results).filter(Boolean));\n\t      });\n\t    });\n\t  }\n\n\t  function hasAttachments(doc) {\n\t    return doc._attachments && Object.keys(doc._attachments).length > 0;\n\t  }\n\n\t  function hasConflicts(doc) {\n\t    return doc._conflicts && doc._conflicts.length > 0;\n\t  }\n\n\t  function fetchRevisionOneDocs(ids) {\n\t    // Optimization: fetch gen-1 docs and attachments in\n\t    // a single request using _all_docs\n\t    return src.allDocs({\n\t      keys: ids,\n\t      include_docs: true,\n\t      conflicts: true\n\t    }).then(function (res) {\n\t      if (state.cancelled) {\n\t        throw new Error('cancelled');\n\t      }\n\t      res.rows.forEach(function (row) {\n\t        if (row.deleted || !row.doc || !isGenOne$1(row.value.rev) ||\n\t            hasAttachments(row.doc) || hasConflicts(row.doc)) {\n\t          // if any of these conditions apply, we need to fetch using get()\n\t          return;\n\t        }\n\n\t        // strip _conflicts array to appease CSG (#5793)\n\t        /* istanbul ignore if */\n\t        if (row.doc._conflicts) {\n\t          delete row.doc._conflicts;\n\t        }\n\n\t        // the doc we got back from allDocs() is sufficient\n\t        resultDocs.push(row.doc);\n\t        delete diffs[row.id];\n\t      });\n\t    });\n\t  }\n\n\t  function getRevisionOneDocs() {\n\t    // filter out the generation 1 docs and get them\n\t    // leaving the non-generation one docs to be got otherwise\n\t    var ids = Object.keys(diffs).filter(function (id) {\n\t      var missing = diffs[id].missing;\n\t      return missing.length === 1 && isGenOne$1(missing[0]);\n\t    });\n\t    if (ids.length > 0) {\n\t      return fetchRevisionOneDocs(ids);\n\t    }\n\t  }\n\n\t  function returnResult() {\n\t    return { ok:ok, docs:resultDocs };\n\t  }\n\n\t  return Promise.resolve()\n\t    .then(getRevisionOneDocs)\n\t    .then(getAllDocs)\n\t    .then(returnResult);\n\t}\n\n\tvar CHECKPOINT_VERSION = 1;\n\tvar REPLICATOR = \"pouchdb\";\n\t// This is an arbitrary number to limit the\n\t// amount of replication history we save in the checkpoint.\n\t// If we save too much, the checkpoing docs will become very big,\n\t// if we save fewer, we'll run a greater risk of having to\n\t// read all the changes from 0 when checkpoint PUTs fail\n\t// CouchDB 2.0 has a more involved history pruning,\n\t// but let's go for the simple version for now.\n\tvar CHECKPOINT_HISTORY_SIZE = 5;\n\tvar LOWEST_SEQ = 0;\n\n\tfunction updateCheckpoint(db, id, checkpoint, session, returnValue) {\n\t  return db.get(id).catch(function (err) {\n\t    if (err.status === 404) {\n\t      if (db.adapter === 'http' || db.adapter === 'https') {\n\t        explainError(\n\t          404, 'PouchDB is just checking if a remote checkpoint exists.'\n\t        );\n\t      }\n\t      return {\n\t        session_id: session,\n\t        _id: id,\n\t        history: [],\n\t        replicator: REPLICATOR,\n\t        version: CHECKPOINT_VERSION\n\t      };\n\t    }\n\t    throw err;\n\t  }).then(function (doc) {\n\t    if (returnValue.cancelled) {\n\t      return;\n\t    }\n\n\t    // if the checkpoint has not changed, do not update\n\t    if (doc.last_seq === checkpoint) {\n\t      return;\n\t    }\n\n\t    // Filter out current entry for this replication\n\t    doc.history = (doc.history || []).filter(function (item) {\n\t      return item.session_id !== session;\n\t    });\n\n\t    // Add the latest checkpoint to history\n\t    doc.history.unshift({\n\t      last_seq: checkpoint,\n\t      session_id: session\n\t    });\n\n\t    // Just take the last pieces in history, to\n\t    // avoid really big checkpoint docs.\n\t    // see comment on history size above\n\t    doc.history = doc.history.slice(0, CHECKPOINT_HISTORY_SIZE);\n\n\t    doc.version = CHECKPOINT_VERSION;\n\t    doc.replicator = REPLICATOR;\n\n\t    doc.session_id = session;\n\t    doc.last_seq = checkpoint;\n\n\t    return db.put(doc).catch(function (err) {\n\t      if (err.status === 409) {\n\t        // retry; someone is trying to write a checkpoint simultaneously\n\t        return updateCheckpoint(db, id, checkpoint, session, returnValue);\n\t      }\n\t      throw err;\n\t    });\n\t  });\n\t}\n\n\tfunction Checkpointer(src, target, id, returnValue, opts) {\n\t  this.src = src;\n\t  this.target = target;\n\t  this.id = id;\n\t  this.returnValue = returnValue;\n\t  this.opts = opts || {};\n\t}\n\n\tCheckpointer.prototype.writeCheckpoint = function (checkpoint, session) {\n\t  var self = this;\n\t  return this.updateTarget(checkpoint, session).then(function () {\n\t    return self.updateSource(checkpoint, session);\n\t  });\n\t};\n\n\tCheckpointer.prototype.updateTarget = function (checkpoint, session) {\n\t  if (this.opts.writeTargetCheckpoint) {\n\t    return updateCheckpoint(this.target, this.id, checkpoint,\n\t      session, this.returnValue);\n\t  } else {\n\t    return Promise.resolve(true);\n\t  }\n\t};\n\n\tCheckpointer.prototype.updateSource = function (checkpoint, session) {\n\t  if (this.opts.writeSourceCheckpoint) {\n\t    var self = this;\n\t    return updateCheckpoint(this.src, this.id, checkpoint,\n\t      session, this.returnValue)\n\t      .catch(function (err) {\n\t        if (isForbiddenError(err)) {\n\t          self.opts.writeSourceCheckpoint = false;\n\t          return true;\n\t        }\n\t        throw err;\n\t      });\n\t  } else {\n\t    return Promise.resolve(true);\n\t  }\n\t};\n\n\tvar comparisons = {\n\t  \"undefined\": function (targetDoc, sourceDoc) {\n\t    // This is the previous comparison function\n\t    if (collate(targetDoc.last_seq, sourceDoc.last_seq) === 0) {\n\t      return sourceDoc.last_seq;\n\t    }\n\t    /* istanbul ignore next */\n\t    return 0;\n\t  },\n\t  \"1\": function (targetDoc, sourceDoc) {\n\t    // This is the comparison function ported from CouchDB\n\t    return compareReplicationLogs(sourceDoc, targetDoc).last_seq;\n\t  }\n\t};\n\n\tCheckpointer.prototype.getCheckpoint = function () {\n\t  var self = this;\n\n\t  if (self.opts && self.opts.writeSourceCheckpoint && !self.opts.writeTargetCheckpoint) {\n\t    return self.src.get(self.id).then(function (sourceDoc) {\n\t      return sourceDoc.last_seq || LOWEST_SEQ;\n\t    }).catch(function (err) {\n\t      /* istanbul ignore if */\n\t      if (err.status !== 404) {\n\t        throw err;\n\t      }\n\t      return LOWEST_SEQ;\n\t    });\n\t  }\n\n\t  return self.target.get(self.id).then(function (targetDoc) {\n\t    if (self.opts && self.opts.writeTargetCheckpoint && !self.opts.writeSourceCheckpoint) {\n\t      return targetDoc.last_seq || LOWEST_SEQ;\n\t    }\n\n\t    return self.src.get(self.id).then(function (sourceDoc) {\n\t      // Since we can't migrate an old version doc to a new one\n\t      // (no session id), we just go with the lowest seq in this case\n\t      /* istanbul ignore if */\n\t      if (targetDoc.version !== sourceDoc.version) {\n\t        return LOWEST_SEQ;\n\t      }\n\n\t      var version;\n\t      if (targetDoc.version) {\n\t        version = targetDoc.version.toString();\n\t      } else {\n\t        version = \"undefined\";\n\t      }\n\n\t      if (version in comparisons) {\n\t        return comparisons[version](targetDoc, sourceDoc);\n\t      }\n\t      /* istanbul ignore next */\n\t      return LOWEST_SEQ;\n\t    }, function (err) {\n\t      if (err.status === 404 && targetDoc.last_seq) {\n\t        return self.src.put({\n\t          _id: self.id,\n\t          last_seq: LOWEST_SEQ\n\t        }).then(function () {\n\t          return LOWEST_SEQ;\n\t        }, function (err) {\n\t          if (isForbiddenError(err)) {\n\t            self.opts.writeSourceCheckpoint = false;\n\t            return targetDoc.last_seq;\n\t          }\n\t          /* istanbul ignore next */\n\t          return LOWEST_SEQ;\n\t        });\n\t      }\n\t      throw err;\n\t    });\n\t  }).catch(function (err) {\n\t    if (err.status !== 404) {\n\t      throw err;\n\t    }\n\t    return LOWEST_SEQ;\n\t  });\n\t};\n\t// This checkpoint comparison is ported from CouchDBs source\n\t// they come from here:\n\t// https://github.com/apache/couchdb-couch-replicator/blob/master/src/couch_replicator.erl#L863-L906\n\n\tfunction compareReplicationLogs(srcDoc, tgtDoc) {\n\t  if (srcDoc.session_id === tgtDoc.session_id) {\n\t    return {\n\t      last_seq: srcDoc.last_seq,\n\t      history: srcDoc.history\n\t    };\n\t  }\n\n\t  return compareReplicationHistory(srcDoc.history, tgtDoc.history);\n\t}\n\n\tfunction compareReplicationHistory(sourceHistory, targetHistory) {\n\t  // the erlang loop via function arguments is not so easy to repeat in JS\n\t  // therefore, doing this as recursion\n\t  var S = sourceHistory[0];\n\t  var sourceRest = sourceHistory.slice(1);\n\t  var T = targetHistory[0];\n\t  var targetRest = targetHistory.slice(1);\n\n\t  if (!S || targetHistory.length === 0) {\n\t    return {\n\t      last_seq: LOWEST_SEQ,\n\t      history: []\n\t    };\n\t  }\n\n\t  var sourceId = S.session_id;\n\t  /* istanbul ignore if */\n\t  if (hasSessionId(sourceId, targetHistory)) {\n\t    return {\n\t      last_seq: S.last_seq,\n\t      history: sourceHistory\n\t    };\n\t  }\n\n\t  var targetId = T.session_id;\n\t  if (hasSessionId(targetId, sourceRest)) {\n\t    return {\n\t      last_seq: T.last_seq,\n\t      history: targetRest\n\t    };\n\t  }\n\n\t  return compareReplicationHistory(sourceRest, targetRest);\n\t}\n\n\tfunction hasSessionId(sessionId, history) {\n\t  var props = history[0];\n\t  var rest = history.slice(1);\n\n\t  if (!sessionId || history.length === 0) {\n\t    return false;\n\t  }\n\n\t  if (sessionId === props.session_id) {\n\t    return true;\n\t  }\n\n\t  return hasSessionId(sessionId, rest);\n\t}\n\n\tfunction isForbiddenError(err) {\n\t  return typeof err.status === 'number' && Math.floor(err.status / 100) === 4;\n\t}\n\n\tvar STARTING_BACK_OFF = 0;\n\n\tfunction backOff(opts, returnValue, error, callback) {\n\t  if (opts.retry === false) {\n\t    returnValue.emit('error', error);\n\t    returnValue.removeAllListeners();\n\t    return;\n\t  }\n\t  /* istanbul ignore if */\n\t  if (typeof opts.back_off_function !== 'function') {\n\t    opts.back_off_function = defaultBackOff;\n\t  }\n\t  returnValue.emit('requestError', error);\n\t  if (returnValue.state === 'active' || returnValue.state === 'pending') {\n\t    returnValue.emit('paused', error);\n\t    returnValue.state = 'stopped';\n\t    var backOffSet = function backoffTimeSet() {\n\t      opts.current_back_off = STARTING_BACK_OFF;\n\t    };\n\t    var removeBackOffSetter = function removeBackOffTimeSet() {\n\t      returnValue.removeListener('active', backOffSet);\n\t    };\n\t    returnValue.once('paused', removeBackOffSetter);\n\t    returnValue.once('active', backOffSet);\n\t  }\n\n\t  opts.current_back_off = opts.current_back_off || STARTING_BACK_OFF;\n\t  opts.current_back_off = opts.back_off_function(opts.current_back_off);\n\t  setTimeout(callback, opts.current_back_off);\n\t}\n\n\tfunction sortObjectPropertiesByKey(queryParams) {\n\t  return Object.keys(queryParams).sort(collate).reduce(function (result, key) {\n\t    result[key] = queryParams[key];\n\t    return result;\n\t  }, {});\n\t}\n\n\t// Generate a unique id particular to this replication.\n\t// Not guaranteed to align perfectly with CouchDB's rep ids.\n\tfunction generateReplicationId(src, target, opts) {\n\t  var docIds = opts.doc_ids ? opts.doc_ids.sort(collate) : '';\n\t  var filterFun = opts.filter ? opts.filter.toString() : '';\n\t  var queryParams = '';\n\t  var filterViewName =  '';\n\t  var selector = '';\n\n\t  // possibility for checkpoints to be lost here as behaviour of\n\t  // JSON.stringify is not stable (see #6226)\n\t  /* istanbul ignore if */\n\t  if (opts.selector) {\n\t    selector = JSON.stringify(opts.selector);\n\t  }\n\n\t  if (opts.filter && opts.query_params) {\n\t    queryParams = JSON.stringify(sortObjectPropertiesByKey(opts.query_params));\n\t  }\n\n\t  if (opts.filter && opts.filter === '_view') {\n\t    filterViewName = opts.view.toString();\n\t  }\n\n\t  return Promise.all([src.id(), target.id()]).then(function (res) {\n\t    var queryData = res[0] + res[1] + filterFun + filterViewName +\n\t      queryParams + docIds + selector;\n\t    return new Promise(function (resolve) {\n\t      binaryMd5(queryData, resolve);\n\t    });\n\t  }).then(function (md5sum) {\n\t    // can't use straight-up md5 alphabet, because\n\t    // the char '/' is interpreted as being for attachments,\n\t    // and + is also not url-safe\n\t    md5sum = md5sum.replace(/\\//g, '.').replace(/\\+/g, '_');\n\t    return '_local/' + md5sum;\n\t  });\n\t}\n\n\tfunction replicate(src, target, opts, returnValue, result) {\n\t  var batches = [];               // list of batches to be processed\n\t  var currentBatch;               // the batch currently being processed\n\t  var pendingBatch = {\n\t    seq: 0,\n\t    changes: [],\n\t    docs: []\n\t  }; // next batch, not yet ready to be processed\n\t  var writingCheckpoint = false;  // true while checkpoint is being written\n\t  var changesCompleted = false;   // true when all changes received\n\t  var replicationCompleted = false; // true when replication has completed\n\t  var last_seq = 0;\n\t  var continuous = opts.continuous || opts.live || false;\n\t  var batch_size = opts.batch_size || 100;\n\t  var batches_limit = opts.batches_limit || 10;\n\t  var changesPending = false;     // true while src.changes is running\n\t  var doc_ids = opts.doc_ids;\n\t  var selector = opts.selector;\n\t  var repId;\n\t  var checkpointer;\n\t  var changedDocs = [];\n\t  // Like couchdb, every replication gets a unique session id\n\t  var session = uuid$1();\n\n\t  result = result || {\n\t    ok: true,\n\t    start_time: new Date().toISOString(),\n\t    docs_read: 0,\n\t    docs_written: 0,\n\t    doc_write_failures: 0,\n\t    errors: []\n\t  };\n\n\t  var changesOpts = {};\n\t  returnValue.ready(src, target);\n\n\t  function initCheckpointer() {\n\t    if (checkpointer) {\n\t      return Promise.resolve();\n\t    }\n\t    return generateReplicationId(src, target, opts).then(function (res) {\n\t      repId = res;\n\n\t      var checkpointOpts = {};\n\t      if (opts.checkpoint === false) {\n\t        checkpointOpts = { writeSourceCheckpoint: false, writeTargetCheckpoint: false };\n\t      } else if (opts.checkpoint === 'source') {\n\t        checkpointOpts = { writeSourceCheckpoint: true, writeTargetCheckpoint: false };\n\t      } else if (opts.checkpoint === 'target') {\n\t        checkpointOpts = { writeSourceCheckpoint: false, writeTargetCheckpoint: true };\n\t      } else {\n\t        checkpointOpts = { writeSourceCheckpoint: true, writeTargetCheckpoint: true };\n\t      }\n\n\t      checkpointer = new Checkpointer(src, target, repId, returnValue, checkpointOpts);\n\t    });\n\t  }\n\n\t  function writeDocs() {\n\t    changedDocs = [];\n\n\t    if (currentBatch.docs.length === 0) {\n\t      return;\n\t    }\n\t    var docs = currentBatch.docs;\n\t    var bulkOpts = {timeout: opts.timeout};\n\t    return target.bulkDocs({docs: docs, new_edits: false}, bulkOpts).then(function (res) {\n\t      /* istanbul ignore if */\n\t      if (returnValue.cancelled) {\n\t        completeReplication();\n\t        throw new Error('cancelled');\n\t      }\n\n\t      // `res` doesn't include full documents (which live in `docs`), so we create a map of \n\t      // (id -> error), and check for errors while iterating over `docs`\n\t      var errorsById = Object.create(null);\n\t      res.forEach(function (res) {\n\t        if (res.error) {\n\t          errorsById[res.id] = res;\n\t        }\n\t      });\n\n\t      var errorsNo = Object.keys(errorsById).length;\n\t      result.doc_write_failures += errorsNo;\n\t      result.docs_written += docs.length - errorsNo;\n\n\t      docs.forEach(function (doc) {\n\t        var error = errorsById[doc._id];\n\t        if (error) {\n\t          result.errors.push(error);\n\t          // Normalize error name. i.e. 'Unauthorized' -> 'unauthorized' (eg Sync Gateway)\n\t          var errorName = (error.name || '').toLowerCase();\n\t          if (errorName === 'unauthorized' || errorName === 'forbidden') {\n\t            returnValue.emit('denied', clone(error));\n\t          } else {\n\t            throw error;\n\t          }\n\t        } else {\n\t          changedDocs.push(doc);\n\t        }\n\t      });\n\n\t    }, function (err) {\n\t      result.doc_write_failures += docs.length;\n\t      throw err;\n\t    });\n\t  }\n\n\t  function finishBatch() {\n\t    if (currentBatch.error) {\n\t      throw new Error('There was a problem getting docs.');\n\t    }\n\t    result.last_seq = last_seq = currentBatch.seq;\n\t    var outResult = clone(result);\n\t    if (changedDocs.length) {\n\t      outResult.docs = changedDocs;\n\t      // Attach 'pending' property if server supports it (CouchDB 2.0+)\n\t      /* istanbul ignore if */\n\t      if (typeof currentBatch.pending === 'number') {\n\t        outResult.pending = currentBatch.pending;\n\t        delete currentBatch.pending;\n\t      }\n\t      returnValue.emit('change', outResult);\n\t    }\n\t    writingCheckpoint = true;\n\t    return checkpointer.writeCheckpoint(currentBatch.seq,\n\t        session).then(function () {\n\t      writingCheckpoint = false;\n\t      /* istanbul ignore if */\n\t      if (returnValue.cancelled) {\n\t        completeReplication();\n\t        throw new Error('cancelled');\n\t      }\n\t      currentBatch = undefined;\n\t      getChanges();\n\t    }).catch(function (err) {\n\t      onCheckpointError(err);\n\t      throw err;\n\t    });\n\t  }\n\n\t  function getDiffs() {\n\t    var diff = {};\n\t    currentBatch.changes.forEach(function (change) {\n\t      // Couchbase Sync Gateway emits these, but we can ignore them\n\t      /* istanbul ignore if */\n\t      if (change.id === \"_user/\") {\n\t        return;\n\t      }\n\t      diff[change.id] = change.changes.map(function (x) {\n\t        return x.rev;\n\t      });\n\t    });\n\t    return target.revsDiff(diff).then(function (diffs) {\n\t      /* istanbul ignore if */\n\t      if (returnValue.cancelled) {\n\t        completeReplication();\n\t        throw new Error('cancelled');\n\t      }\n\t      // currentBatch.diffs elements are deleted as the documents are written\n\t      currentBatch.diffs = diffs;\n\t    });\n\t  }\n\n\t  function getBatchDocs() {\n\t    return getDocs(src, target, currentBatch.diffs, returnValue).then(function (got) {\n\t      currentBatch.error = !got.ok;\n\t      got.docs.forEach(function (doc) {\n\t        delete currentBatch.diffs[doc._id];\n\t        result.docs_read++;\n\t        currentBatch.docs.push(doc);\n\t      });\n\t    });\n\t  }\n\n\t  function startNextBatch() {\n\t    if (returnValue.cancelled || currentBatch) {\n\t      return;\n\t    }\n\t    if (batches.length === 0) {\n\t      processPendingBatch(true);\n\t      return;\n\t    }\n\t    currentBatch = batches.shift();\n\t    getDiffs()\n\t      .then(getBatchDocs)\n\t      .then(writeDocs)\n\t      .then(finishBatch)\n\t      .then(startNextBatch)\n\t      .catch(function (err) {\n\t        abortReplication('batch processing terminated with error', err);\n\t      });\n\t  }\n\n\n\t  function processPendingBatch(immediate$$1) {\n\t    if (pendingBatch.changes.length === 0) {\n\t      if (batches.length === 0 && !currentBatch) {\n\t        if ((continuous && changesOpts.live) || changesCompleted) {\n\t          returnValue.state = 'pending';\n\t          returnValue.emit('paused');\n\t        }\n\t        if (changesCompleted) {\n\t          completeReplication();\n\t        }\n\t      }\n\t      return;\n\t    }\n\t    if (\n\t      immediate$$1 ||\n\t      changesCompleted ||\n\t      pendingBatch.changes.length >= batch_size\n\t    ) {\n\t      batches.push(pendingBatch);\n\t      pendingBatch = {\n\t        seq: 0,\n\t        changes: [],\n\t        docs: []\n\t      };\n\t      if (returnValue.state === 'pending' || returnValue.state === 'stopped') {\n\t        returnValue.state = 'active';\n\t        returnValue.emit('active');\n\t      }\n\t      startNextBatch();\n\t    }\n\t  }\n\n\n\t  function abortReplication(reason, err) {\n\t    if (replicationCompleted) {\n\t      return;\n\t    }\n\t    if (!err.message) {\n\t      err.message = reason;\n\t    }\n\t    result.ok = false;\n\t    result.status = 'aborting';\n\t    batches = [];\n\t    pendingBatch = {\n\t      seq: 0,\n\t      changes: [],\n\t      docs: []\n\t    };\n\t    completeReplication(err);\n\t  }\n\n\n\t  function completeReplication(fatalError) {\n\t    if (replicationCompleted) {\n\t      return;\n\t    }\n\t    /* istanbul ignore if */\n\t    if (returnValue.cancelled) {\n\t      result.status = 'cancelled';\n\t      if (writingCheckpoint) {\n\t        return;\n\t      }\n\t    }\n\t    result.status = result.status || 'complete';\n\t    result.end_time = new Date().toISOString();\n\t    result.last_seq = last_seq;\n\t    replicationCompleted = true;\n\n\t    if (fatalError) {\n\t      // need to extend the error because Firefox considers \".result\" read-only\n\t      fatalError = createError(fatalError);\n\t      fatalError.result = result;\n\n\t      // Normalize error name. i.e. 'Unauthorized' -> 'unauthorized' (eg Sync Gateway)\n\t      var errorName = (fatalError.name || '').toLowerCase();\n\t      if (errorName === 'unauthorized' || errorName === 'forbidden') {\n\t        returnValue.emit('error', fatalError);\n\t        returnValue.removeAllListeners();\n\t      } else {\n\t        backOff(opts, returnValue, fatalError, function () {\n\t          replicate(src, target, opts, returnValue);\n\t        });\n\t      }\n\t    } else {\n\t      returnValue.emit('complete', result);\n\t      returnValue.removeAllListeners();\n\t    }\n\t  }\n\n\n\t  function onChange(change, pending, lastSeq) {\n\t    /* istanbul ignore if */\n\t    if (returnValue.cancelled) {\n\t      return completeReplication();\n\t    }\n\t    // Attach 'pending' property if server supports it (CouchDB 2.0+)\n\t    /* istanbul ignore if */\n\t    if (typeof pending === 'number') {\n\t      pendingBatch.pending = pending;\n\t    }\n\n\t    var filter = filterChange(opts)(change);\n\t    if (!filter) {\n\t      return;\n\t    }\n\t    pendingBatch.seq = change.seq || lastSeq;\n\t    pendingBatch.changes.push(change);\n\t    lib(function () {\n\t      processPendingBatch(batches.length === 0 && changesOpts.live);\n\t    });\n\t  }\n\n\n\t  function onChangesComplete(changes) {\n\t    changesPending = false;\n\t    /* istanbul ignore if */\n\t    if (returnValue.cancelled) {\n\t      return completeReplication();\n\t    }\n\n\t    // if no results were returned then we're done,\n\t    // else fetch more\n\t    if (changes.results.length > 0) {\n\t      changesOpts.since = changes.results[changes.results.length - 1].seq;\n\t      getChanges();\n\t      processPendingBatch(true);\n\t    } else {\n\n\t      var complete = function () {\n\t        if (continuous) {\n\t          changesOpts.live = true;\n\t          getChanges();\n\t        } else {\n\t          changesCompleted = true;\n\t        }\n\t        processPendingBatch(true);\n\t      };\n\n\t      // update the checkpoint so we start from the right seq next time\n\t      if (!currentBatch && changes.results.length === 0) {\n\t        writingCheckpoint = true;\n\t        checkpointer.writeCheckpoint(changes.last_seq,\n\t            session).then(function () {\n\t          writingCheckpoint = false;\n\t          result.last_seq = last_seq = changes.last_seq;\n\t          complete();\n\t        })\n\t        .catch(onCheckpointError);\n\t      } else {\n\t        complete();\n\t      }\n\t    }\n\t  }\n\n\n\t  function onChangesError(err) {\n\t    changesPending = false;\n\t    /* istanbul ignore if */\n\t    if (returnValue.cancelled) {\n\t      return completeReplication();\n\t    }\n\t    abortReplication('changes rejected', err);\n\t  }\n\n\n\t  function getChanges() {\n\t    if (!(\n\t      !changesPending &&\n\t      !changesCompleted &&\n\t      batches.length < batches_limit\n\t      )) {\n\t      return;\n\t    }\n\t    changesPending = true;\n\t    function abortChanges() {\n\t      changes.cancel();\n\t    }\n\t    function removeListener() {\n\t      returnValue.removeListener('cancel', abortChanges);\n\t    }\n\n\t    if (returnValue._changes) { // remove old changes() and listeners\n\t      returnValue.removeListener('cancel', returnValue._abortChanges);\n\t      returnValue._changes.cancel();\n\t    }\n\t    returnValue.once('cancel', abortChanges);\n\n\t    var changes = src.changes(changesOpts)\n\t      .on('change', onChange);\n\t    changes.then(removeListener, removeListener);\n\t    changes.then(onChangesComplete)\n\t      .catch(onChangesError);\n\n\t    if (opts.retry) {\n\t      // save for later so we can cancel if necessary\n\t      returnValue._changes = changes;\n\t      returnValue._abortChanges = abortChanges;\n\t    }\n\t  }\n\n\n\t  function startChanges() {\n\t    initCheckpointer().then(function () {\n\t      /* istanbul ignore if */\n\t      if (returnValue.cancelled) {\n\t        completeReplication();\n\t        return;\n\t      }\n\t      return checkpointer.getCheckpoint().then(function (checkpoint) {\n\t        last_seq = checkpoint;\n\t        changesOpts = {\n\t          since: last_seq,\n\t          limit: batch_size,\n\t          batch_size: batch_size,\n\t          style: 'all_docs',\n\t          doc_ids: doc_ids,\n\t          selector: selector,\n\t          return_docs: true // required so we know when we're done\n\t        };\n\t        if (opts.filter) {\n\t          if (typeof opts.filter !== 'string') {\n\t            // required for the client-side filter in onChange\n\t            changesOpts.include_docs = true;\n\t          } else { // ddoc filter\n\t            changesOpts.filter = opts.filter;\n\t          }\n\t        }\n\t        if ('heartbeat' in opts) {\n\t          changesOpts.heartbeat = opts.heartbeat;\n\t        }\n\t        if ('timeout' in opts) {\n\t          changesOpts.timeout = opts.timeout;\n\t        }\n\t        if (opts.query_params) {\n\t          changesOpts.query_params = opts.query_params;\n\t        }\n\t        if (opts.view) {\n\t          changesOpts.view = opts.view;\n\t        }\n\t        getChanges();\n\t      });\n\t    }).catch(function (err) {\n\t      abortReplication('getCheckpoint rejected with ', err);\n\t    });\n\t  }\n\n\t  /* istanbul ignore next */\n\t  function onCheckpointError(err) {\n\t    writingCheckpoint = false;\n\t    abortReplication('writeCheckpoint completed with error', err);\n\t  }\n\n\t  /* istanbul ignore if */\n\t  if (returnValue.cancelled) { // cancelled immediately\n\t    completeReplication();\n\t    return;\n\t  }\n\n\t  if (!returnValue._addedListeners) {\n\t    returnValue.once('cancel', completeReplication);\n\n\t    if (typeof opts.complete === 'function') {\n\t      returnValue.once('error', opts.complete);\n\t      returnValue.once('complete', function (result) {\n\t        opts.complete(null, result);\n\t      });\n\t    }\n\t    returnValue._addedListeners = true;\n\t  }\n\n\t  if (typeof opts.since === 'undefined') {\n\t    startChanges();\n\t  } else {\n\t    initCheckpointer().then(function () {\n\t      writingCheckpoint = true;\n\t      return checkpointer.writeCheckpoint(opts.since, session);\n\t    }).then(function () {\n\t      writingCheckpoint = false;\n\t      /* istanbul ignore if */\n\t      if (returnValue.cancelled) {\n\t        completeReplication();\n\t        return;\n\t      }\n\t      last_seq = opts.since;\n\t      startChanges();\n\t    }).catch(onCheckpointError);\n\t  }\n\t}\n\n\t// We create a basic promise so the caller can cancel the replication possibly\n\t// before we have actually started listening to changes etc\n\tinherits$2(Replication, EventEmitter);\n\tfunction Replication() {\n\t  EventEmitter.call(this);\n\t  this.cancelled = false;\n\t  this.state = 'pending';\n\t  var self = this;\n\t  var promise = new Promise(function (fulfill, reject) {\n\t    self.once('complete', fulfill);\n\t    self.once('error', reject);\n\t  });\n\t  self.then = function (resolve, reject) {\n\t    return promise.then(resolve, reject);\n\t  };\n\t  self.catch = function (reject) {\n\t    return promise.catch(reject);\n\t  };\n\t  // As we allow error handling via \"error\" event as well,\n\t  // put a stub in here so that rejecting never throws UnhandledError.\n\t  self.catch(function () {});\n\t}\n\n\tReplication.prototype.cancel = function () {\n\t  this.cancelled = true;\n\t  this.state = 'cancelled';\n\t  this.emit('cancel');\n\t};\n\n\tReplication.prototype.ready = function (src, target) {\n\t  var self = this;\n\t  if (self._readyCalled) {\n\t    return;\n\t  }\n\t  self._readyCalled = true;\n\n\t  function onDestroy() {\n\t    self.cancel();\n\t  }\n\t  src.once('destroyed', onDestroy);\n\t  target.once('destroyed', onDestroy);\n\t  function cleanup() {\n\t    src.removeListener('destroyed', onDestroy);\n\t    target.removeListener('destroyed', onDestroy);\n\t  }\n\t  self.once('complete', cleanup);\n\t};\n\n\tfunction toPouch(db, opts) {\n\t  var PouchConstructor = opts.PouchConstructor;\n\t  if (typeof db === 'string') {\n\t    return new PouchConstructor(db, opts);\n\t  } else {\n\t    return db;\n\t  }\n\t}\n\n\tfunction replicateWrapper(src, target, opts, callback) {\n\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  if (typeof opts === 'undefined') {\n\t    opts = {};\n\t  }\n\n\t  if (opts.doc_ids && !Array.isArray(opts.doc_ids)) {\n\t    throw createError(BAD_REQUEST,\n\t                       \"`doc_ids` filter parameter is not a list.\");\n\t  }\n\n\t  opts.complete = callback;\n\t  opts = clone(opts);\n\t  opts.continuous = opts.continuous || opts.live;\n\t  opts.retry = ('retry' in opts) ? opts.retry : false;\n\t  /*jshint validthis:true */\n\t  opts.PouchConstructor = opts.PouchConstructor || this;\n\t  var replicateRet = new Replication(opts);\n\t  var srcPouch = toPouch(src, opts);\n\t  var targetPouch = toPouch(target, opts);\n\t  replicate(srcPouch, targetPouch, opts, replicateRet);\n\t  return replicateRet;\n\t}\n\n\tinherits$2(Sync, EventEmitter);\n\tfunction sync(src, target, opts, callback) {\n\t  if (typeof opts === 'function') {\n\t    callback = opts;\n\t    opts = {};\n\t  }\n\t  if (typeof opts === 'undefined') {\n\t    opts = {};\n\t  }\n\t  opts = clone(opts);\n\t  /*jshint validthis:true */\n\t  opts.PouchConstructor = opts.PouchConstructor || this;\n\t  src = toPouch(src, opts);\n\t  target = toPouch(target, opts);\n\t  return new Sync(src, target, opts, callback);\n\t}\n\n\tfunction Sync(src, target, opts, callback) {\n\t  var self = this;\n\t  this.canceled = false;\n\n\t  var optsPush = opts.push ? $inject_Object_assign({}, opts, opts.push) : opts;\n\t  var optsPull = opts.pull ? $inject_Object_assign({}, opts, opts.pull) : opts;\n\n\t  this.push = replicateWrapper(src, target, optsPush);\n\t  this.pull = replicateWrapper(target, src, optsPull);\n\n\t  this.pushPaused = true;\n\t  this.pullPaused = true;\n\n\t  function pullChange(change) {\n\t    self.emit('change', {\n\t      direction: 'pull',\n\t      change: change\n\t    });\n\t  }\n\t  function pushChange(change) {\n\t    self.emit('change', {\n\t      direction: 'push',\n\t      change: change\n\t    });\n\t  }\n\t  function pushDenied(doc) {\n\t    self.emit('denied', {\n\t      direction: 'push',\n\t      doc: doc\n\t    });\n\t  }\n\t  function pullDenied(doc) {\n\t    self.emit('denied', {\n\t      direction: 'pull',\n\t      doc: doc\n\t    });\n\t  }\n\t  function pushPaused() {\n\t    self.pushPaused = true;\n\t    /* istanbul ignore if */\n\t    if (self.pullPaused) {\n\t      self.emit('paused');\n\t    }\n\t  }\n\t  function pullPaused() {\n\t    self.pullPaused = true;\n\t    /* istanbul ignore if */\n\t    if (self.pushPaused) {\n\t      self.emit('paused');\n\t    }\n\t  }\n\t  function pushActive() {\n\t    self.pushPaused = false;\n\t    /* istanbul ignore if */\n\t    if (self.pullPaused) {\n\t      self.emit('active', {\n\t        direction: 'push'\n\t      });\n\t    }\n\t  }\n\t  function pullActive() {\n\t    self.pullPaused = false;\n\t    /* istanbul ignore if */\n\t    if (self.pushPaused) {\n\t      self.emit('active', {\n\t        direction: 'pull'\n\t      });\n\t    }\n\t  }\n\n\t  var removed = {};\n\n\t  function removeAll(type) { // type is 'push' or 'pull'\n\t    return function (event, func) {\n\t      var isChange = event === 'change' &&\n\t        (func === pullChange || func === pushChange);\n\t      var isDenied = event === 'denied' &&\n\t        (func === pullDenied || func === pushDenied);\n\t      var isPaused = event === 'paused' &&\n\t        (func === pullPaused || func === pushPaused);\n\t      var isActive = event === 'active' &&\n\t        (func === pullActive || func === pushActive);\n\n\t      if (isChange || isDenied || isPaused || isActive) {\n\t        if (!(event in removed)) {\n\t          removed[event] = {};\n\t        }\n\t        removed[event][type] = true;\n\t        if (Object.keys(removed[event]).length === 2) {\n\t          // both push and pull have asked to be removed\n\t          self.removeAllListeners(event);\n\t        }\n\t      }\n\t    };\n\t  }\n\n\t  if (opts.live) {\n\t    this.push.on('complete', self.pull.cancel.bind(self.pull));\n\t    this.pull.on('complete', self.push.cancel.bind(self.push));\n\t  }\n\n\t  function addOneListener(ee, event, listener) {\n\t    if (ee.listeners(event).indexOf(listener) == -1) {\n\t      ee.on(event, listener);\n\t    }\n\t  }\n\n\t  this.on('newListener', function (event) {\n\t    if (event === 'change') {\n\t      addOneListener(self.pull, 'change', pullChange);\n\t      addOneListener(self.push, 'change', pushChange);\n\t    } else if (event === 'denied') {\n\t      addOneListener(self.pull, 'denied', pullDenied);\n\t      addOneListener(self.push, 'denied', pushDenied);\n\t    } else if (event === 'active') {\n\t      addOneListener(self.pull, 'active', pullActive);\n\t      addOneListener(self.push, 'active', pushActive);\n\t    } else if (event === 'paused') {\n\t      addOneListener(self.pull, 'paused', pullPaused);\n\t      addOneListener(self.push, 'paused', pushPaused);\n\t    }\n\t  });\n\n\t  this.on('removeListener', function (event) {\n\t    if (event === 'change') {\n\t      self.pull.removeListener('change', pullChange);\n\t      self.push.removeListener('change', pushChange);\n\t    } else if (event === 'denied') {\n\t      self.pull.removeListener('denied', pullDenied);\n\t      self.push.removeListener('denied', pushDenied);\n\t    } else if (event === 'active') {\n\t      self.pull.removeListener('active', pullActive);\n\t      self.push.removeListener('active', pushActive);\n\t    } else if (event === 'paused') {\n\t      self.pull.removeListener('paused', pullPaused);\n\t      self.push.removeListener('paused', pushPaused);\n\t    }\n\t  });\n\n\t  this.pull.on('removeListener', removeAll('pull'));\n\t  this.push.on('removeListener', removeAll('push'));\n\n\t  var promise = Promise.all([\n\t    this.push,\n\t    this.pull\n\t  ]).then(function (resp) {\n\t    var out = {\n\t      push: resp[0],\n\t      pull: resp[1]\n\t    };\n\t    self.emit('complete', out);\n\t    if (callback) {\n\t      callback(null, out);\n\t    }\n\t    self.removeAllListeners();\n\t    return out;\n\t  }, function (err) {\n\t    self.cancel();\n\t    if (callback) {\n\t      // if there's a callback, then the callback can receive\n\t      // the error event\n\t      callback(err);\n\t    } else {\n\t      // if there's no callback, then we're safe to emit an error\n\t      // event, which would otherwise throw an unhandled error\n\t      // due to 'error' being a special event in EventEmitters\n\t      self.emit('error', err);\n\t    }\n\t    self.removeAllListeners();\n\t    if (callback) {\n\t      // no sense throwing if we're already emitting an 'error' event\n\t      throw err;\n\t    }\n\t  });\n\n\t  this.then = function (success, err) {\n\t    return promise.then(success, err);\n\t  };\n\n\t  this.catch = function (err) {\n\t    return promise.catch(err);\n\t  };\n\t}\n\n\tSync.prototype.cancel = function () {\n\t  if (!this.canceled) {\n\t    this.canceled = true;\n\t    this.push.cancel();\n\t    this.pull.cancel();\n\t  }\n\t};\n\n\tfunction replication(PouchDB) {\n\t  PouchDB.replicate = replicateWrapper;\n\t  PouchDB.sync = sync;\n\n\t  Object.defineProperty(PouchDB.prototype, 'replicate', {\n\t    get: function () {\n\t      var self = this;\n\t      if (typeof this.replicateMethods === 'undefined') {\n\t        this.replicateMethods = {\n\t          from: function (other, opts, callback) {\n\t            return self.constructor.replicate(other, self, opts, callback);\n\t          },\n\t          to: function (other, opts, callback) {\n\t            return self.constructor.replicate(self, other, opts, callback);\n\t          }\n\t        };\n\t      }\n\t      return this.replicateMethods;\n\t    }\n\t  });\n\n\t  PouchDB.prototype.sync = function (dbName, opts, callback) {\n\t    return this.constructor.sync(this, dbName, opts, callback);\n\t  };\n\t}\n\n\tPouchDB.plugin(IDBPouch)\n\t  .plugin(HttpPouch$1)\n\t  .plugin(mapreduce)\n\t  .plugin(replication);\n\n\t/**\r\n\t * Copyright 2019 Google Inc. All Rights Reserved.\r\n\t * Licensed under the Apache License, Version 2.0 (the \"License\");\r\n\t * you may not use this file except in compliance with the License.\r\n\t * You may obtain a copy of the License at\r\n\t *     http://www.apache.org/licenses/LICENSE-2.0\r\n\t * Unless required by applicable law or agreed to in writing, software\r\n\t * distributed under the License is distributed on an \"AS IS\" BASIS,\r\n\t * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\r\n\t * See the License for the specific language governing permissions and\r\n\t * limitations under the License.\r\n\t */\r\n\tconst proxyMarker = Symbol(\"Comlink.proxy\");\r\n\tconst createEndpoint = Symbol(\"Comlink.endpoint\");\r\n\tconst releaseProxy = Symbol(\"Comlink.releaseProxy\");\r\n\tconst throwMarker = Symbol(\"Comlink.thrown\");\r\n\tconst isObject$1 = (val) => (typeof val === \"object\" && val !== null) || typeof val === \"function\";\r\n\t/**\r\n\t * Internal transfer handle to handle objects marked to proxy.\r\n\t */\r\n\tconst proxyTransferHandler = {\r\n\t    canHandle: (val) => isObject$1(val) && val[proxyMarker],\r\n\t    serialize(obj) {\r\n\t        const { port1, port2 } = new MessageChannel();\r\n\t        expose(obj, port1);\r\n\t        return [port2, [port2]];\r\n\t    },\r\n\t    deserialize(port) {\r\n\t        port.start();\r\n\t        return wrap(port);\r\n\t    },\r\n\t};\r\n\t/**\r\n\t * Internal transfer handler to handle thrown exceptions.\r\n\t */\r\n\tconst throwTransferHandler = {\r\n\t    canHandle: (value) => isObject$1(value) && throwMarker in value,\r\n\t    serialize({ value }) {\r\n\t        let serialized;\r\n\t        if (value instanceof Error) {\r\n\t            serialized = {\r\n\t                isError: true,\r\n\t                value: {\r\n\t                    message: value.message,\r\n\t                    name: value.name,\r\n\t                    stack: value.stack,\r\n\t                },\r\n\t            };\r\n\t        }\r\n\t        else {\r\n\t            serialized = { isError: false, value };\r\n\t        }\r\n\t        return [serialized, []];\r\n\t    },\r\n\t    deserialize(serialized) {\r\n\t        if (serialized.isError) {\r\n\t            throw Object.assign(new Error(serialized.value.message), serialized.value);\r\n\t        }\r\n\t        throw serialized.value;\r\n\t    },\r\n\t};\r\n\t/**\r\n\t * Allows customizing the serialization of certain values.\r\n\t */\r\n\tconst transferHandlers = new Map([\r\n\t    [\"proxy\", proxyTransferHandler],\r\n\t    [\"throw\", throwTransferHandler],\r\n\t]);\r\n\tfunction expose(obj, ep = self) {\r\n\t    ep.addEventListener(\"message\", function callback(ev) {\r\n\t        if (!ev || !ev.data) {\r\n\t            return;\r\n\t        }\r\n\t        const { id, type, path } = Object.assign({ path: [] }, ev.data);\r\n\t        const argumentList = (ev.data.argumentList || []).map(fromWireValue);\r\n\t        let returnValue;\r\n\t        try {\r\n\t            const parent = path.slice(0, -1).reduce((obj, prop) => obj[prop], obj);\r\n\t            const rawValue = path.reduce((obj, prop) => obj[prop], obj);\r\n\t            switch (type) {\r\n\t                case 0 /* GET */:\r\n\t                    {\r\n\t                        returnValue = rawValue;\r\n\t                    }\r\n\t                    break;\r\n\t                case 1 /* SET */:\r\n\t                    {\r\n\t                        parent[path.slice(-1)[0]] = fromWireValue(ev.data.value);\r\n\t                        returnValue = true;\r\n\t                    }\r\n\t                    break;\r\n\t                case 2 /* APPLY */:\r\n\t                    {\r\n\t                        returnValue = rawValue.apply(parent, argumentList);\r\n\t                    }\r\n\t                    break;\r\n\t                case 3 /* CONSTRUCT */:\r\n\t                    {\r\n\t                        const value = new rawValue(...argumentList);\r\n\t                        returnValue = proxy(value);\r\n\t                    }\r\n\t                    break;\r\n\t                case 4 /* ENDPOINT */:\r\n\t                    {\r\n\t                        const { port1, port2 } = new MessageChannel();\r\n\t                        expose(obj, port2);\r\n\t                        returnValue = transfer(port1, [port1]);\r\n\t                    }\r\n\t                    break;\r\n\t                case 5 /* RELEASE */:\r\n\t                    {\r\n\t                        returnValue = undefined;\r\n\t                    }\r\n\t                    break;\r\n\t            }\r\n\t        }\r\n\t        catch (value) {\r\n\t            returnValue = { value, [throwMarker]: 0 };\r\n\t        }\r\n\t        Promise.resolve(returnValue)\r\n\t            .catch((value) => {\r\n\t            return { value, [throwMarker]: 0 };\r\n\t        })\r\n\t            .then((returnValue) => {\r\n\t            const [wireValue, transferables] = toWireValue(returnValue);\r\n\t            ep.postMessage(Object.assign(Object.assign({}, wireValue), { id }), transferables);\r\n\t            if (type === 5 /* RELEASE */) {\r\n\t                // detach and deactive after sending release response above.\r\n\t                ep.removeEventListener(\"message\", callback);\r\n\t                closeEndPoint(ep);\r\n\t            }\r\n\t        });\r\n\t    });\r\n\t    if (ep.start) {\r\n\t        ep.start();\r\n\t    }\r\n\t}\r\n\tfunction isMessagePort(endpoint) {\r\n\t    return endpoint.constructor.name === \"MessagePort\";\r\n\t}\r\n\tfunction closeEndPoint(endpoint) {\r\n\t    if (isMessagePort(endpoint))\r\n\t        endpoint.close();\r\n\t}\r\n\tfunction wrap(ep, target) {\r\n\t    return createProxy(ep, [], target);\r\n\t}\r\n\tfunction throwIfProxyReleased(isReleased) {\r\n\t    if (isReleased) {\r\n\t        throw new Error(\"Proxy has been released and is not useable\");\r\n\t    }\r\n\t}\r\n\tfunction createProxy(ep, path = [], target = function () { }) {\r\n\t    let isProxyReleased = false;\r\n\t    const proxy = new Proxy(target, {\r\n\t        get(_target, prop) {\r\n\t            throwIfProxyReleased(isProxyReleased);\r\n\t            if (prop === releaseProxy) {\r\n\t                return () => {\r\n\t                    return requestResponseMessage(ep, {\r\n\t                        type: 5 /* RELEASE */,\r\n\t                        path: path.map((p) => p.toString()),\r\n\t                    }).then(() => {\r\n\t                        closeEndPoint(ep);\r\n\t                        isProxyReleased = true;\r\n\t                    });\r\n\t                };\r\n\t            }\r\n\t            if (prop === \"then\") {\r\n\t                if (path.length === 0) {\r\n\t                    return { then: () => proxy };\r\n\t                }\r\n\t                const r = requestResponseMessage(ep, {\r\n\t                    type: 0 /* GET */,\r\n\t                    path: path.map((p) => p.toString()),\r\n\t                }).then(fromWireValue);\r\n\t                return r.then.bind(r);\r\n\t            }\r\n\t            return createProxy(ep, [...path, prop]);\r\n\t        },\r\n\t        set(_target, prop, rawValue) {\r\n\t            throwIfProxyReleased(isProxyReleased);\r\n\t            // FIXME: ES6 Proxy Handler `set` methods are supposed to return a\r\n\t            // boolean. To show good will, we return true asynchronously ¯\\_(ツ)_/¯\r\n\t            const [value, transferables] = toWireValue(rawValue);\r\n\t            return requestResponseMessage(ep, {\r\n\t                type: 1 /* SET */,\r\n\t                path: [...path, prop].map((p) => p.toString()),\r\n\t                value,\r\n\t            }, transferables).then(fromWireValue);\r\n\t        },\r\n\t        apply(_target, _thisArg, rawArgumentList) {\r\n\t            throwIfProxyReleased(isProxyReleased);\r\n\t            const last = path[path.length - 1];\r\n\t            if (last === createEndpoint) {\r\n\t                return requestResponseMessage(ep, {\r\n\t                    type: 4 /* ENDPOINT */,\r\n\t                }).then(fromWireValue);\r\n\t            }\r\n\t            // We just pretend that `bind()` didn’t happen.\r\n\t            if (last === \"bind\") {\r\n\t                return createProxy(ep, path.slice(0, -1));\r\n\t            }\r\n\t            const [argumentList, transferables] = processArguments(rawArgumentList);\r\n\t            return requestResponseMessage(ep, {\r\n\t                type: 2 /* APPLY */,\r\n\t                path: path.map((p) => p.toString()),\r\n\t                argumentList,\r\n\t            }, transferables).then(fromWireValue);\r\n\t        },\r\n\t        construct(_target, rawArgumentList) {\r\n\t            throwIfProxyReleased(isProxyReleased);\r\n\t            const [argumentList, transferables] = processArguments(rawArgumentList);\r\n\t            return requestResponseMessage(ep, {\r\n\t                type: 3 /* CONSTRUCT */,\r\n\t                path: path.map((p) => p.toString()),\r\n\t                argumentList,\r\n\t            }, transferables).then(fromWireValue);\r\n\t        },\r\n\t    });\r\n\t    return proxy;\r\n\t}\r\n\tfunction myFlat(arr) {\r\n\t    return Array.prototype.concat.apply([], arr);\r\n\t}\r\n\tfunction processArguments(argumentList) {\r\n\t    const processed = argumentList.map(toWireValue);\r\n\t    return [processed.map((v) => v[0]), myFlat(processed.map((v) => v[1]))];\r\n\t}\r\n\tconst transferCache = new WeakMap();\r\n\tfunction transfer(obj, transfers) {\r\n\t    transferCache.set(obj, transfers);\r\n\t    return obj;\r\n\t}\r\n\tfunction proxy(obj) {\r\n\t    return Object.assign(obj, { [proxyMarker]: true });\r\n\t}\r\n\tfunction toWireValue(value) {\r\n\t    for (const [name, handler] of transferHandlers) {\r\n\t        if (handler.canHandle(value)) {\r\n\t            const [serializedValue, transferables] = handler.serialize(value);\r\n\t            return [\r\n\t                {\r\n\t                    type: 3 /* HANDLER */,\r\n\t                    name,\r\n\t                    value: serializedValue,\r\n\t                },\r\n\t                transferables,\r\n\t            ];\r\n\t        }\r\n\t    }\r\n\t    return [\r\n\t        {\r\n\t            type: 0 /* RAW */,\r\n\t            value,\r\n\t        },\r\n\t        transferCache.get(value) || [],\r\n\t    ];\r\n\t}\r\n\tfunction fromWireValue(value) {\r\n\t    switch (value.type) {\r\n\t        case 3 /* HANDLER */:\r\n\t            return transferHandlers.get(value.name).deserialize(value.value);\r\n\t        case 0 /* RAW */:\r\n\t            return value.value;\r\n\t    }\r\n\t}\r\n\tfunction requestResponseMessage(ep, msg, transfers) {\r\n\t    return new Promise((resolve) => {\r\n\t        const id = generateUUID();\r\n\t        ep.addEventListener(\"message\", function l(ev) {\r\n\t            if (!ev.data || !ev.data.id || ev.data.id !== id) {\r\n\t                return;\r\n\t            }\r\n\t            ep.removeEventListener(\"message\", l);\r\n\t            resolve(ev.data);\r\n\t        });\r\n\t        if (ep.start) {\r\n\t            ep.start();\r\n\t        }\r\n\t        ep.postMessage(Object.assign({ id }, msg), transfers);\r\n\t    });\r\n\t}\r\n\tfunction generateUUID() {\r\n\t    return new Array(4)\r\n\t        .fill(0)\r\n\t        .map(() => Math.floor(Math.random() * Number.MAX_SAFE_INTEGER).toString(16))\r\n\t        .join(\"-\");\r\n\t}\n\n\tconst retryUntilWritten = async (db, doc, i = 0) => {\r\n\t    try {\r\n\t        const origDoc = await db.get(doc._id);\r\n\t        doc._rev = origDoc._rev;\r\n\t    }\r\n\t    catch (err) {\r\n\t        if (err.status !== 404) {\r\n\t            console.debug(`Error in retryUntilWritten`);\r\n\t            console.debug(err);\r\n\t        }\r\n\t    }\r\n\t    try {\r\n\t        return db.put(doc);\r\n\t    }\r\n\t    catch (err) {\r\n\t        if (i > 10) {\r\n\t            //prevent infinite loop\r\n\t            console.error(`Error in retryUntilWritten and loop over 10 times`);\r\n\t            console.error(err);\r\n\t            throw err;\r\n\t        }\r\n\t        return retryUntilWritten(db, doc, i++);\r\n\t    }\r\n\t};\r\n\tclass Worker {\r\n\t    constructor() {\r\n\t        this.db = new PouchDB(\"offline-tiles\");\r\n\t        console.debug(\"Worker created\");\r\n\t    }\r\n\t    async saveTile(format, override, tileUrl, existingRevision) {\r\n\t        try {\r\n\t            if (!override) {\r\n\t                try {\r\n\t                    const data = await this.db.get(tileUrl);\r\n\t                    if (data) {\r\n\t                        return;\r\n\t                    }\r\n\t                }\r\n\t                catch (_a) {\r\n\t                    //\r\n\t                }\r\n\t            }\r\n\t            console.debug(`No data for ${tileUrl} in _seedOneTile`);\r\n\t            const response = await fetch(tileUrl);\r\n\t            const blob = await response.blob();\r\n\t            console.debug(`saveTileBlobThread: Saving ${tileUrl}`);\r\n\t            await retryUntilWritten(this.db, {\r\n\t                _id: tileUrl,\r\n\t                _rev: existingRevision,\r\n\t                timestamp: Date.now(),\r\n\t                _attachments: {\r\n\t                    tile: {\r\n\t                        // eslint-disable-next-line @typescript-eslint/camelcase\r\n\t                        content_type: format,\r\n\t                        data: blob,\r\n\t                    },\r\n\t                },\r\n\t            });\r\n\t            console.debug(`${tileUrl}: Done`);\r\n\t        }\r\n\t        catch (err) {\r\n\t            console.error(err);\r\n\t        }\r\n\t    }\r\n\t}\r\n\texpose(new Worker());\n\n})));\n";
